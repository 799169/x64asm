Opcode	Instruction	64-bit Mode	Compat/32-bit-Legacy Mode	Description
				
37	AAA	I	V	ASCII adjust AL after addition.
				
D5 0A	AAD	I	V	ASCII adjust AX before division.
D5 ib	(No mnemonic)	I	V	Adjust AX before division to number base imm8.
				
D4 0A	AAM	I	V	ASCII adjust AX after multiply.
D4 ib	(No mnemonic)	I	V	Adjust AX after multiply to number base imm8.
				
3F	AAS	I	V	ASCII adjust AL after subtraction.
				
14 ib	ADC AL, imm8	V	V	Add with carry imm8 to AL
15 iw	ADC AX, imm16	V	V	Add with carry imm16 to AX.
15 id	ADC EAX, imm32	V	V	Add with carry imm32 to EAX.
REX.W+ 15 id	ADC RAX, imm32	V	NE	Add with carry imm32 sign extended to 64- bits to RAX.
80 /2 ib	ADC r/m8, imm8	V	V	Add with carry imm8 to r/m8.
REX+ 80 /2 ib	ADC r/m8, imm8	V	NE	Add with carry imm8 to r/m8.
81 /2 iw	ADC r/m16, imm16	V	V	Add with carry imm16 to r/m16.
81 /2 id	ADC r/m32, imm32	V	V	Add with CF imm32 to r/m32.
REX.W+ 81 /2 id	ADC r/m64, imm32	V	NE	Add with CF imm32 sign extended to 64-bits to r/m64.
83 /2 ib	ADC r/m16, imm8	V	V	Add with CF sign-extended imm8 to r/m16.
83 /2 ib	ADC r/m32, imm8	V	V	Add with CF sign-extended imm8 into r/m32.
REX.W+ 83 /2 ib	ADC r/m64, imm8	V	NE	Add with CF sign-extended imm8 into r/m64.
10 /r	ADC r/m8, r8	V	V	Add with carry byte register to r/m8.
REX+ 10 /r	ADC r/m8, r8	V	NE	Add with carry byte register to r/m8.
11 /r	ADC r/m16, r16	V	V	Add with carry r16 to r/m16.
11 /r	ADC r/m32, r32	V	V	Add with CF r32 to r/m32.
REX.W+ 11 /r	ADC r/m64, r64	V	NE	Add with CF r64 to r/m64.
12 /r	ADC r8, r/m8	V	V	Add with carry r/m8 to byte register.
REX+ 12 /r	ADC r8, r/m8	V	NE	Add with carry r/m64 to byte register.
13 /r	ADC r16, r/m16	V	V	Add with carry r/m16 to r16.
13 /r	ADC r32, r/m32	V	V	Add with CF r/m32 to r32.
REX.W+ 13 /r	ADC r64, r/m64	V	NE	Add with CF r/m64 to r64.
				
04 ib	ADD AL, imm8	V	V	Add imm8 to AL.
05 iw	ADD AX, imm16 	V	V	Add imm16 to AX.
05 id	ADD EAX, imm32 	V	V	Add imm32 to EAX.
REX.W+ 05 id 	ADD RAX, imm32 	V	NE	Add imm32 sign-extended to 64-bits to RAX.  
80 /0 ib	ADD r/m8, imm8 	V	V	Add imm8 to r/m8.
REX+ 80 /0 ib 	ADD r/m8, imm8 	V	NE	Add sign-extended imm8 to r/m64.
81 /0 iw	ADD r/m16, imm16 	V	V	Add imm16 to r/m16.
81 /0 id	ADD r/m32, imm32 	V	V	Add imm32 to r/m32.
REX.W+ 81 /0 id	ADD r/m64, imm32	V	NE	Add imm32 sign-extended to 64-bits to r/m64.
83 /0 ib	ADD r/m16, imm8 	V	V	Add sign-extended imm8 to r/m16. 
83 /0 ib	ADD r/m32, imm8	V	V	Add sign-extended imm8 to r/m32. 
REX.W+ 83 /0 ib 	ADD r/m64, imm8 	V	NE	Add sign-extended imm8 to r/m64. 
00 /r 	ADD r/m8, r8 	V	V	Add r8 to r/m8.
REX+ 00 /r	ADD r/m8, r8	V	NE	Add r8 to r/m8. 
01 /r	ADD r/m16, r16 	V	V	Add r16 to r/m16. 
01 /r	ADD r/m32, r32 	V	V	Add r32 to r/m32. 
REX.W+ 01 /r 	ADD r/m64, r64 	V	NE	Add r64 to r/m64. 
02 /r 	ADD r8, r/m8 	V	V	Add r/m8 to r8. 
REX+ 02 /r	ADD r8, r/m8	V	NE	Add r/m8 to r8. 
03 /r	ADD r16, r/m16 	V	V	Add r/m16 to r16. 
03 /r	ADD r32, r/m32 	V	V	Add r/m32 to r32. 
REX.W+ 03 /r	ADD r64, r/m64	V	NE	Add r/m64 to r64.
				
66 0F 58 /r	ADDPD xmm1, xmm2/m128	V	V	Add packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG 58 /r	VADDPD xmm1, xmm2, xmm3/m128	V	V	Add packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG 58 /r	VADDPD ymm1, ymm2, ymm3/m256	V	V	Add packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
				
0F 58 /r	ADDPS xmm1, xmm2/m128	V	V	Add packed single-precision floating-point values from xmm2/m128 to xmm1 and stores result in xmm1.
VEX.NDS.128.0F.WIG 58 /r	VADDPS xmm1, xmm2, xmm3/m128	V	V	Add packed single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.0F.WIG 58 /r	VADDPS ymm1, ymm2, ymm3/m256	V	V	Add packed single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
				
F2 0F 58 /r	ADDSD xmm1, xmm2/m64	V	V	Add the low double-precision floating-point value from xmm2/m64 to xmm1.
VEX.NDS.LIG.F2.0F.WIG 58 /r	VADDSD xmm1, xmm2, xmm3/m64	V	V	Add the low double-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1.
				
F3 0F 58 /r	ADDSS xmm1, xmm2/m32	V	V	Add the low single-precision floating-point value from xmm2/m32 to xmm1.
VEX.NDS.LIG.F3.0F.WIG 58 /r	VADDSS xmm1, xmm2, xmm3/m32	V	V	Add the low single-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1.
				
66 0F D0 /r	ADDSUBPD xmm1, xmm2/m128	V	V	Add/subtract double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG D0 /r	VADDSUBPD xmm1, xmm2, xmm3/m128	V	V	Add/subtract packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG D0 /r	VADDSUBPD ymm1, ymm2, ymm3/m256	V	V	Add / subtract packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
				
F2 0F D0 /r	ADDSUBPS xmm1, xmm2/m128	V	V	Add/subtract single-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.F2.0F.WIG D0 /r	VADDSUBPS xmm1, xmm2, xmm3/m128	V	V	Add/subtract single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.F2.0F.WIG D0 /r	VADDSUBPS ymm1, ymm2, ymm3/m256	V	V	Add / subtract single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
				
66 0F 38 DE /r	AESDEC xmm1, xmm2/m128	V	V	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DE /r	VAESDEC xmm1, xmm2, xmm3/m128	V	V	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
				
66 0F 38 DF /r	AESDECLAST xmm1, xmm2/m128	V	V	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DF /r	VAESDECLAST xmm1, xmm2, xmm3/m128	V	V	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
				
66 0F 38 DC /r	AESENC xmm1, xmm2/m128	V	V	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DC /r	VAESENC xmm1, xmm2, xmm3/m128	V	V	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from the xmm3/m128; store the result in xmm1.
				
66 0F 38 DD /r	AESENCLAST xmm1, xmm2/m128	V	V	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DD /r	VAESENCLAST xmm1, xmm2, xmm3/m128	V	V	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128 bit round key from xmm3/m128; store the result in xmm1.
				
66 0F 38 DB /r	AESIMC xmm1, xmm2/m128	V	V	Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
VEX.128.66.0F38.WIG DB /r	VAESIMC xmm1, xmm2/m128	V	V	Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
				
66 0F 3A DF /r ib	AESKEYGENASSIST xmm1, xmm2/m128, imm8	V	V	Assist in AES round key generation using an 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
VEX.128.66.0F3A.WIG DF /r ib	VAESKEYGENASSIST xmm1, xmm2/m128, imm8	V	V	Assist in AES round key generation using 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
				
24 ib	AND AL, imm8	V	V	AL AND imm8.
25 iw	AND AX, imm16	V	V	AX AND imm16.
25 id	AND EAX, imm32	V	V	EAX AND imm32.
REX.W+ 25 id	AND RAX, imm32	V	NE	RAX AND imm32 sign-extended to 64-bits.
80 /4 ib	AND r/m8, imm8	V	V	r/m8 AND imm8.
REX+ 80 /4 ib	AND r/m8, imm8	V	NE	r/m8 AND imm8.
81 /4 iw	AND r/m16, imm16	V	V	r/m16 AND imm16.
81 /4 id	AND r/m32, imm32	V	V	r/m32 AND imm32.
REX.W+ 81 /4 id	AND r/m64, imm32	V	NE	r/m64 AND imm32 sign extended to 64-bits.
83 /4 ib	AND r/m16, imm8	V	V	r/m16 AND imm8 (sign-extended).
83 /4 ib	AND r/m32, imm8	V	V	r/m32 AND imm8 (sign-extended).
REX.W+ 83 /4 ib	AND r/m64, imm8	V	NE	r/m64 AND imm8 (sign-extended).
20 /r	AND r/m8, r8	V	V	r/m8 AND r8.
REX+ 20 /r	AND r/m8, r8	V	NE	r/m64 AND r8 (sign-extended).
21 /r	AND r/m16, r16	V	V	r/m16 AND r16.
21 /r	AND r/m32, r32	V	V	r/m32 AND r32.
REX.W+ 21 /r	AND r/m64, r64	V	NE	r/m64 AND r32.
22 /r	AND r8, r/m8	V	V	r8 AND r/m8.
REX+ 22 /r	AND r8, r/m8	V	NE	r/m64 AND r8 (sign-extended).
23 /r	AND r16, r/m16	V	V	r16 AND r/m16.
23 /r	AND r32, r/m32	V	V	r32 AND r/m32.
REX.W+ 23 /r	AND r64, r/m64	V	NE	r64 AND r/m64.
				
66 0F 54 /r	ANDPD xmm1, xmm2/m128	V	V	Return the bitwise logical AND of packed double-precision floating-point values in xmm1 and xmm2/m128.
VEX.NDS.128.66.0F.WIG 54 /r	VANDPD xmm1, xmm2, xmm3/m128	V	V	Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 54 /r	VANDPD ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/mem.
				
0F 54 /r	ANDPS xmm1, xmm2/m128	V	V	Bitwise logical AND of xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 54 /r	VANDPS xmm1,xmm2, xmm3/m128	V	V	Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 54 /r	VANDPS ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/mem.
				
66 0F 55 /r	ANDNPD xmm1, xmm2/m128	V	V	Bitwise logical AND NOT of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 55 /r	VANDNPD xmm1, xmm2, xmm3/m128	V	V	Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 55 /r	VANDNPD ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/mem.
				
0F 55 /r	ANDNPS xmm1, xmm2/m128	V	V	Bitwise logical AND NOT of xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 55 /r	VANDNPS xmm1, xmm2, xmm3/m128	V	V	Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 55 /r	VANDNPS ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical AND NOT of packed single-precision floating-point values in ymm2 and ymm3/mem.
				
63 /r	ARPL r/m16, r16	NE	V	Adjust RPL of r/m16 to not less than RPL of r16.
				
66 0F 3A 0D /r ib	BLENDPD xmm1, xmm2/m128, imm8	V	V	Select packed DP-FP values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.128.66.0F3A.WIG0D /r ib	VBLENDPD xmm1, xmm2, xmm3/m128, imm8	V	V	Select packed double-precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1. 
VEX.NDS.256.66.0F3A.WIG0D /r ib	VBLENDPD ymm1, ymm2, ymm3/m256, imm8	V	V	Select packed double-precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
				
66 0F 3A 0C /r ib	BLENDPS xmm1, xmm2/m128, imm8	V	V	Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.128.66.0F3A.WIG0C /r ib	VBLENDPS xmm1, xmm2, xmm3/m128, imm8	V	V	Select packed single-precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1.
VEX.NDS.256.66.0F3A.WIG0C /r ib	VBLENDPS ymm1, ymm2, ymm3/m256, imm8	V	V	Select packed single-precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
				
66 0F 38 15 /r	BLENDVPD xmm1, xmm2/m128 , <XMM0>	V	V	Select packed DP FP values from xmm1 and xmm2 from mask specified in XMM0 and store the values in xmm1.
VEX.NDS.128.66.0F3A.W0 4B /r /is4	VBLENDVPD xmm1, xmm2, xmm3/m128, xmm4	V	V	Conditionally copy double-precision floating- point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4.
VEX.NDS.256.66.0F3A.W0 4B /r /is4	VBLENDVPD ymm1, ymm2, ymm3/m256, ymm4	V	V	Conditionally copy double-precision floating- point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4.
				
66 0F 38 14 /r	BLENDVPS xmm1, xmm2/m128, <XMM0>	V	V	Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in XMM0 and store the values into xmm1.
VEX.NDS.128.66.0F3A.W0 4A /r /is4	VBLENDVPS xmm1, xmm2, xmm3/m128, xmm4	V	V	Conditionally copy single-precision floating- point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4.
VEX.NDS.256.66.0F3A.W0 4A /r /is4	VBLENDVPS ymm1, ymm2, ymm3/m256, ymm4	V	V	Conditionally copy single-precision floating- point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4.
				
62 /r	BOUND r16, m16&16	I	V	Check if r16 (array index) is within bounds specified by m16&16.
62 /r	BOUND r32, m32&32	I	V	Check if r32 (array index) is within bounds specified by m16&16.
				
0F BC /r	BSF r16, r/m16	V	V	Bit scan forward on r/m16.
0F BC /r	BSF r32, r/m32	V	V	Bit scan forward on r/m32.
REX.W+ 0F BC	BSF r64, r/m64	V	NE	Bit scan forward on r/m64.
				
0F BD /r	BSR r16, r/m16	V	V	Bit scan reverse on r/m16.
0F BD /r	BSR r32, r/m32	V	V	Bit scan reverse on r/m32.
REX.W+ 0F BD	BSR r64, r/m64	V	NE	Bit scan reverse on r/m64.
				
0F C8 +rd	BSWAP r32	V	V	Reverses the byte order of a 32-bit register.
REX.W+ 0F C8 +rd	BSWAP r64	V	NE	Reverses the byte order of a 64-bit register.
				
0F A3	BT r/m16, r16	V	V	Store selected bit in CF flag.
0F A3	BT r/m32, r32	V	V	Store selected bit in CF flag.
REX.W+ 0F A3	BT r/m64, r64	V	NE	Store selected bit in CF flag.
0F BA /4 ib	BT r/m16, imm8	V	V	Store selected bit in CF flag.
0F BA /4 ib	BT r/m32, imm8	V	V	Store selected bit in CF flag.
REX.W+ 0F BA /4 ib	BT r/m64, imm8	V	NE	Store selected bit in CF flag.
				
0F BB	BTC r/m16, r16	V	V	Store selected bit in CF flag and complement.
0F BB	BTC r/m32, r32	V	V	Store selected bit in CF flag and complement.
REX.W+ 0F BB	BTC r/m64, r64	V	NE	Store selected bit in CF flag and complement.
0F BA /7 ib	BTC r/m16, imm8	V	V	Store selected bit in CF flag and complement.
0F BA /7 ib	BTC r/m32, imm8	V	V	Store selected bit in CF flag and complement.
REX.W+ 0F BA /7 ib	BTC r/m64, imm8	V	NE	Store selected bit in CF flag and complement.
				
0F B3	BTR r/m16, r16	V	V	Store selected bit in CF flag and clear.
0F B3	BTR r/m32, r32 	V	V	Store selected bit in CF flag and clear.
REX.W+ 0F B3	BTR r/m64, r64 	V	NE	Store selected bit in CF flag and clear.
0F BA /6 ib	BTR r/m16, imm8 	V	V	Store selected bit in CF flag and clear.
0F BA /6 ib	BTR r/m32, imm8 	V	V	Store selected bit in CF flag and clear.
REX.W+ 0F BA /6 ib	BTR r/m64, imm8	V	NE	Store selected bit in CF flag and clear.
				
0F AB	BTS r/m16, r16 	V	V	Store selected bit in CF flag and set.
0F AB	BTS r/m32, r32 	V	V	Store selected bit in CF flag and set.
REX.W+ 0F AB	BTS r/m64, r64 	V	NE	Store selected bit in CF flag and set.
0F BA /5 ib	BTS r/m16, imm8 	V	V	Store selected bit in CF flag and set.
0F BA /5 ib	BTS r/m32, imm8 	V	V	Store selected bit in CF flag and set.
REX.W+ 0F BA /5 ib	BTS r/m64, imm8	V	NE	Store selected bit in CF flag and set.
				
E8 cw 	CALL rel16 	NS	V	Call near, relative, displacement relative to next instruction.
E8 cd	CALL rel32	V	V	Call near, relative, displacement relative to next instruction. 32-bit displacement sign extended to 64-bits in 64-bit mode.
FF /2	CALL r/m16	NE	V	Call near, absolute indirect, address given in r/m16.
FF /2	CALL r/m32	NE	V	Call near, absolute indirect, address given in r/m32.
FF /2	CALL r/m64	V	NE	Call near, absolute indirect, address given in r/m64.
9A cd 	CALL ptr16:16 	I	V	Call far, absolute, address given in operand.
9A cp 	CALL ptr16:32 	I	V	Call far, absolute, address given in operand.
FF /3	CALL m16:16	V	V	Call far, absolute indirect address given in m16:16. In 32-bit mode: if selector points to a gate, then RIP = 32-bit zero extended displacement taken from gate; else RIP = zero extended 16- bit offset from far pointer referenced in the instruction.
FF /3	CALL m16:32	V	V	In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = zero extended 32-bit offset from far pointer referenced in the instruction.
REX.W+ FF /3	CALL m16:64	V	NE	In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = 64-bit offset from far pointer referenced in the instruction.
				
98	CBW	V	V	AX = sign-extend of AL.
98	CWDE	V	V	EAX = sign-extend of AX.
REX.W+ 98	CDQE	V	NE	RAX = sign-extend of EAX.
				
F8	CLC	V	V	Clear CF flag.
				
FC	CLD	V	V	Clear DF flag.
				
0F AE /7	CLFLUSH m8	V	V	Flushes cache line containing m8.
				
FA	CLI	V	V	Clear interrupt flag; interrupts disabled when interrupt flag cleared.
				
0F 06	CLTS	V	V	Clears TS flag in CR0.
				
F5	CMC	V	V	Complement CF flag.
				
0F 47 /r	CMOVA r16, r/m16 	V	V	Move if above (CF=0 and ZF=0).
0F 47 /r 	CMOVA r32, r/m32 	V	V	Move if above (CF=0 and ZF=0).
REX.W+ 0F 47 /r 	CMOVA r64, r/m64 	V	NE	Move if above (CF=0 and ZF=0).
0F 43 /r	CMOVAE r16, r/m16 	V	V	Move if above or equal (CF=0).
0F 43 /r 	CMOVAE r32, r/m32 	V	V	Move if above or equal (CF=0).
REX.W+ 0F 43 /r 	CMOVAE r64, r/m64 	V	NE	Move if above or equal (CF=0).
0F 42 /r	CMOVB r16, r/m16 	V	V	Move if below (CF=1).
0F 42 /r 	CMOVB r32, r/m32 	V	V	Move if below (CF=1).
REX.W+ 0F 42 /r 	CMOVB r64, r/m64 	V	NE	Move if below (CF=1).
0F 46 /r	CMOVBE r16, r/m16 	V	V	Move if below or equal (CF=1 or ZF=1).
0F 46 /r 	CMOVBE r32, r/m32 	V	V	Move if below or equal (CF=1 or ZF=1).
REX.W+ 0F 46 /r 	CMOVBE r64, r/m64	V	NE	Move if below or equal (CF=1 or ZF=1).
0F 42 /r	CMOVC r16, r/m16 	V	V	Move if carry (CF=1).
0F 42 /r 	CMOVC r32, r/m32 	V	V	Move if carry (CF=1).
REX.W+ 0F 42 /r 	CMOVC r64, r/m64 	V	NE	Move if carry (CF=1).
0F 44 /r	CMOVE r16, r/m16 	V	V	Move if equal (ZF=1).
0F 44 /r 	CMOVE r32, r/m32 	V	V	Move if equal (ZF=1).
REX.W+ 0F 44 /r 	CMOVE r64, r/m64 	V	NE	Move if equal (ZF=1).
0F 4F /r	CMOVG r16, r/m16 	V	V	Move if greater (ZF=0 and SF=OF).
0F 4F /r 	CMOVG r32, r/m32 	V	V	Move if greater (ZF=0 and SF=OF).
REX.W+ 0F 4F /r 	CMOVG r64, r/m64 	V	NE	Move if greater (ZF=0 and SF=OF).
0F 4D /r	CMOVGE r16, r/m16 	V	V	Move if greater or equal (SF=OF).
0F 4D /r 	CMOVGE r32, r/m32 	V	V	Move if greater or equal (SF=OF).
REX.W+ 0F 4D /r 	CMOVGE r64, r/m64 	V	NE	Move if greater or equal (SF=OF).
0F 4C /r	CMOVL r16, r/m16 	V	V	Move if less (SF != OF).
0F 4C /r 	CMOVL r32, r/m32 	V	V	Move if less (SF!= OF).
REX.W+ 0F 4C /r 	CMOVL r64, r/m64 	V	NE	Move if less (SF!= OF).
0F 4E /r	CMOVLE r16, r/m16 	V	V	Move if less or equal (ZF=1 or SF!= OF).
0F 4E /r 	CMOVLE r32, r/m32 	V	V	Move if less or equal (ZF=1 or SF!= OF).
REX.W+ 0F 4E /r 	CMOVLE r64, r/m64 	V	NE	Move if less or equal (ZF=1 or SF!= OF).
0F 46 /r	CMOVNA r16, r/m16 	V	V	Move if not above (CF=1 or ZF=1).
0F 46 /r 	CMOVNA r32, r/m32 	V	V	Move if not above (CF=1 or ZF=1).
REX.W+ 0F 46 /r	CMOVNA r64, r/m64 	V	NE	Move if not above (CF=1 or ZF=1).
0F 42 /r	CMOVNAE r16, r/m16 	V	V	Move if not above or equal (CF=1).
0F 42 /r 	CMOVNAE r32, r/m32 	V	V	Move if not above or equal (CF=1).
REX.W+ 0F 42 /r 	CMOVNAE r64, r/m64 	V	NE	Move if not above or equal (CF=1).
0F 43 /r	CMOVNB r16, r/m16 	V	V	Move if not below (CF=0).
0F 43 /r 	CMOVNB r32, r/m32 	V	V	Move if not below (CF=0).
REX.W+ 0F 43 /r 	CMOVNB r64, r/m64 	V	NE	Move if not below (CF=0).
0F 47 /r	CMOVNBE r16, r/m16	V	V	Move if not below or equal (CF=0 and ZF=0).
0F 47 /r 	CMOVNBE r32, r/m32 	V	V	Move if not below or equal (CF=0 and ZF=0).
REX.W+ 0F 47 /r 	CMOVNBE r64, r/m64 	V	NE	Move if not below or equal (CF=0 and ZF=0).
0F 43 /r	CMOVNC r16, r/m16 	V	V	Move if not carry (CF=0).
0F 43 /r 	CMOVNC r32, r/m32 	V	V	Move if not carry (CF=0).
REX.W+ 0F 43 /r 	CMOVNC r64, r/m64 	V	NE	Move if not carry (CF=0).
0F 45 /r	CMOVNE r16, r/m16 	V	V	Move if not equal (ZF=0).
0F 45 /r 	CMOVNE r32, r/m32 	V	V	Move if not equal (ZF=0).
REX.W+ 0F 45 /r 	CMOVNE r64, r/m64 	V	NE	Move if not equal (ZF=0).
0F 4E /r	CMOVNG r16, r/m16 	V	V	Move if not greater (ZF=1 or SF!= OF).
0F 4E /r 	CMOVNG r32, r/m32 	V	V	Move if not greater (ZF=1 or SF!= OF).
REX.W+ 0F 4E /r 	CMOVNG r64, r/m64 	V	NE	Move if not greater (ZF=1 or SF!= OF).
0F 4C /r	CMOVNGE r16, r/m16 	V	V	Move if not greater or equal (SF!= OF).
0F 4C /r 	CMOVNGE r32, r/m32 	V	V	Move if not greater or equal (SF!= OF).
REX.W+ 0F 4C /r 	CMOVNGE r64, r/m64 	V	NE	Move if not greater or equal (SF!= OF).
0F 4D /r	CMOVNL r16, r/m16 	V	V	Move if not less (SF=OF).
0F 4D /r 	CMOVNL r32, r/m32 	V	V	Move if not less (SF=OF).
REX.W+ 0F 4D /r 	CMOVNL r64, r/m64 	V	NE	Move if not less (SF=OF).
0F 4F /r	CMOVNLE r16, r/m16 	V	V	Move if not less or equal (ZF=0 and SF=OF).
0F 4F /r 	CMOVNLE r32, r/m32 	V	V	Move if not less or equal (ZF=0 and SF=OF).
REX.W+ 0F 4F /r 	CMOVNLE r64, r/m64 	V	NE	Move if not less or equal (ZF=0 and SF=OF).
0F 41 /r	CMOVNO r16, r/m16 	V	V	Move if not overflow (OF=0).
0F 41 /r 	CMOVNO r32, r/m32 	V	V	Move if not overflow (OF=0).
REX.W+ 0F 41 /r 	CMOVNO r64, r/m64 	V	NE	Move if not overflow (OF=0).
0F 4B /r	CMOVNP r16, r/m16 	V	V	Move if not parity (PF=0).
0F 4B /r 	CMOVNP r32, r/m32 	V	V	Move if not parity (PF=0).
REX.W+ 0F 4B /r 	CMOVNP r64, r/m64 	V	NE	Move if not parity (PF=0).
0F 49 /r	CMOVNS r16, r/m16 	V	V	Move if not sign (SF=0).
0F 49 /r 	CMOVNS r32, r/m32 	V	V	Move if not sign (SF=0).
REX.W+ 0F 49 /r 	CMOVNS r64, r/m64 	V	NE	Move if not sign (SF=0).
0F 45 /r	CMOVNZ r16, r/m16 	V	V	Move if not zero (ZF=0).
0F 45 /r 	CMOVNZ r32, r/m32 	V	V	Move if not zero (ZF=0).
REX.W+ 0F 45 /r 	CMOVNZ r64, r/m64 	V	NE	Move if not zero (ZF=0).
0F 40 /r	CMOVO r16, r/m16 	V	V	Move if overflow (OF=1).
0F 40 /r 	CMOVO r32, r/m32 	V	V	Move if overflow (OF=1).
REX.W+ 0F 40 /r 	CMOVO r64, r/m64 	V	NE	Move if overflow (OF=1).
0F 4A /r	CMOVP r16, r/m16 	V	V	Move if parity (PF=1).
0F 4A /r 	CMOVP r32, r/m32 	V	V	Move if parity (PF=1).
REX.W+ 0F 4A /r 	CMOVP r64, r/m64 	V	NE	Move if parity (PF=1).
0F 4A /r	CMOVPE r16, r/m16 	V	V	Move if parity even (PF=1).
0F 4A /r 	CMOVPE r32, r/m32 	V	V	Move if parity even (PF=1).
REX.W+ 0F 4A /r	CMOVPE r64, r/m64	V	NE	Move if parity even (PF=1).
0F 4B /r	CMOVPO r16, r/m16 	V	V	Move if parity odd (PF=0).
0F 4B /r	CMOVPO r32, r/m32 	V	V	Move if parity odd (PF=0).
REX.W+ 0F 4B /r 	CMOVPO r64, r/m64 	V	NE	Move if parity odd (PF=0).
0F 48 /r	CMOVS r16, r/m16 	V	V	Move if sign (SF=1).
0F 48 /r	CMOVS r32, r/m32 	V	V	Move if sign (SF=1).
REX.W+ 0F 48 /r 	CMOVS r64, r/m64 	V	NE	Move if sign (SF=1).
0F 44 /r	CMOVZ r16, r/m16 	V	V	Move if zero (ZF=1).
0F 44 /r	CMOVZ r32, r/m32 	V	V	Move if zero (ZF=1).
REX.W+ 0F 44 /r	CMOVZ r64, r/m64	V	NE	Move if zero (ZF=1).
				
3C ib	CMP AL, imm8 	V	V	Compare imm8 with AL.
3D iw	CMP AX, imm16	V	V	Compare imm16 with AX.
3D id	CMP EAX, imm32 	V	V	Compare imm32 with EAX.
REX.W+ 3D id	CMP RAX, imm32	V	NE	Compare imm32 sign-extended to 64-bits with RAX.
80 /7 ib	CMP r/m8, imm8 	V	V	Compare imm8 with r/m8.
REX+ 80 /7 ib	CMP r/m8, imm8	V	NE	Compare imm8 with r/m8.
81 /7 iw	CMP r/m16, imm16 	V	V	Compare imm16 with r/m16.
81 /7 id	CMP r/m32, imm32 	V	V	Compare imm32 with r/m32.
REX.W+ 81 /7 id	CMP r/m64, imm32	V	NE	Compare imm32 sign-extended to 64-bits with r/m64.
83 /7 ib	CMP r/m16, imm8 	V	V	Compare imm8 with r/m16.
83 /7 ib	CMP r/m32, imm8 	V	V	Compare imm8 with r/m32.
REX.W+ 83 /7 ib	CMP r/m64, imm8 	V	NE	Compare imm8 with r/m64.
38 /r	CMP r/m8, r8 	V	V	Compare r8 with r/m8.
REX+ 38 /r	CMP r/m8, r8	V	NE	Compare r8 with r/m8.
39 /r	CMP r/m16, r16 	V	V	Compare r16 with r/m16.
39 /r	CMP r/m32, r32 	V	V	Compare r32 with r/m32.
REX.W+ 39 /r	CMP r/m64, r64 	V	NE	Compare r64 with r/m64.
3A /r	CMP r8, r/m8 	V	V	Compare r/m8 with r8.
REX+ 3A /r	CMP r8, r/m8	V	NE	Compare r/m8 with r8.
3B /r	CMP r16, r/m16 	V	V	Compare r/m16 with r16.
3B /r	CMP r32, r/m32 	V	V	Compare r/m32 with r32.
REX.W+ 3B /r	CMP r64, r/m64	V	NE	Compare r/m64 with r64.
				
66 0F C2 /r ib	CMPPD xmm1, xmm2/m128, imm8	V	V	Compare packed double-precision floating- point values in xmm2/m128 and xmm1 using imm8 as comparison predicate.
VEX.NDS.128.66.0F.WIG C2 /r ib	VCMPPD xmm1, xmm2, xmm3/m128, imm8	V	V	Compare packed double-precision floating- point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VEX.NDS.256.66.0F.WIG C2 /r ib	VCMPPD ymm1, ymm2, ymm3/m256, imm8	V	V	Compare packed double-precision floating- point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
				
0F C2 /r ib	CMPPS xmm1, xmm2/m128, imm8	V	V	Compare packed single-precision floating- point values in xmm2/mem and xmm1 using imm8 as comparison predicate.
VEX.NDS.128.0F.WIG C2 /r ib	VCMPPS xmm1, xmm2, xmm3/m128, imm8	V	V	Compare packed single-precision floating- point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VEX.NDS.256.0F.WIG C2 /r ib	VCMPPS ymm1, ymm2, ymm3/m256, imm8	V	V	Compare packed single-precision floating- point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
				
A6	CMPS m8, m8	V	V	For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI to byte at address (R|E)DI. The status flags are set accordingly.
A7	CMPS m16, m16	V	V	For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
A7	CMPS m32, m32	V	V	For legacy mode, compare dword at address DS:(E)SI at dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI at dword at address (R|E)DI. The status flags are set accordingly.
REX.W+ A7 	CMPS m64, m64 	V	NE	Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
A6	CMPSB	V	V	For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64- bit mode compare byte at address (R|E)SI with byte at address (R|E)DI. The status flags are set accordingly.
PREF.66+ A7	CMPSW	V	V	For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64- bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
A7	CMPSD	V	V	For legacy mode, compare dword at address DS:(E)SI with dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI with dword at address (R|E)DI. The status flags are set accordingly.
REX.W+ A7	CMPSQ	V	NE	Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
				
F2 0F C2 /r ib	CMPSD xmm1, xmm2/m64, imm8	V	V	Compare low double-precision floating-point value in xmm2/m64 and xmm1 using imm8 as comparison predicate.
VEX.NDS.LIG.F2.0F.WIG C2 /r ib	VCMPSD xmm1, xmm2, xmm3/m64, imm8	V	V	Compare low double precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate.
				
F3 0F C2 /r ib	CMPSS xmm1, xmm2/m32, imm8	V	V	Compare low single-precision floating-point value in xmm2/m32 and xmm1 using imm8 as comparison predicate.
VEX.NDS.LIG.F3.0F.WIG C2 /r ib	VCMPSS xmm1, xmm2, xmm3/m32, imm8	V	V	Compare low single precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate.
				
0F B0 /r	CMPXCHG r/m8, r8	V	V	Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
REX+ 0F B0 /r	CMPXCHG r/m8,r8	V	NE	Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
0F B1 /r	CMPXCHG r/m16, r16	V	V	Compare AX with r/m16. If equal, ZF is set and r16 is loaded into r/m16. Else, clear ZF and load r/m16 into AX.
0F B1 /r	CMPXCHG r/m32, r32	V	V	Compare EAX with r/m32. If equal, ZF is set and r32 is loaded into r/m32. Else, clear ZF and load r/m32 into EAX.
REX.W+ 0F B1 /r	CMPXCHG r/m64, r64	V	NE	Compare RAX with r/m64. If equal, ZF is set and r64 is loaded into r/m64. Else, clear ZF and load r/m64 into RAX.
				
0F C7 /1	CMPXCHG8B m64	V	V	Compare EDX:EAX with m64. If equal, set ZF and load ECX:EBX into m64. Else, clear ZF and load m64 into EDX:EAX.
REX.W+ 0F C7 /1	CMPXCHG16B m128	V	NE	Compare RDX:RAX with m128. If equal, set ZF and load RCX:RBX into m128. Else, clear ZF and load m128 into RDX:RAX.
				
66 0F 2F /r	COMISD xmm1, xmm2/m64	V	V	Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VEX.LIG.66.0F.WIG 2F /r	VCOMISD xmm1, xmm2/m64	V	V	Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
				
0F 2F /r	COMISS xmm1, xmm2/m32	V	V	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VEX.LIG.0F.2F.WIG /r	VCOMISS xmm1, xmm2/m32	V	V	Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
				
0F A2	CPUID	V	V	Returns processor identification and feature information to the EAX, EBX, ECX, and EDX registers, as determined by input entered in EAX (in some cases, ECX as well).
				
F2 0F 38 F0 /r	CRC32 r32, r/m8	V	V	Accumulate CRC32 on r/m8.
F2 REX+ 0F 38 F0 /r	CRC32 r32, r/m8	V	NE	Accumulate CRC32 on r/m8.
F2 0F 38 F1 /r	CRC32 r32, r/m16	V	V	Accumulate CRC32 on r/m16.
F2 0F 38 F1 /r	CRC32 r32, r/m32	V	V	Accumulate CRC32 on r/m32.
F2 REX.W+ 0F 38 F0 /r	CRC32 r64, r/m8	V	NE	Accumulate CRC32 on r/m8.
F2 REX.W+ 0F 38 F1 /r	CRC32 r64, r/m64	V	NE	Accumulate CRC32 on r/m64.
				
F3 0F E6	CVTDQ2PD xmm1, xmm2/m64	V	V	Convert two packed signed doubleword integers from xmm2/m128 to two packed double-precision floating-point values in xmm1.
VEX.128.F3.0F.WIG E6 /r	VCVTDQ2PD xmm1, xmm2/m64	V	V	Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1.
VEX.256.F3.0F.WIG E6 /r	VCVTDQ2PD ymm1, ymm2/m128	V	V	Convert four packed signed doubleword integers from ymm2/mem to four packed double-precision floating-point values in ymm1.
				
0F 5B /r	CVTDQ2PS xmm1, xmm2/m128	V	V	Convert four packed signed doubleword integers from xmm2/m128 to four packed single-precision floating-point values in xmm1.
VEX.128.0F.WIG 5B /r	VCVTDQ2PS xmm1, xmm2/m128	V	V	Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1.
VEX.256.0F.WIG 5B /r	VCVTDQ2PS ymm1, ymm2/m256	V	V	Convert eight packed signed doubleword integers from ymm2/mem to eight packed single-precision floating-point values in ymm1.
				
F2 0F E6	CVTPD2DQ xmm1, xmm2/m128	V	V	Convert two packed double-precision floating- point values from xmm2/m128 to two packed signed doubleword integers in xmm1.
VEX.128.F2.0F.WIG E6 /r	VCVTPD2DQ xmm1, xmm2/m128	V	V	Convert two packed double-precision floating- point values in xmm2/mem to two signed doubleword integers in xmm1.
VEX.256.F2.0F.WIG E6 /r	VCVTPD2DQ xmm1, ymm2/m256	V	V	Convert four packed double-precision floating- point values in ymm2/mem to four signed doubleword integers in xmm1.
				
66 0F 2D /r	CVTPD2PI mm, xmm/m128	V	V	Convert two packed double-precision floating- point values from xmm/m128 to two packed signed doubleword integers in mm.
				
66 0F 5A /r	CVTPD2PS xmm1, xmm2/m128	V	V	Convert two packed double-precision floating- point values in xmm2/m128 to two packed single-precision floating-point values in xmm1.
VEX.128.66.0F.WIG 5A /r	VCVTPD2PS xmm1, xmm2/m128	V	V	Convert two packed double-precision floating- point values in xmm2/mem to two single- precision floating-point values in xmm1.
VEX.256.66.0F.WIG 5A /r	VCVTPD2PS xmm1, ymm2/m256	V	V	Convert four packed double-precision floating- point values in ymm2/mem to four single- precision floating-point values in xmm1.
				
66 0F 2A /r	CVTPI2PD xmm, mm/m64	V	V	Convert two packed signed doubleword integers from mm/mem64 to two packed double-precision floating-point values in xmm.
				
0F 2A /r	CVTPI2PS xmm, mm/m64	V	V	Convert two signed doubleword integers from mm/m64 to two single-precision floating-point values in xmm.
				
66 0F 5B /r	CVTPS2DQ xmm1, xmm2/m128	V	V	Convert four packed single-precision floating- point values from xmm2/m128 to four packed signed doubleword integers in xmm1.
VEX.128.66.0F.WIG 5B /r	VCVTPS2DQ xmm1, xmm2/m128	V	V	Convert four packed single precision floating- point values from xmm2/mem to four packed signed doubleword values in xmm1.
VEX.256.66.0F.WIG 5B /r	VCVTPS2DQ ymm1, ymm2/m256	V	V	Convert eight packed single precision floating- point values from ymm2/mem to eight packed signed doubleword values in ymm1.
				
0F 5A /r	CVTPS2PD xmm1, xmm2/m64	V	V	Convert two packed single-precision floating- point values in xmm2/m64 to two packed double-precision floating-point values in xmm1.
VEX.128.0F.WIG 5A /r	VCVTPS2PD xmm1, xmm2/m64	V	V	Convert two packed single-precision floating- point values in xmm2/mem to two packed double-precision floating-point values in xmm1.
VEX.256.0F.WIG 5A /r	VCVTPS2PD ymm1, xmm2/m128	V	V	Convert four packed single-precision floating- point values in xmm2/mem to four packed double-precision floating-point values in ymm1.
				
0F 2D /r	CVTPS2PI mm, xmm/m64	V	V	Convert two packed single-precision floating- point values from xmm/m64 to two packed signed doubleword integers in mm.
				
F2 0F 2D /r	CVTSD2SI r32, xmm/m64	V	V	Convert one double-precision floating-point value from xmm/m64 to one signed doubleword integer r32.
F2 REX.w+ 0F 2D /r	CVTSD2SI r64, xmm/m64	V	NE	Convert one double-precision floating-point value from xmm/m64 to one signed quadword integer sign-extended into r64.
VEX.LIG.F2.0F.W0 2D /r	VCVTSD2SI r32, xmm1/m64	V	V	Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
VEX.LIG.F2.0F.W1 2D /r	VCVTSD2SI r64, xmm1/m64	V	NE	Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer sign-extended into r64.
				
F2 0F 5A /r	CVTSD2SS xmm1, xmm2/m64	V	V	Convert one double-precision floating-point value in xmm2/m64 to one single-precision floating-point value in xmm1.
VEX.NDS.LIG.F2.0F.WIG 5A /r	VCVTSD2SS xmm1,xmm2, xmm3/m64	V	V	Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2.
				
F2 0F 2A /r	CVTSI2SD xmm, r/m32	V	V	Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm.
F2 REX.w+ 0F 2A /r	CVTSI2SD xmm, r/m64	V	NE	Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm.
VEX.NDS.LIG.F2.0F.W0 2A /r	VCVTSI2SD xmm1, xmm2, r/m32	V	V	Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
VEX.NDS.LIG.F2.0F.W1 2A /r	VCVTSI2SD xmm1, xmm2, r/m64	V	NE	Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
				
F3 0F 2A /r	CVTSI2SS xmm, r/m32	V	V	Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm.
F3 REX.w+ 0F 2A /r	CVTSI2SS xmm, r/m64	V	NE	Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm.
VEX.NDS.LIG.F3.0F.W0 2A /r	VCVTSI2SS xmm1, xmm2, r/m32	V	V	Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
VEX.NDS.LIG.F3.0F.W1 2A /r	VCVTSI2SS xmm1, xmm2, r/m64	V	NE	Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
				
F3 0F 5A /r	CVTSS2SD xmm1, xmm2/m32	V	V	Convert one single-precision floating-point value in xmm2/m32 to one double-precision floating-point value in xmm1.
VEX.NDS.LIG.F3.0F.WIG 5A /r	VCVTSS2SD xmm1, xmm2, xmm3/m32	V	V	Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2.
				
F3 0F 2D /r	CVTSS2SI r32, xmm/m32	V	V	Convert one single-precision floating-point value from xmm/m32 to one signed doubleword integer in r32.
F3 REX.w+ 0F 2D /r	CVTSS2SI r64, xmm/m32	V	NE	Convert one single-precision floating-point value from xmm/m32 to one signed quadword integer in r64.
VEX.LIG.F3.0F.W0 2D /r	VCVTSS2SI r32, xmm1/m32	V	V	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
VEX.LIG.F3.0F.W1 2D /r	VCVTSS2SI r64, xmm1/m32	V	NE	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
				
66 0F E6	CVTTPD2DQ xmm1, xmm2/m128	V	V	Convert two packed double-precision floating- point values from xmm2/m128 to two packed signed doubleword integers in xmm1 using truncation.
VEX.128.66.0F.WIG E6 /r	VCVTTPD2DQ xmm1, xmm2/m128	V	V	Convert two packed double-precision floating- point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation.
VEX.256.66.0F.WIG E6 /r	VCVTTPD2DQ xmm1, ymm2/m256	V	V	Convert four packed double-precision floating- point values in ymm2/mem to four signed doubleword integers in xmm1 using truncation.
				
66 0F 2C /r	CVTTPD2PI mm, xmm/m128	V	V	Convert two packer double-precision floating- point values from xmm/m128 to two packed signed doubleword integers in mm using truncation.
				
F3 0F 5B /r	CVTTPS2DQ xmm1, xmm2/m128	V	V	Convert four single-precision floating-point values from xmm2/m128 to four signed doubleword integers in xmm1 using truncation.
VEX.128.F3.0F.WIG 5B /r	VCVTTPS2DQ xmm1, xmm2/m128	V	V	Convert four packed single precision floating- point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation.
VEX.256.F3.0F.WIG 5B /r	VCVTTPS2DQ ymm1, ymm2/m256	V	V	Convert eight packed single precision floating- point values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation.
				
0F 2C /r	CVTTPS2PI mm, xmm/m64	V	V	Convert two single-precision floating-point values from xmm/m64 to two signed doubleword signed integers in mm using truncation.
				
F2 0F 2C /r	CVTTSD2SI r32, xmm/m64	V	V	Convert one double-precision floating-point value from xmm/m64 to one signed doubleword integer in r32 using truncation.
F2 REX.w+ 0F 2C /r	CVTTSD2SI r64, xmm/m64	V	NE	Convert one double precision floating-point value from xmm/m64 to one signedquadword integer in r64 using truncation.
VEX.LIG.F2.0F.W0 2C /r	VCVTTSD2SI r32, xmm1/m64	V	V	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
VEX.LIG.F2.0F.W1 2C /r	VCVTTSD2SI r64, xmm1/m64	V	NE	Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
				
F3 0F 2C /r	CVTTSS2SI r32, xmm/m32	V	V	Convert one single-precision floating-point value from xmm/m32 to one signed doubleword integer in r32 using truncation.
F3 REX.w+ 0F 2C /r	CVTTSS2SI r64, xmm/m32	V	NE	Convert one single-precision floating-point value from xmm/m32 to one signed quadword integer in r64 using truncation.
VEX.LIG.F3.0F.W0 2C /r	VCVTTSS2SI r32, xmm1/m32	V	V	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
VEX.LIG.F3.0F.W1 2C /r	VCVTTSS2SI r64, xmm1/m32	V	NE	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
				
PREF.66+ 99	CWD	V	V	DX:AX = sign-extend of AX.
99	CDQ	V	V	EDX:EAX = sign-extend of EAX.
REX.W+ 99	CQO	V	NE	RDX:RAX = sign-extend of RAX.
				
27	DAA	I	V	Decimal adjust AL after addition.
				
2F	DAS	I	V	Decimal adjust AL after subtraction.
				
FE /1	DEC r/m8 	V	V	Decrement r/m8 by 1.
REX+ FE /1	DEC r/m8	V	NE	Decrement r/m8 by 1.
FF /1	DEC r/m16 	V	V	Decrement r/m16 by 1.
FF /1	DEC r/m32 	V	V	Decrement r/m32 by 1.
REX.W+ FF /1	DEC r/m64 	V	NE	Decrement r/m64 by 1.
48 +rw	DEC r16 	NE	V	Decrement r16 by 1.
48 +rd	DEC r32	NE	V	Decrement r32 by 1.
				
F6 /6	DIV r/m8	V	V	Unsigned divide AX by r/m8, with result stored in AL = Quotient, AH = Remainder.
REX+ F6 /6	DIV r/m8	V	NE	Unsigned divide AX by r/m8, with result stored in AL = Quotient, AH = Remainder.
F7 /6	DIV r/m16 	V	V	Unsigned divide DX:AX by r/m16, with result stored in AX = Quotient, DX = Remainder.
F7 /6	DIV r/m32	V	V	Unsigned divide EDX:EAX by r/m32, with result stored in EAX = Quotient, EDX = Remainder.
REX.W+ F7 /6	DIV r/m64	V	NE	Unsigned divide RDX:RAX by r/m64, with result stored in RAX = Quotient, RDX = Remainder.
				
66 0F 5E /r	DIVPD xmm1, xmm2/m128	V	V	Divide packed double-precision floating-point values in xmm1 by packed double-precision floating-point values xmm2/m128.
VEX.NDS.128.66.0F.WIG 5E /r	VDIVPD xmm1, xmm2, xmm3/m128	V	V	Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem.
VEX.NDS.256.66.0F.WIG 5E /r	VDIVPD ymm1, ymm2, ymm3/m256	V	V	Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem.
				
0F 5E /r	DIVPS xmm1, xmm2/m128	V	V	Divide packed single-precision floating-point values in xmm1 by packed single-precision floating-point values xmm2/m128.
VEX.NDS.128.0F.WIG 5E /r	VDIVPS xmm1, xmm2, xmm3/m128	V	V	Divide packed single-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem.
VEX.NDS.256.0F.WIG 5E /r	VDIVPS ymm1, ymm2, ymm3/m256	V	V	Divide packed single-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem.
				
F2 0F 5E /r	DIVSD xmm1, xmm2/m64	V	V	Divide low double-precision floating-point value in xmm1 by low double-precision floating-point value in xmm2/mem64.
VEX.NDS.LIG.F2.0F.WIG 5E /r	VDIVSD xmm1, xmm2, xmm3/m64	V	V	Divide low double-precision floating point values in xmm2 by low double precision floating-point value in xmm3/mem64.
				
F3 0F 5E /r	DIVSS xmm1, xmm2/m32	V	V	Divide low single-precision floating-point value in xmm1 by low single-precision floating-point value in xmm2/m32.
VEX.NDS.LIG.F3.0F.WIG 5E /r	VDIVSS xmm1, xmm2, xmm3/m32	V	V	Divide low single-precision floating point value in xmm2 by low single precision floating-point value in xmm3/m32.
				
66 0F 3A 41 /r ib	DPPD xmm1, xmm2/m128, imm8	V	V	Selectively multiply packed DP floating-point values from xmm1 with packed DP floating- point values from xmm2, add and selectively store the packed DP floating-point values to xmm1.
VEX.NDS.128.66.0F3A.WIG 41 /r ib	VDPPD xmm1, xmm2, xmm3/m128, imm8	V	V	Selectively multiply packed DP floating-point values from xmm2 with packed DP floating- point values from xmm3, add and selectively store the packed DP floating-point values to xmm1.
				
66 0F 3A 40 /r ib	DPPS xmm1, xmm2/m128, imm8	V	V	Selectively multiply packed SP floating-point values from xmm1 with packed SP floating- point values from xmm2, add and selectively store the packed SP floating-point values or zero values to xmm1.
VEX.NDS.128.66.0F3A.WIG 40 /r ib	VDPPS xmm1, xmm2, xmm3/m128, imm8	V	V	Multiply packed SP floating point values from xmm1 with packed SP floating point values from xmm2/mem selectively add and store to xmm1.
VEX.NDS.256.66.0F3A.WIG 40 /r ib	VDPPS ymm1, ymm2, ymm3/m256, imm8	V	V	Multiply packed single-precision floating-point values from ymm2 with packed SP floating point values from ymm3/mem, selectively add pairs of elements and store to ymm1.
				
0F 77	EMMS	V	V	Set the x87 FPU tag word to empty.
				
C8 iw 00 	ENTER imm16, 0	V	V	Create a stack frame for a procedure.
C8 iw 01	ENTER imm16, 1 	V	V	Create a nested stack frame for a procedure.
C8 iw ib	ENTER imm16, imm8	V	V	Create a nested stack frame for a procedure.
				
66 0F 3A 17 /r ib	EXTRACTPS reg/m32, xmm2, imm8	V	V	Extract a single-precision floating-point value from xmm2 at the source offset specified by imm8 and store the result to reg or m32. The upper 32 bits of r64 is zeroed if reg is r64.
VEX.128.66.0F3A.WIG 17 /r ib	VEXTRACTPS r/m32, xmm1, imm8	V	V	Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
				
D9 F0	F2XM1	V	V	Replace ST(0) with (2^(ST(0)) – 1).
				
D9 E1	FABS	V	V	Replace ST with its absolute value.
				
D8 /0	FADD m32fp 	V	V	Add m32fp to ST(0) and store result in ST(0).
DC /0	FADD m64fp 	V	V	Add m64fp to ST(0) and store result in ST(0).
D8 C0 +i	FADD ST(0), ST(i) 	V	V	Add ST(0) to ST(i) and store result in ST(0).
DC C0 +i	FADD ST(i), ST(0) 	V	V	Add ST(i) to ST(0) and store result in ST(i).
DE C0 +i	FADDP ST(i), ST(0)	V	V	Add ST(0) to ST(i), store result in ST(i), and pop the register stack.
DE C1	FADDP	V	V	Add ST(0) to ST(1), store result in ST(1), and pop the register stack.
DA /0	FIADD m32int	V	V	Add m32int to ST(0) and store result in ST(0).
DE /0	FIADD m16int	V	V	Add m16int to ST(0) and store result in ST(0).
				
DF /4	FBLD m80 dec	V	V	Convert BCD value to floating-point and push onto the FPU stack.
				
DF /6	FBSTP m80bcd	V	V	Store ST(0) in m80bcd and pop ST(0).
				
D9 E0	FCHS	V	V	Complements sign of ST(0).
				
9B DB E2	FCLEX	V	V	Clear floating-point exception flags after checking for pending unmasked floating-point exceptions.
DB E2	FNCLEX	V	V	Clear floating-point exception flags without checking for pending unmasked floating-point exceptions.
				
DA C0 +i 	FCMOVB ST(0), ST(i) 	V	V	Move if below (CF=1).
DA C8 +i 	FCMOVE ST(0), ST(i) 	V	V	Move if equal (ZF=1).
DA D0 +i 	FCMOVBE ST(0), ST(i) 	V	V	Move if below or equal (CF=1 or ZF=1).
DA D8 +i 	FCMOVU ST(0), ST(i) 	V	V	Move if unordered (PF=1).
DB C0 +i 	FCMOVNB ST(0), ST(i) 	V	V	Move if not below (CF=0).
DB C8 +i 	FCMOVNE ST(0), ST(i) 	V	V	Move if not equal (ZF=0).
DB D0 +i 	FCMOVNBE ST(0), ST(i) 	V	V	Move if not below or equal (CF=0 and ZF=0).
DB D8 +i	FCMOVNU ST(0), ST(i)	V	V	Move if not unordered (PF=0).
				
D8 /2 	FCOM m32fp 	V	V	Compare ST(0) with m32fp.
DC /2 	FCOM m64fp 	V	V	Compare ST(0) with m64fp.
D8 D0 +i 	FCOM ST(i) 	V	V	Compare ST(0) with ST(i).
D8 D1 	FCOM	V	V	Compare ST(0) with ST(1).
D8 /3 	FCOMP m32fp 	V	V	Compare ST(0) with m32fp and pop register stack.
DC /3 	FCOMP m64fp 	V	V	Compare ST(0) with m64fp and pop register stack.
D8 D8 +i 	FCOMP ST(i) 	V	V	Compare ST(0) with ST(i) and pop register stack.
D8 D9 	FCOMP 	V	V	Compare ST(0) with ST(1) and pop register stack.
DE D9	FCOMPP	V	V	Compare ST(0) with ST(1) and pop register stack twice.
				
DB F0 +i 	FCOMI ST, ST(i) 	V	V	Compare ST(0) with ST(i) and set status flags accordingly.
DF F0 +i	FCOMIP ST, ST(i) 	V	V	Compare ST(0) with ST(i), set status flags accordingly, and pop register stack.
DB E8 +i 	FUCOMI ST, ST(i) 	V	V	Compare ST(0) with ST(i), check for ordered values, and set status flags accordingly.
DF E8 +i	FUCOMIP ST, ST(i) 	V	V	Compare ST(0) with ST(i), check for ordered values, set status flags accordingly, and pop register stack.
				
D9 FF 	FCOS 	V	V	Replace ST(0) with its cosine.
				
D9 F6 	FDECSTP 	V	V	Decrement TOP field in FPU status word.
				
D8 /6 	FDIV m32fp 	V	V	Divide ST(0) by m32fp and store result in ST(0).
DC /6 	FDIV m64fp 	V	V	Compare ST(0) with ST(i), set status flags accordingly, and pop register stack.
D8 F0 +i 	FDIV ST(0), ST(i) 	V	V	Divide ST(0) by ST(i) and store result in ST(0).
DC F8 +i 	FDIV ST(i), ST(0) 	V	V	Divide ST(i) by ST(0) and store result in ST(i).
DE F8 +i 	FDIVP ST(i), ST(0) 	V	V	Divide ST(i) by ST(0), store result in ST(i), and pop the register stack.
DE F9 	FDIVP 	V	V	Divide ST(1) by ST(0), store result in ST(1), and pop the register stack.
DA /6 	FIDIV m32int 	V	V	Divide ST(0) by m32int and store result in ST(0).
DE /6 	FIDIV m16int 	V	V	Divide ST(0) by m64int and store result in ST(0).
				
D8 /7 	FDIVR m32fp 	V	V	Divide m32fp by ST(0) and store result in ST(0).
DC /7 	FDIVR m64fp 	V	V	Divide m64fp by ST(0) and store result in ST(0).
D8 F8 +i 	FDIVR ST(0), ST(i) 	V	V	Divide ST(i) by ST(0) and store result in ST(0).
DC F0 +i 	FDIVR ST(i), ST(0) 	V	V	Divide ST(0) by ST(i) and store result in ST(i).
DE F0 +i 	FDIVRP ST(i), ST(0) 	V	V	Divide ST(0) by ST(i), store result in ST(i), and pop the register stack.
DE F1 	FDIVRP 	V	V	Divide ST(0) by ST(1), store result in ST(1), and pop the register stack.
DA /7 	FIDIVR m32int 	V	V	Divide m32int by ST(0) and store result in ST(0).
DE /7 	FIDIVR m16int 	V	V	Divide m16int by ST(0) and store result in ST(0).
				
DD C0 +i 	FFREE ST(i) 	V	V	Sets tag for ST(i) to empty.
				
DE /2 	FICOM m16int 	V	V	Compare ST(0) with m16int.
DA /2 	FICOM m32int 	V	V	Compare ST(0) with m32int.
DE /3 	FICOMP m16int 	V	V	Compare ST(0) with m16int and pop stack register.
DA /3 	FICOMP m32int 	V	V	Compare ST(0) with m32int and pop stack register.
				
DF /0 	FILD m16int 	V	V	Push m16int onto the FPU register stack.
DB /0 	FILD m32int 	V	V	Push m32int onto the FPU register stack.
DF /5 	FILD m64int 	V	V	Push m64int onto the FPU register stack.
				
D9 F7 	FINCSTP 	V	V	Increment the TOP field in the FPU status register.
				
9B DB E3 	FINIT 	V	V	Initialize FPU after checking for pending unmasked floating-point exceptions.
DB E3 	FNINIT	V	V	Initialize FPU without checking for pending unmasked floating-point exceptions.
				
DF /2 	FIST m16int 	V	V	Store ST(0) in m16int.
DB /2 	FIST m32int 	V	V	Store ST(0) in m32int.
DF /3 	FISTP m16int 	V	V	Store ST(0) in m16int and pop register stack.
DB /3 	FISTP m32int 	V	V	Store ST(0) in m32int and pop register stack.
DF /7 	FISTP m64int 	V	V	Store ST(0) in m64int and pop register stack.
				
DF /1 	FISTTP m16int 	V	V	Store ST(0) in m16int with truncation.
DB /1 	FISTTP m32int 	V	V	Store ST(0) in m32int with truncation.
DD /1 	FISTTP m64int 	V	V	Store ST(0) in m64int with truncation.
				
D9 /0 	FLD m32fp 	V	V	Push m32fp onto the FPU register stack.
DD /0 	FLD m64fp 	V	V	Push m64fp onto the FPU register stack.
DB /5 	FLD m80fp 	V	V	Push m80fp onto the FPU register stack.
D9 C0 +i 	FLD ST(i) 	V	V	Push ST(i) onto the FPU register stack.
				
D9 E8 	FLD1 	V	V	Push +1.0 onto the FPU register stack.
D9 E9 	FLDL2T 	V	V	Push log210 onto the FPU register stack.
D9 EA 	FLDL2E 	V	V	Push log2e onto the FPU register stack.
D9 EB 	FLDPI 	V	V	Push π onto the FPU register stack.
D9 EC 	FLDLG2 	V	V	Push log102 onto the FPU register stack.
D9 ED 	FLDLN2 	V	V	Push loge2 onto the FPU register stack.
D9 EE 	FLDZ 	V	V	Push +0.0 onto the FPU register stack.
				
D9 /5 	FLDCW m2byte 	V	V	Load FPU control word from m2byte.
				
D9 /4 	FLDENV m14/28byte 	V	V	Load FPU environment from m14byte or m28byte.
				
D8 /1 	FMUL m32fp 	V	V	Multiply ST(0) by m32fp and store result in ST(0).
DC /1 	FMUL m64fp 	V	V	Multiply ST(0) by m64fp and store result in ST(0).
D8 C8 +i 	FMUL ST(0), ST(i) 	V	V	Multiply ST(0) by ST(i) and store result in ST(0).
DC C8 +i 	FMUL ST(i), ST(0) 	V	V	Multiply ST(i) by ST(0) and store result in ST(i).
DE C8 +i 	FMULP ST(i), ST(0) 	V	V	Multiply ST(i) by ST(0), store result in ST(i), and pop the register stack.
DE C9 	FMULP 	V	V	Multiply ST(1) by ST(0), store result in ST(1), and pop the register stack.
DA /1 	FIMUL m32int 	V	V	Multiply ST(0) by m32int and store result in ST(0).
DE /1 	FIMUL m16int 	V	V	Multiply ST(0) by m16int and store result in ST(0).
				
D9 D0 	FNOP 	V	V	No operation is performed.
				
D9 F3 	FPATAN 	V	V	Replace ST(1) with arctan(ST(1)/ST(0)) and pop the register stack.
				
D9 F8 	FPREM 	V	V	Replace ST(0) with the remainder obtained from dividing ST(0) by ST(1).
				
D9 F5 	FPREM1 	V	V	Replace ST(0) with the IEEE remainder obtained from dividing ST(0) by ST(1).
				
D9 F2 	FPTAN 	V	V	Replace ST(0) with its tangent and push 1 onto the FPU stack.
				
D9 FC 	FRNDINT 	V	V	Round ST(0) to an integer.
				
DD /4 	FRSTOR m94/108byte 	V	V	Load FPU state from m94byte or m108byte.
				
9B DD /6 	FSAVE m94/108byte 	V	V	Store FPU state to m94byte or m108byte after checking for pending unmasked floating-point exceptions. Then re-initialize the FPU.
DD /6 	FNSAVE m94/108byte 	V	V	Store FPU environment to m94byte or m108byte without checking for pending unmasked floating-point exceptions. Then re-initialize the FPU.
				
D9 FD 	FSCALE 	V	V	Scale ST(0) by ST(1).
				
D9 FE 	FSIN 	V	V	Replace ST(0) with its sine.
				
D9 FB 	FSINCOS 	V	V	Compute the sine and cosine of ST(0); replace ST(0) with the sine, and push the cosine onto the register stack.
				
D9 FA 	FSQRT 	V	V	Computes square root of ST(0) and stores the result in ST(0).
				
D9 /2 	FST m32fp 	V	V	Copy ST(0) to m32fp.
DD /2 	FST m64fp 	V	V	Copy ST(0) to m64fp.
DD D0 +i 	FST ST(i) 	V	V	Copy ST(0) to ST(i).
D9 /3 	FSTP m32fp 	V	V	Copy ST(0) to m32fp and pop register stack.
DD /3 	FSTP m64fp 	V	V	Copy ST(0) to m64fp and pop register stack.
DB /7 	FSTP m80fp 	V	V	Copy ST(0) to m80fp and pop register stack.
DD D8 +i 	FSTP ST(i) 	V	V	Copy ST(0) to ST(i) and pop register stack.
				
9B D9 /7 	FSTCW m2byte 	V	V	Store FPU control word to m2byte after checking for pending unmasked floating-point exceptions.
D9 /7 	FNSTCW m2byte 	V	V	Store FPU control word to m2byte without checking for pending unmasked floating-point exceptions.
				
9B D9 /6 	FSTENV m14/28byte 	V	V	Store FPU environment to m14byte or m28byte after checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions.
D9 /6 	FNSTENV m14/28byte 	V	V	Store FPU environment to m14byte or m28byte without checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions.
				
9B DD /7 	FSTSW m2byte 	V	V	Store FPU status word at m2byte after checking for pending unmasked floating-point exceptions.
9B DF E0 	FSTSW AX 	V	V	Store FPU status word in AX register after checking for pending unmasked floating-point exceptions.
DD /7 	FNSTSW m2byte 	V	V	Store FPU status word at m2byte without checking for pending unmasked floating-point exceptions.
DF E0 	FNSTSW AX 	V	V	Store FPU status word in AX register without checking for pending unmasked floating-point exceptions.
				
D8 /4 	FSUB m32fp 	V	V	Subtract m32fp from ST(0) and store result in ST(0).
DC /4 	FSUB m64fp 	V	V	Subtract m64fp from ST(0) and store result in ST(0).
D8 E0 +i 	FSUB ST(0), ST(i) 	V	V	Subtract ST(i) from ST(0) and store result in ST(0).
DC E8 +i 	FSUB ST(i), ST(0) 	V	V	Subtract ST(0) from ST(i) and store result in ST(i).
DE E8 +i 	FSUBP ST(i), ST(0) 	V	V	Subtract ST(0) from ST(i), store result in ST(i), and pop register stack.
DE E9 	FSUBP 	V	V	Subtract ST(0) from ST(1), store result in ST(1), and pop register stack.
DA /4 	FISUB m32int 	V	V	Subtract m32int from ST(0) and store result in ST(0).
DE /4 	FISUB m16int 	V	V	Subtract m16int from ST(0) and store result in ST(0).
				
D8 /5 	FSUBR m32fp 	V	V	Subtract ST(0) from m32fp and store result in ST(0).
DC /5 	FSUBR m64fp 	V	V	Subtract ST(0) from m64fp and store result in ST(0).
D8 E8 +i 	FSUBR ST(0), ST(i) 	V	V	Subtract ST(0) from ST(i) and store result in ST(0).
DC E0 +i 	FSUBR ST(i), ST(0) 	V	V	Subtract ST(i) from ST(0) and store result in ST(i).
DE E0 +i 	FSUBRP ST(i), ST(0) 	V	V	Subtract ST(i) from ST(0), store result in ST(i), and pop register stack.
DE E1 	FSUBRP 	V	V	Subtract ST(1) from ST(0), store result in ST(1), and pop register stack.
DA /5 	FISUBR m32int 	V	V	Subtract ST(0) from m32int and store result in ST(0).
DE /5 	FISUBR m16int 	V	V	Subtract ST(0) from m16int and store result in ST(0).
				
D9 E4 	FTST 	V	V	Compare ST(0) with 0.0.
				
DD E0 +i 	FUCOM ST(i) 	V	V	Compare ST(0) with ST(i).
DD E1 	FUCOM 	V	V	Compare ST(0) with ST(1).
DD E8 +i 	FUCOMP ST(i) 	V	V	Compare ST(0) with ST(i) and pop register stack.
DD E9 	FUCOMP 	V	V	Compare ST(0) with ST(1) and pop register stack.
DA E9 	FUCOMPP 	V	V	Compare ST(0) with ST(1) and pop register stack twice.
				
D9 E5 	FXAM 	V	V	Classify value or number in ST(0).
				
D9 C8 +i 	FXCH ST(i) 	V	V	Exchange the contents of ST(0) and ST(i).
D9 C9 	FXCH 	V	V	Exchange the contents of ST(0) and ST(1).
				
0F AE /1 	FXRSTOR m512byte	V	V	Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
REX.W+ 0F AE /1	FXRSTOR64 m512byte	V	NE	Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
				
0F AE /0 	FXSAVE m512byte	V	V	Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
REX.W+ 0F AE /0	FXSAVE64 m512byte	V	NE	Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
				
D9 F4 	FXTRACT	V	V	Separate value in ST(0) into exponent and significand, store exponent in ST(0), and push the significand onto the register stack.
				
D9 F1 	FYL2X 	V	V	Replace ST(1) with (ST(1) ∗ log2ST(0)) and pop the register stack.
				
D9 F9 	FYL2XP1 	V	V	Replace ST(1) with ST(1) ∗ log2(ST(0) + 1.0) and pop the register stack.
				
66 0F 7C /r 	HADDPD xmm1, xmm2/m128	V	V	Horizontal add packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG 7C /r	VHADDPD xmm1, xmm2, xmm3/m128	V	V	Horizontal add packed double-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 7C /r	VHADDPD ymm1, ymm2, ymm3/m256	V	V	Horizontal add packed double-precision floating-point values from ymm2 and ymm3/mem.
				
F2 0F 7C /r 	HADDPS xmm1, xmm2/m128	V	V	Horizontal add packed single-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.F2.0F.WIG 7C /r	VHADDPS xmm1, xmm2, xmm3/m128	V	V	Horizontal add packed single-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.F2.0F.WIG 7C /r	VHADDPS ymm1, ymm2, ymm3/m256	V	V	Horizontal add packed single-precision floating-point values from ymm2 and ymm3/mem.
				
F4 	HLT 	V	V	Halt
				
66 0F 7D /r 	HSUBPD xmm1, xmm2/m128	V	V	Horizontal subtract packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG 7D /r 	VHSUBPD xmm1, xmm2, xmm3/m128 	V	V	Horizontal subtract packed double-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 7D /r 	VHSUBPD ymm1, ymm2, ymm3/m256 	V	V	Horizontal subtract packed double-precision floating-point values from ymm2 and ymm3/mem.
				
F2 0F 7D /r 	HSUBPS xmm1, xmm2/m128	V	V	Horizontal subtract packed single-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.F2.0F.WIG 7D /r	VHSUBPS xmm1, xmm2, xmm3/m128	V	V	Horizontal subtract packed single-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.F2.0F.WIG 7D /r	VHSUBPS ymm1, ymm2, ymm3/m256	V	V	Horizontal subtract packed single-precision floating-point values from ymm2 and ymm3/mem.
				
F6 /7 	IDIV r/m8 	V	V	Signed divide AX by r/m8, with result stored in: AL = Quotient, AH = Remainder.
REX+ F6 /7 	IDIV r/m8	V	NE	Signed divide AX by r/m8, with result stored in AL = Quotient, AH = Remainder.
F7 /7 	IDIV r/m16 	V	V	Signed divide DX:AX by r/m16, with result stored in AX = Quotient, DX = Remainder.
F7 /7 	IDIV r/m32 	V	V	Signed divide EDX:EAX by r/m32, with result stored in EAX = Quotient, EDX = Remainder.
REX.W+ F7 /7 	IDIV r/m64 	V	NE	Signed divide RDX:RAX by r/m64, with result stored in RAX = Quotient, RDX = Remainder.
				
F6 /5	IMUL r/m8	V	V	AX= AL ∗ r/m byte.
F7 /5 	IMUL r/m16 	V	V	DX:AX = AX ∗ r/m word.
F7 /5 	IMUL r/m32 	V	V	EDX:EAX = EAX ∗ r/m32.
REX.W+ F7 /5 	IMUL r/m64 	V	NE	RDX:RAX = RAX ∗ r/m64.
0F AF /r 	IMUL r16, r/m16 	V	V	word register = word register ∗ r/m16.
0F AF /r 	IMUL r32, r/m32 	V	V	doubleword register = doubleword register ∗  r/m32.
REX.W+ 0F AF /r 	IMUL r64, r/m64 	V	NE	Quadword register = Quadword register ∗  r/m64.
6B /r ib 	IMUL r16, r/m16, imm8 	V	V	word register = r/m16 ∗ sign-extended immediate byte.
6B /r ib 	IMUL r32, r/m32, imm8 	V	V	doubleword register = r/m32 ∗ sign- extended immediate byte.
REX.W+ 6B /r ib 	IMUL r64, r/m64, imm8 	V	NE	Quadword register = r/m64 ∗ sign-extended  immediate byte.
69 /r iw 	IMUL r16, r/m16, imm16 	V	V	word register = r/m16 ∗ immediate word.
69 /r id 	IMUL r32, r/m32, imm32 	V	V	doubleword register = r/m32 ∗ immediate doubleword.
REX.W+ 69 /r id 	IMUL r64, r/m64, imm32 	V	NE	Quadword register = r/m64 ∗ immediate doubleword.
				
E4 ib 	IN AL, imm8 	V	V	Input byte from imm8 I/O port address into AL.
E5 ib 	IN AX, imm8 	V	V	Input word from imm8 I/O port address into AX.
E5 ib 	IN EAX, imm8 	V	V	Input dword from imm8 I/O port address into EAX.
EC 	IN AL,DX 	V	V	Input byte from I/O port in DX into AL.
ED 	IN AX,DX 	V	V	Input word from I/O port in DX into AX.
ED 	IN EAX,DX 	V	V	Input doubleword from I/O port in DX into EAX.
				
FE /0 	INC r/m8 	V	V	Increment r/m byte by 1.
REX+ FE /0 	INC r/m8	V	NE	Increment r/m byte by 1.
FF /0 	INC r/m16 	V	V	Increment r/m word by 1.
FF /0 	INC r/m32 	V	V	Increment r/m doubleword by 1.
REX.W+ FF /0	INC r/m64 	V	NE	Increment r/m quadword by 1.
40 +rw 	INC r16 	NE	V	Increment word register by 1.
40 +rd 	INC r32 	NE	V	Increment doubleword register by 1.
				
6C 	INS m8, DX 	V	V	Input byte from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
6D 	INS m16, DX 	V	V	Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
6D 	INS m32, DX 	V	V	Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
6C 	INSB 	V	V	Input byte from I/O port specified in DX into memory location specified with ES:(E)DI or RDI.1
PREF.66+ 6D 	INSW 	V	V	Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
6D 	INSD 	V	V	Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
				
66 0F 3A 21 /r ib 	INSERTPS xmm1, xmm2/m32, imm8	V	V	Insert a single precision floating-point value selected by imm8 from xmm2/m32 into xmm1 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8.
VEX.NDS.128.66.0F3A.WIG 21 /r ib	VINSERTPS xmm1, xmm2, xmm3/m32, imm8	V	V	Insert a single precision floating point value selected by imm8 from xmm3/m32 and merge into xmm2 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8.
				
CC 	INT 3 	V	V	Interrupt 3—trap to debugger.
CD ib 	INT imm8 	V	V	Interrupt vector number specified by immediate byte.
CE 	INTO 	I	V	Interrupt 4—if overflow flag is 1.
				
0F 08 	INVD 	V	V	Flush internal caches; initiate flushing of external caches.
				
0F 01 /7 	INVLPG m 	V	V	Invalidate TLB Entry for page that contains m.
				
66 0F 38 82 /r 	INVPCID r32, m128	NE	V	Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r32 and descriptor in m128.
66 0F 38 82 /r	INVPCID r64, m128	V	NE	Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r64 and descriptor in m128.
				
CF 	IRET 	V	V	Interrupt return (16-bit operand size).
CF 	IRETD 	V	V	Interrupt return (32-bit operand size).
REX.W+ CF 	IRETQ 	V	NE	Interrupt return (64-bit operand size).
				
77 cb 	JA rel8 	V	V	Jump short if above (CF=0 and ZF=0).
73 cb 	JAE rel8 	V	V	Jump short if above or equal (CF=0).
72 cb 	JB rel8 	V	V	Jump short if below (CF=1).
76 cb 	JBE rel8 	V	V	Jump short if below or equal (CF=1 or ZF=1).
72 cb 	JC rel8 	V	V	Jump short if carry (CF=1).
E3 cb 	JCXZ rel8 	NE	V	Jump short if CX register is 0.
E3 cb 	JECXZ rel8 	V	V	Jump short if ECX register is 0.
E3 cb 	JRCXZ rel8 	V	NE	Jump short if RCX register is 0.
74 cb 	JE rel8 	V	V	Jump short if equal (ZF=1).
7F cb 	JG rel8 	V	V	Jump short if greater (ZF=0 and SF=OF).
7D cb 	JGE rel8 	V	V	Jump short if greater or equal (SF=OF).
7C cb 	JL rel8 	V	V	Jump short if less (SF!= OF).
7E cb 	JLE rel8 	V	V	Jump short if less or equal (ZF=1 or SF!= OF).
76 cb 	JNA rel8 	V	V	Jump short if not above (CF=1 or ZF=1).
72 cb 	JNAE rel8 	V	V	Jump short if not above or equal (CF=1).
73 cb 	JNB rel8 	V	V	Jump short if not below (CF=0).
77 cb 	JNBE rel8 	V	V	Jump short if not below or equal (CF=0 and ZF=0).
73 cb 	JNC rel8 	V	V	Jump short if not carry (CF=0).
75 cb 	JNE rel8 	V	V	Jump short if not equal (ZF=0).
7E cb 	JNG rel8 	V	V	Jump short if not greater (ZF=1 or SF!= OF).
7C cb 	JNGE rel8 	V	V	Jump short if not greater or equal (SF!= OF).
7D cb 	JNL rel8 	V	V	Jump short if not less (SF=OF).
7F cb 	JNLE rel8 	V	V	Jump short if not less or equal (ZF=0 and SF=OF).
71 cb 	JNO rel8 	V	V	Jump short if not overflow (OF=0).
7B cb 	JNP rel8 	V	V	Jump short if not parity (PF=0).
79 cb 	JNS rel8 	V	V	Jump short if not sign (SF=0).
75 cb 	JNZ rel8 	V	V	Jump short if not zero (ZF=0).
70 cb 	JO rel8 	V	V	Jump short if overflow (OF=1).
7A cb 	JP rel8 	V	V	Jump short if parity (PF=1).
7A cb 	JPE rel8 	V	V	Jump short if parity even (PF=1).
7B cb 	JPO rel8 	V	V	Jump short if parity odd (PF=0).
78 cb 	JS rel8 	V	V	Jump short if sign (SF=1).
74 cb 	JZ rel8 	V	V	Jump short if zero (ZF = 1).
0F 87 cw 	JA rel16 	NS	V	Jump near if above (CF=0 and ZF=0). Not supported in 64-bit mode.
0F 87 cd 	JA rel32 	V	V	Jump near if above (CF=0 and ZF=0).
0F 83 cw 	JAE rel16 	NS	V	Jump near if above or equal (CF=0). Not supported in 64-bit mode.
0F 83 cd 	JAE rel32 	V	V	Jump near if above or equal (CF=0).
0F 82 cw 	JB rel16 	NS	V	Jump near if below (CF=1). Not supported in 64-bit mode.
0F 82 cd 	JB rel32 	V	V	Jump near if below (CF=1).
0F 86 cw 	JBE rel16 	NS	V	Jump near if below or equal (CF=1 or ZF=1). Not supported in 64-bit mode.
0F 86 cd 	JBE rel32 	V	V	Jump near if below or equal (CF=1 or ZF=1).
0F 82 cw 	JC rel16 	NS	V	Jump near if carry (CF=1). Not supported in 64-bit mode.
0F 82 cd 	JC rel32 	V	V	Jump near if carry (CF=1).
0F 84 cw 	JE rel16 	NS	V	Jump near if equal (ZF=1). Not supported in 64-bit mode.
0F 84 cd 	JE rel32 	V	V	Jump near if 0 (ZF=1).
0F 8F cw 	JG rel16 	NS	V	Jump near if greater (ZF=0 and SF=OF). Not supported in 64-bit mode.
0F 8F cd 	JG rel32 	V	V	Jump near if greater (ZF=0 and SF=OF).
0F 8D cw 	JGE rel16 	NS	V	Jump near if greater or equal (SF=OF). Not supported in 64-bit mode.
0F 8D cd 	JGE rel32 	V	V	Jump near if greater or equal (SF=OF).
0F 8C cw 	JL rel16 	NS	V	Jump near if less (SF!= OF). Not supported in  64-bit mode.
0F 8C cd 	JL rel32 	V	V	Jump near if less (SF!= OF).
0F 8E cw 	JLE rel16 	NS	V	Jump near if less or equal (ZF=1 or SF!= OF).  Not supported in 64-bit mode.
0F 8E cd 	JLE rel32 	V	V	Jump near if less or equal (ZF=1 or SF!= OF).
0F 86 cw 	JNA rel16 	NS	V	Jump near if not above (CF=1 or ZF=1). Not supported in 64-bit mode.
0F 86 cd 	JNA rel32 	V	V	Jump near if not above (CF=1 or ZF=1).
0F 82 cw 	JNAE rel16 	NS	V	Jump near if not above or equal (CF=1). Not supported in 64-bit mode.
0F 82 cd 	JNAE rel32 	V	V	Jump near if not above or equal (CF=1).
0F 83 cw 	JNB rel16 	NS	V	Jump near if not below (CF=0). Not supported in 64-bit mode.
0F 83 cd 	JNB rel32 	V	V	Jump near if not below (CF=0).
0F 87 cw 	JNBE rel16 	NS	V	Jump near if not below or equal (CF=0 and ZF=0). Not supported in 64-bit mode.
0F 87 cd 	JNBE rel32 	V	V	Jump near if not below or equal (CF=0 and ZF=0). 
0F 83 cw 	JNC rel16 	NS	V	Jump near if not carry (CF=0). Not supported in 64-bit mode.
0F 83 cd 	JNC rel32 	V	V	Jump near if not carry (CF=0).
0F 85 cw 	JNE rel16 	NS	V	Jump near if not equal (ZF=0). Not supported in 64-bit mode.
0F 85 cd 	JNE rel32 	V	V	Jump near if not equal (ZF=0).
0F 8E cw 	JNG rel16 	NS	V	Jump near if not greater (ZF=1 or SF != OF). Not supported in 64-bit mode.
0F 8E cd 	JNG rel32 	V	V	Jump near if not greater (ZF=1 or SF != OF).
0F 8C cw 	JNGE rel16 	NS	V	Jump near if not greater or equal (SF != OF). Not supported in 64-bit mode.
0F 8C cd 	JNGE rel32 	V	V	Jump near if not greater or equal (SF != OF).
0F 8D cw 	JNL rel16 	NS	V	Jump near if not less (SF=OF). Not supported in 64-bit mode.
0F 8D cd 	JNL rel32 	V	V	Jump near if not less (SF=OF).
0F 8F cw 	JNLE rel16 	NS	V	Jump near if not less or equal (ZF=0 and SF=OF). Not supported in 64-bit mode.
0F 8F cd 	JNLE rel32 	V	V	Jump near if not less or equal (ZF=0 and SF=OF).
0F 81 cw 	JNO rel16 	NS	V	Jump near if not overflow (OF=0). Not supported in 64-bit mode.
0F 81 cd 	JNO rel32 	V	V	Jump near if not overflow (OF=0).
0F 8B cw 	JNP rel16 	NS	V	Jump near if not parity (PF=0). Not supported in 64-bit mode.
0F 8B cd 	JNP rel32 	V	V	Jump near if not parity (PF=0).
0F 89 cw 	JNS rel16 	NS	V	Jump near if not sign (SF=0). Not supported in 64-bit mode.
0F 89 cd 	JNS rel32 	V	V	Jump near if not sign (SF=0).
0F 85 cw 	JNZ rel16 	NS	V	Jump near if not zero (ZF=0). Not supported in 64-bit mode.
0F 85 cd 	JNZ rel32 	V	V	Jump near if not zero (ZF=0).
0F 80 cw 	JO rel16 	NS	V	Jump near if overflow (OF=1). Not supported in 64-bit mode.
0F 80 cd 	JO rel32 	V	V	Jump near if overflow (OF=1).
0F 8A cw 	JP rel16 	NS	V	Jump near if parity (PF=1). Not supported in 64-bit mode.
0F 8A cd 	JPE rel32 	V	V	Jump near if parity even (PF=1).
0F 8B cw 	JPO rel16 	NS	V	Jump near if parity odd (PF=0). Not supported in 64-bit mode.
0F 8B cd 	JPO rel32 	V	V	Jump near if parity odd (PF=0).
0F 88 cw 	JS rel16 	NS	V	Jump near if sign (SF=1). Not supported in 64-bit mode.
0F 88 cd 	JS rel32 	V	V	Jump near if sign (SF=1).
0F 84 cw 	JZ rel16 	NS	V	Jump near if 0 (ZF=1). Not supported in 64-bit mode.
0F 84 cd 	JZ rel32 	V	V	Jump near if 0 (ZF=1).
				
EB cb 	JMP rel8 	V	V	Jump short, RIP = RIP + 8-bit displacement sign extended to 64-bits
E9 cw 	JMP rel16 	NS	V	Jump near, relative, displacement relative to next instruction. Not supported in 64-bit mode.
E9 cd 	JMP rel32 	V	V	Jump near, relative, RIP = RIP + 32-bit displacement sign extended to 64-bits
FF /4 	JMP r/m16 	NS	V	Jump near, absolute indirect, address = zero-extended r/m16. Not supported in 64-bit mode.
FF /4 	JMP r/m32 	NS	V	Jump near, absolute indirect, address given in r/m32. Not supported in 64-bit mode.
FF /4 	JMP r/m64 	V	NE	Jump near, absolute indirect, RIP = 64-Bit offset from register or memory
EA cd 	JMP ptr16:16 	I	V	Jump far, absolute, address given in operand
EA cp 	JMP ptr16:32 	I	V	Jump far, absolute, address given in operand
FF /5 	JMP m16:16 	V	V	Jump far, absolute indirect, address given in m16:16
FF /5 	JMP m16:32 	V	V	Jump far, absolute indirect, address given in m16:32.
REX.W+ FF /5 	JMP m16:64 	V	NE	Jump far, absolute indirect, address given in m16:64.
				
9F 	LAHF 	V	V	Load: AH = EFLAGS(SF:ZF:0:AF:0:PF:1:CF).
				
0F 02 /r 	LAR r16, r16/m16 	V	V	r16 = access rights referenced by r16/m16
0F 02 /r 	LAR reg, r32/m16	V	V	reg = access rights referenced by r32/m16
				
F2 0F F0 /r 	LDDQU xmm1, mem	V	V	Load unaligned data from mem and return double quadword in xmm1.
VEX.128.F2.0F.WIG F0 /r	VLDDQU xmm1, m128	V	V	Load unaligned packed integer values from mem to xmm1.
VEX.256.F2.0F.WIG F0 /r	VLDDQU ymm1, m256	V	V	Load unaligned packed integer values from mem to ymm1.
				
0F AE /2 	LDMXCSR m32	V	V	Load MXCSR register from m32.
VEX.LZ.0F.WIG AE /2	VLDMXCSR m32	V	V	Load MXCSR register from m32.
				
C5 /r 	LDS r16, m16:16 	I	V	Load DS:r16 with far pointer from memory.
C5 /r 	LDS r32, m16:32 	I	V	Load DS:r32 with far pointer from memory.
0F B2 /r 	LSS r16, m16:16 	V	V	Load SS:r16 with far pointer from memory.
0F B2 /r 	LSS r32, m16:32 	V	V	Load SS:r32 with far pointer from memory.
REX+ 0F B2 /r 	LSS r64, m16:64 	V	NE	Load SS:r64 with far pointer from memory.
C4 /r 	LES r16, m16:16 	I	V	Load ES:r16 with far pointer from memory.
C4 /r 	LES r32, m16:32 	I	V	Load ES:r32 with far pointer from memory.
0F B4 /r 	LFS r16, m16:16 	V	V	Load FS:r16 with far pointer from memory.
0F B4 /r 	LFS r32, m16:32 	V	V	Load FS:r32 with far pointer from memory.
REX+ 0F B4 /r 	LFS r64, m16:64 	V	NE	Load FS:r64 with far pointer from memory.
0F B5 /r 	LGS r16, m16:16 	V	V	Load GS:r16 with far pointer from memory.
0F B5 /r 	LGS r32, m16:32 	V	V	Load GS:r32 with far pointer from memory.
REX+ 0F B5 /r 	LGS r64, m16:64 	V	NE	Load GS:r64 with far pointer from memory.
				
8D /r 	LEA r16, m 	V	V	Store effective address for m in register r16.
8D /r 	LEA r32, m 	V	V	Store effective address for m in register r32.
REX.W+ 8D /r 	LEA r64, m 	V	NE	Store effective address for m in register r64.
				
PREF.66+ C9 	LEAVE p66	V	V	Set SP to BP, then pop BP.
C9 	LEAVE 	NE	V	Set ESP to EBP, then pop EBP.
C9 	LEAVE 	V	NE	Set RSP to RBP, then pop RBP.
				
0F AE /5 	LFENCE 	V	V	Serializes load operations.
				
0F 01 /2 	LGDT m16&32 	NE	V	Load m into GDTR.
0F 01 /3 	LIDT m16&32 	NE	V	Load m into IDTR.
0F 01 /2 	LGDT m16&64 	V	NE	Load m into GDTR.
0F 01 /3 	LIDT m16&64 	V	NE	Load m into IDTR.
				
0F 00 /2 	LLDT r/m16 	V	V	Load segment selector r/m16 into LDTR.
				
0F 01 /6 	LMSW r/m16 	V	V	Loads r/m16 in machine status word of CR0.
				
F0 	LOCK 	V	V	Asserts LOCK# signal for duration of the accompanying instruction.
				
AC 	LODS m8 	V	V	For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
AD 	LODS m16 	V	V	For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
AD 	LODS m32 	V	V	For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
REX.W+ AD 	LODS m64 	V	NE	Load qword at address (R)SI into RAX.
AC 	LODSB 	V	V	For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
PREF.66+ AD 	LODSW 	V	V	For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
AD 	LODSD 	V	V	For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
REX.W+ AD 	LODSQ 	V	NE	Load qword at address (R)SI into RAX.
				
E2 cb 	LOOP rel8 	V	V	Decrement count; jump short if count != 0.
E1 cb 	LOOPE rel8 	V	V	Decrement count; jump short if count != 0 and  ZF = 1.
E0 cb 	LOOPNE rel8 	V	V	Decrement count; jump short if count != 0 and  ZF = 0.
				
0F 03 /r 	LSL r16, r16/m16 	V	V	Load: r16 = segment limit, selector r16/m16.
0F 03 /r 	LSL r32, r32/m16	V	V	Load: r32 = segment limit, selector r32/m16.
REX.W+ 0F 03 /r	LSL r64, r32/m16	V	V	Load: r64 = segment limit, selector r32/m16
				
0F 00 /3 	LTR r/m16 	V	V	Load r/m16 into task register.
				
66 0F F7 /r 	MASKMOVDQU xmm1, xmm2	V	V	Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
VEX.128.66.0F.WIG F7 /r	VMASKMOVDQU xmm1, xmm2	V	V	Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
				
0F F7 /r 	MASKMOVQ mm1, mm2	V	V	Selectively write bytes from mm1 to memory location using the byte mask in mm2. The default memory location is specified by DS:DI/EDI/RDI.
				
66 0F 5F /r 	MAXPD xmm1, xmm2/m128	V	V	Return the maximum double-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 5F /r	VMAXPD xmm1, xmm2, xmm3/m128	V	V	Return the maximum double-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 5F /r	VMAXPD ymm1, ymm2, ymm3/m256	V	V	Return the maximum packed double-precision floating-point values between ymm2 and ymm3/mem.
				
0F 5F /r 	MAXPS xmm1, xmm2/m128	V	V	Return the maximum single-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 5F /r	VMAXPS xmm1, xmm2, xmm3/m128	V	V	Return the maximum single-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 5F /r	VMAXPS ymm1, ymm2, ymm3/m256	V	V	Return the maximum single double-precision floating-point values between ymm2 and ymm3/mem.
				
F2 0F 5F /r 	MAXSD xmm1, xmm2/m64	V	V	Return the maximum scalar double-precision floating-point value between xmm2/mem64 and xmm1.
VEX.NDS.LIG.F2.0F.WIG 5F /r	VMAXSD xmm1, xmm2, xmm3/m64	V	V	Return the maximum scalar double-precision floating-point value between xmm3/mem64 and xmm2.
				
F3 0F 5F /r 	MAXSS xmm1, xmm2/m32	V	V	Return the maximum scalar single-precision floating-point value between xmm2/mem32 and xmm1.
VEX.NDS.LIG.F3.0F.WIG 5F /r	VMAXSS xmm1, xmm2, xmm3/m32	V	V	Return the maximum scalar single-precision floating-point value between xmm3/mem32 and xmm2.
				
0F AE /6 	MFENCE 	V	V	Serializes load and store operations.
				
66 0F 5D /r 	MINPD xmm1, xmm2/m128	V	V	Return the minimum double-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 5D /r	VMINPD xmm1, xmm2, xmm3/m128	V	V	Return the minimum double-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 5D /r	VMINPD ymm1, ymm2, ymm3/m256	V	V	Return the minimum packed double-precision floating-point values between ymm2 and ymm3/mem.
				
0F 5D /r 	MINPS xmm1, xmm2/m128	V	V	Return the minimum single-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 5D /r	VMINPS xmm1, xmm2, xmm3/m128	V	V	Return the minimum single-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 5D /r	VMINPS ymm1, ymm2, ymm3/m256	V	V	Return the minimum single double-precision floating-point values between ymm2 and ymm3/mem.
				
F2 0F 5D /r 	MINSD xmm1, xmm2/m64	V	V	Return the minimum scalar double-precision floating-point value between xmm2/mem64 and xmm1.
VEX.NDS.LIG.F2.0F.WIG 5D /r	VMINSD xmm1, xmm2, xmm3/m64	V	V	Return the minimum scalar double precision floating-point value between xmm3/mem64 and xmm2.
				
F3 0F 5D /r 	MINSS xmm1, xmm2/m32	V	V	Return the minimum scalar single-precision floating-point value between xmm2/mem32 and xmm1.
VEX.NDS.LIG.F3.0F.WIG 5D /r	VMINSS xmm1,xmm2, xmm3/m32	V	V	Return the minimum scalar single precision floating-point value between xmm3/mem32 and xmm2.
				
0F 01 C8 	MONITOR 	V	V	Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a write-back memory caching type. The address is DS:EAX (DS:RAX in 64-bit mode).
				
88 /r 	MOV r/m8, r8 	V	V	Move r8 to r/m8.
REX+ 88 /r 	MOV r/m8, r8	V	NE	Move r8 to r/m8.
89 /r 	MOV r/m16, r16 	V	V	Move r16 to r/m16.
89 /r 	MOV r/m32, r32 	V	V	Move r32 to r/m32.
REX.W+ 89 /r 	MOV r/m64, r64 	V	NE	Move r64 to r/m64.
8A /r 	MOV r8, r/m8 	V	V	Move r/m8 to r8.
REX+ 8A /r 	MOV r8, r/m8	V	NE	Move r/m8 to r8.
8B /r 	MOV r16, r/m16 	V	V	Move r/m16 to r16.
8B /r 	MOV r32, r/m32 	V	V	Move r/m32 to r32.
REX.W+ 8B /r 	MOV r64, r/m64 	V	NE	Move r/m64 to r64.
8C /r 	MOV r/m16, Sreg	V	V	Move segment register to r/m16.
REX.W+ 8C /r 	MOV r/m64, Sreg	V	V	Move zero extended 16-bit segment register to r/m64.
8E /r 	MOV Sreg, r/m16	V	V	Move r/m16 to segment register.
REX.W+ 8E /r 	MOV Sreg, r/m64	V	V	Move lower 16 bits of r/m64 to segment register.
A0 	MOV AL, moffs8	V	V	Move byte at (seg:offset) to AL.
REX.W+ A0 	MOV AL, moffs8, pw	V	NE	Move byte at (offset) to AL.
A1 	MOV AX, moffs16	V	V	Move word at (seg:offset) to AX.
A1 	MOV EAX, moffs32	V	V	Move doubleword at (seg:offset) to EAX.
REX.W+ A1 	MOV RAX, moffs64	V	NE	Move quadword at (offset) to RAX.
A2 	MOV moffs8, AL 	V	V	Move AL to (seg:offset).
REX.W+ A2 	MOV moffs8, AL, pw 	V	NE	Move AL to (offset).
A3 	MOV moffs16, AX 	V	V	Move AX to (seg:offset).
A3 	MOV moffs32, EAX 	V	V	Move EAX to (seg:offset).
REX.W+ A3 	MOV moffs64, RAX 	V	NE	Move RAX to (offset).
B0 +rb 	MOV r8, imm8 	V	V	Move imm8 to r8.
REX+ B0 +rb	MOV r8, imm8 	V	NE	Move imm8 to r8.
B8 +rw 	MOV r16, imm16 	V	V	Move imm16 to r16.
B8 +rd 	MOV r32, imm32 	V	V	Move imm32 to r32.
REX.W+ B8 +rd 	MOV r64, imm64 	V	NE	Move imm64 to r64.
C6 /0 	MOV r/m8, imm8 	V	V	Move imm8 to r/m8.
REX+ C6 /0 	MOV r/m8, imm8 	V	NE	Move imm8 to r/m8.
C7 /0 	MOV r/m16, imm16 	V	V	Move imm16 to r/m16.
C7 /0 	MOV r/m32, imm32 	V	V	Move imm32 to r/m32.
REX.W+ C7 /0 	MOV r/m64, imm32 	V	NE	Move imm32 sign extended to 64-bits to r/m64.
				
0F 20 /r 	MOV r32, CR0-CR7	NE	V	Move control register to r32.
0F 20 /r	MOV r64, CR0-CR7	V	NE	Move extended control register to r64.
REX.R+ 0F 20 /0	MOV r64, CR8	V	NE	Move extended CR8 to r64.1
0F 22 /r	MOV CR0-CR7, r32	NE	V	Move r32 to control register.
0F 22 /r	MOV CR0-CR7, r64	V	NE	Move r64 to extended control register.
REX.R+ 0F 22 /0	MOV CR8, r64	V	NE	Move r64 to extended CR8.1
				
0F 21 /r 	MOV r32, DR0-DR7	NE	V	Move debug register to r32.
0F 21 /r	MOV r64, DR0-DR7	V	NE	Move extended debug register to r64.
0F 23 /r	MOV DR0-DR7, r32	NE	V	Move r32 to debug register.
0F 23 /r	MOV DR0-DR7, r64	V	NE	Move r64 to extended debug register.
				
66 0F 28 /r 	MOVAPD xmm1, xmm2/m128	V	V	Move packed double-precision floating-point values from xmm2/m128 to xmm1.
66 0F 29 /r	MOVAPD xmm2/m128, xmm1	V	V	Move packed double-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.66.0F.WIG 28 /r	VMOVAPD xmm1, xmm2/m128	V	V	Move aligned packed double-precision floating-point values from xmm2/mem to xmm1.
VEX.128.66.0F.WIG 29 /r	VMOVAPD xmm2/m128, xmm1	V	V	Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem.
VEX.256.66.0F.WIG 28 /r	VMOVAPD ymm1, ymm2/m256	V	V	Move aligned packed double-precision floating-point values from ymm2/mem to ymm1.
VEX.256.66.0F.WIG 29 /r	VMOVAPD ymm2/m256, ymm1	V	V	Move aligned packed double-precision floating-point values from ymm1 to ymm2/mem.
				
0F 28 /r 	MOVAPS xmm1, xmm2/m128	V	V	Move packed single-precision floating-point values from xmm2/m128 to xmm1.
0F 29 /r	MOVAPS xmm2/m128, xmm1	V	V	Move packed single-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.0F.WIG 28 /r	VMOVAPS xmm1, xmm2/m128	V	V	Move aligned packed single-precision floating-point values from xmm2/mem to xmm1.
VEX.128.0F.WIG 29 /r	VMOVAPS xmm2/m128, xmm1	V	V	Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem.
VEX.256.0F.WIG 28 /r	VMOVAPS ymm1, ymm2/m256	V	V	Move aligned packed single-precision floating-point values from ymm2/mem to ymm1.
VEX.256.0F.WIG 29 /r	VMOVAPS ymm2/m256, ymm1	V	V	Move aligned packed single-precision floating-point values from ymm1 to ymm2/mem.
				
0F 38 F0 /r 	MOVBE r16, m16 	V	V	Reverse byte order in m16 and move to r16
0F 38 F0 /r 	MOVBE r32, m32 	V	V	Reverse byte order in m32 and move to r32
REX.W+ 0F 38 F0 /r 	MOVBE r64, m64 	V	NE	Reverse byte order in m64 and move to r64.
0F 38 F1 /r 	MOVBE m16, r16 	V	V	Reverse byte order in r16 and move to m16
0F 38 F1 /r 	MOVBE m32, r32 	V	V	Reverse byte order in r32 and move to m32
REX.W+ 0F 38 F1 /r 	MOVBE m64, r64 	V	NE	Reverse byte order in r64 and move to m64.
				
0F 6E /r 	MOVD mm, r/m32	V	V	Move doubleword from r/m32 to mm.
REX.W+ 0F 6E /r	MOVQ mm, r/m64	V	NE	Move quadword from r/m64 to mm.
0F 7E /r	MOVD r/m32, mm	V	V	Move doubleword from mm to r/m32.
REX.W+ 0F 7E /r	MOVQ r/m64, mm	V	NE	Move quadword from mm to r/m64.
VEX.128.66.0F.W0 6E /r	VMOVD xmm1, r32/m32	V	V	Move doubleword from r/m32 to xmm1.
VEX.128.66.0F.W1 6E /r	VMOVQ xmm1, r64/m64	V	NE	Move quadword from r/m64 to xmm1.
66 0F 6E /r	MOVD xmm, r/m32	V	V	Move doubleword from r/m32 to xmm.
66 REX.W+ 0F 6E /r	MOVQ xmm, r/m64	V	NE	Move quadword from r/m64 to xmm.
66 0F 7E /r	MOVD r/m32, xmm	V	V	Move doubleword from xmm register to r/m32.
66 REX.W+ 0F 7E /r	MOVQ r/m64, xmm	V	NE	Move quadword from xmm register to r/m64.
VEX.128.66.0F.W0 7E /r	VMOVD r32/m32, xmm1	V	V	Move doubleword from xmm1 register to r/m32.
VEX.128.66.0F.W1 7E /r	VMOVQ r64/m64, xmm1	V	NE	Move quadword from xmm1 register to r/m64.
				
F2 0F 12 /r 	MOVDDUP xmm1, xmm2/m64	V	V	Move one double-precision floating-point value from the lower 64-bit operand in xmm2/m64 to xmm1 and duplicate.
VEX.128.F2.0F.WIG 12 /r	VMOVDDUP xmm1, xmm2/m64	V	V	Move double-precision floating-point values from xmm2/mem and duplicate into xmm1.
VEX.256.F2.0F.WIG 12 /r	VMOVDDUP ymm1, ymm2/m256	V	V	Move even index double-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
				
66 0F 6F /r 	MOVDQA xmm1, xmm2/m128	V	V	Move aligned double quadword from xmm2/m128 to xmm1.
66 0F 7F /r	MOVDQA xmm2/m128, xmm1	V	V	Move aligned double quadword from xmm1 to xmm2/m128.
VEX.128.66.0F.WIG 6F /r	VMOVDQA xmm1, xmm2/m128	V	V	Move aligned packed integer values from xmm2/mem to xmm1.
VEX.128.66.0F.WIG 7F /r	VMOVDQA xmm2/m128, xmm1	V	V	Move aligned packed integer values from xmm1 to xmm2/mem.
VEX.256.66.0F.WIG 6F /r	VMOVDQA ymm1, ymm2/m256	V	V	Move aligned packed integer values from ymm2/mem to ymm1.
VEX.256.66.0F.WIG 7F /r	VMOVDQA ymm2/m256, ymm1	V	V	Move aligned packed integer values from ymm1 to ymm2/mem.
				
F3 0F 6F /r 	MOVDQU xmm1, xmm2/m128	V	V	Move unaligned double quadword from xmm2/m128 to xmm1.
F3 0F 7F /r	MOVDQU xmm2/m128, xmm1	V	V	Move unaligned double quadword from xmm1 to xmm2/m128.
VEX.128.F3.0F.WIG 6F /r	VMOVDQU xmm1, xmm2/m128	V	V	Move unaligned packed integer values from xmm2/mem to xmm1.
VEX.128.F3.0F.WIG 7F /r	VMOVDQU xmm2/m128, xmm1	V	V	Move unaligned packed integer values from xmm1 to xmm2/mem.
VEX.256.F3.0F.WIG 6F /r	VMOVDQU ymm1, ymm2/m256	V	V	Move unaligned packed integer values from ymm2/mem to ymm1.
VEX.256.F3.0F.WIG 7F /r	VMOVDQU ymm2/m256, ymm1	V	V	Move unaligned packed integer values from ymm1 to ymm2/mem.
				
F2 0F D6 	MOVDQ2Q mm, xmm 	V	V	Move low quadword from xmm to mmx register.
				
0F 12 /r 	MOVHLPS xmm1, xmm2	V	V	Move two packed single-precision floating-point values from high quadword of xmm2 to low quadword of xmm1.
VEX.NDS.128.0F.WIG 12 /r	VMOVHLPS xmm1, xmm2, xmm3	V	V	Merge two packed single-precision floating-point values from high quadword of xmm3 and low quadword of xmm2.
				
66 0F 16 /r 	MOVHPD xmm, m64	V	V	Move double-precision floating-point value from m64 to high quadword of xmm.
66 0F 17 /r	MOVHPD m64, xmm	V	V	Move double-precision floating-point value from high quadword of xmm to m64.
VEX.NDS.128.66.0F.WIG 16 /r	VMOVHPD xmm2, xmm1, m64	V	V	Merge double-precision floating-point value from m64 and the low quadword of xmm1.
VEX128.66.0F.WIG 17 /r	VMOVHPD m64, xmm1	V	V	Move double-precision floating-point values from high quadword of xmm1 to m64.
				
0F 16 /r 	MOVHPS xmm, m64	V	V	Move two packed single-precision floating-point values from m64 to high quadword of xmm.
0F 17 /r	MOVHPS m64, xmm	V	V	Move two packed single-precision floating-point values from high quadword of xmm to m64.
VEX.NDS.128.0F.WIG 16 /r	VMOVHPS xmm2, xmm1, m64	V	V	Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1.
VEX.128.0F.WIG 17 /r	VMOVHPS m64, xmm1	V	V	Move two packed single-precision floating-point values from high quadword of xmm1to m64.
				
0F 16 /r 	MOVLHPS xmm1, xmm2	V	V	Move two packed single-precision floating-point values from low quadword of xmm2 to high quadword of xmm1.
VEX.NDS.128.0F.WIG 16 /r	VMOVLHPS xmm1, xmm2, xmm3	V	V	Merge two packed single-precision floating-point values from low quadword of xmm3 and low quadword of xmm2.
				
66 0F 12 /r 	MOVLPD xmm, m64	V	V	Move double-precision floating-point value from m64 to low quadword of xmm register.
66 0F 13 /r	MOVLPD m64, xmm	V	V	Move double-precision floating-point nvalue from low quadword of xmm register to m64.
VEX.NDS.128.66.0F.WIG 12 /r	VMOVLPD xmm2, xmm1, m64	V	V	Merge double-precision floating-point value from m64 and the high quadword of xmm1.
VEX.128.66.0F.WIG 13 /r	VMOVLPD m64, xmm1	V	V	Move double-precision floating-point values from low quadword of xmm1 to m64.
				
0F 12 /r 	MOVLPS xmm, m64	V	V	Move two packed single-precision floating-point values from m64 to low quadword of xmm.
0F 13 /r	MOVLPS m64, xmm	V	V	Move two packed single-precision floating-point values from low quadword of xmm to m64.
VEX.NDS.128.0F.WIG 12 /r	VMOVLPS xmm2, xmm1, m64	V	V	Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1.
VEX.128.0F.WIG 13 /r	VMOVLPS m64, xmm1	V	V	Move two packed single-precision floating-point values from low quadword of xmm1 to m64.
				
66 0F 50 /r 	MOVMSKPD reg, xmm	V	V	Extract 2-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
VEX.128.66.0F.WIG 50 /r	VMOVMSKPD reg, xmm2	V	V	Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VEX.256.66.0F.WIG 50 /r	VMOVMSKPD reg, ymm2	V	V	Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
				
0F 50 /r 	MOVMSKPS reg, xmm	V	V	Extract 4-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
VEX.128.0F.WIG 50 /r	VMOVMSKPS reg, xmm2	V	V	Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VEX.256.0F.WIG 50 /r	VMOVMSKPS reg, ymm2	V	V	Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
				
66 0F 38 2A /r 	MOVNTDQA xmm1, m128	V	V	Move double quadword from m128 to xmm using non-temporal hint if WC memory type.
VEX.128.66.0F38.WIG 2A /r	VMOVNTDQA xmm1, m128	V	V	Move double quadword from m128 to xmm using non-temporal hint if WC memory type.
				
66 0F E7 /r 	MOVNTDQ m128, xmm	V	V	Move double quadword from xmm to m128 using non-temporal hint.
VEX.128.66.0F.WIG E7 /r	VMOVNTDQ m128, xmm1	V	V	Move packed integer values in xmm1 to m128 using non-temporal hint.
VEX.256.66.0F.WIG E7 /r	MOVNTDQ m256, ymm1	V	V	Move packed integer values in ymm1 to m256 using non-temporal hint.
				
0F C3 /r 	MOVNTI m32, r32 	V	V	Move doubleword from r32 to m32 using non-temporal hint.
REX.W+ 0F C3 /r 	MOVNTI m64, r64 	V	NE	Move quadword from r64 to m64 using non-temporal hint.
				
66 0F 2B /r 	MOVNTPD m128, xmm	V	V	Move packed double-precision floating-point values from xmm to m128 using non-temporal hint.
VEX.128.66.0F.WIG 2B /r	VMOVNTPD m128, xmm1	V	V	Move packed double-precision values in xmm1 to m128 using non-temporal hint.
VEX.256.66.0F.WIG 2B /r	VMOVNTPD m256, ymm1	V	V	Move packed double-precision values in ymm1 to m256 using non-temporal hint.
				
0F 2B /r 	MOVNTPS m128, xmm	V	V	Move packed single-precision floating-point values from xmm to m128 using non-temporal hint.
VEX.128.0F.WIG 2B /r	VMOVNTPS m128, xmm1	V	V	Move packed single-precision values xmm1 to mem using non-temporal hint.
VEX.256.0F.WIG 2B /r	VMOVNTPS m256, ymm1	V	V	Move packed single-precision values ymm1 to mem using non-temporal hint.
				
0F E7 /r 	MOVNTQ m64, mm 	V	V	Move quadword from mm to m64 using non-temporal hint.
				
0F 6F /r 	MOVQ mm, mm/m64	V	V	Move quadword from mm/m64 to mm.
0F 7F /r	MOVQ mm/m64, mm	V	V	Move quadword from mm to mm/m64.
F3 0F 7E	MOVQ xmm1, xmm2/m64	V	V	Move quadword from xmm2/mem64 to xmm1.
VEX.128.F3.0F.WIG 7E /r	VMOVQ xmm1, xmm2	V	V	Move quadword from xmm2 to xmm1.
VEX.128.F3.0F.WIG 7E /r	VMOVQ xmm1, m64	V	V	Load quadword from m64 to xmm1.
66 0F D6	MOVQ xmm2/m64, xmm1	V	V	Move quadword from xmm1 to xmm2/mem64.
VEX.128.66.0F.WIG D6 /r	VMOVQ xmm1/m64, xmm2	V	V	Move quadword from xmm2 register to xmm1/m64.
				
F3 0F D6 	MOVQ2DQ xmm, mm 	V	V	Move quadword from mmx to low quadword of xmm.
				
A4 	MOVS m8, m8 	V	V	For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
A5 	MOVS m16, m16 	V	V	For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
A5 	MOVS m32, m32 	V	V	For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
REX.W+ A5 	MOVS m64, m64 	V	NE	Move qword from address (R|E)SI to (R|E)DI.
A4 	MOVSB 	V	V	For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
PREF.66+ A5 	MOVSW 	V	V	For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
A5 	MOVSD 	V	V	For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
REX.W+ A5 	MOVSQ 	V	NE	Move qword from address (R|E)SI to (R|E)DI.
				
F2 0F 10 /r 	MOVSD xmm1, xmm2/m64	V	V	Move scalar double-precision floating-point value from xmm2/m64 to xmm1 register.
VEX.NDS.LIG.F2.0F.WIG 10 /r	VMOVSD xmm1, xmm2, xmm3	V	V	Merge scalar double-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VEX.LIG.F2.0F.WIG 10 /r	VMOVSD xmm1, m64	V	V	Load scalar double-precision floating-point value from m64 to xmm1 register.
F2 0F 11 /r	MOVSD xmm2/m64, xmm1	V	V	Move scalar double-precision floating-point value from xmm1 register to xmm2/m64.
VEX.NDS.LIG.F2.0F.WIG 11 /r	VMOVSD xmm1, xmm2, xmm3	V	V	Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1.
VEX.LIG.F2.0F.WIG 11 /r	VMOVSD m64, xmm1	V	V	Move scalar double-precision floating-point value from xmm1 register to m64.
				
F3 0F 16 /r 	MOVSHDUP xmm1, xmm2/m128	V	V	Move two single-precision floating-point values from the higher 32-bit operand of each qword in xmm2/m128 to xmm1 and duplicate each 32-bit operand to the lower 32-bits of each qword.
VEX.128.F3.0F.WIG 16 /r	VMOVSHDUP xmm1, xmm2/m128	V	V	Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
VEX.256.F3.0F.WIG 16 /r	VMOVSHDUP ymm1, ymm2/m256	V	V	Move odd index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
				
F3 0F 12 /r 	MOVSLDUP xmm1, xmm2/m128	V	V	Move two single-precision floating-point values from the lower 32-bit operand of each qword in xmm2/m128 to xmm1 and duplicate each 32-bit operand to the higher 32-bits of each qword.
VEX.128.F3.0F.WIG 12 /r	VMOVSLDUP xmm1, xmm2/m128	V	V	Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
VEX.256.F3.0F.WIG 12 /r	VMOVSLDUP ymm1, ymm2/m256	V	V	Move even index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
				
F3 0F 10 /r 	MOVSS xmm1, xmm2/m32	V	V	Move scalar single-precision floating-point value from xmm2/m32 to xmm1 register.
VEX.NDS.LIG.F3.0F.WIG 10 /r	VMOVSS xmm1, xmm2, xmm3	V	V	Merge scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VEX.LIG.F3.0F.WIG 10 /r	VMOVSS xmm1, m32	V	V	Load scalar single-precision floating-point value from m32 to xmm1 register.
F3 0F 11 /r	MOVSS xmm2/m32, xmm	V	V	Move scalar single-precision floating-point value from xmm1 register to xmm2/m32.
VEX.NDS.LIG.F3.0F.WIG 11 /r	VMOVSS xmm1, xmm2, xmm3	V	V	Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VEX.LIG.F3.0F.WIG 11 /r	VMOVSS m32, xmm1	V	V	Move scalar single-precision floating-point value from xmm1 register to m32.
				
0F BE /r 	MOVSX r16, r/m8 	V	V	Move byte to word with sign-extension.
0F BE /r 	MOVSX r32, r/m8 	V	V	Move byte to doubleword with sign-extension.
REX+ 0F BE /r 	MOVSX r64, r/m8	V	NE	Move byte to quadword with sign-extension.
0F BF /r 	MOVSX r32, r/m16 	V	V	Move word to doubleword, with sign-extension.
REX.W+ 0F BF /r 	MOVSX r64, r/m16 	V	NE	Move word to quadword with sign-extension.
REX.W+ 63 /r 	MOVSXD r64, r/m32 	V	NE	Move doubleword to quadword with sign-extension.
				
66 0F 10 /r 	MOVUPD xmm1, xmm2/m128	V	V	Move packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.128.66.0F.WIG 10 /r	VMOVUPD xmm1, xmm2/m128	V	V	Move unaligned packed double-precision floating-point from xmm2/mem to xmm1.
VEX.256.66.0F.WIG 10 /r	VMOVUPD ymm1, ymm2/m256	V	V	Move unaligned packed double-precision floating-point from ymm2/mem to ymm1.
66 0F 11 /r	MOVUPD xmm2/m128, xmm	V	V	Move packed double-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.66.0F.WIG 11 /r	VMOVUPD xmm2/m128, xmm1	V	V	Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem.
VEX.256.66.0F.WIG 11 /r	VMOVUPD ymm2/m256, ymm1	V	V	Move unaligned packed double-precision floating-point from ymm1 to ymm2/mem.
				
0F 10 /r 	MOVUPS xmm1, xmm2/m128	V	V	Move packed single-precision floating-point values from xmm2/m128 to xmm1.
VEX.128.0F.WIG 10 /r	VMOVUPS xmm1, xmm2/m128	V	V	Move unaligned packed single-precision floating-point from xmm2/mem to xmm1.
VEX.256.0F.WIG 10 /r	VMOVUPS ymm1, ymm2/m256	V	V	Move unaligned packed single-precision floating-point from ymm2/mem to ymm1.
0F 11 /r	MOVUPS xmm2/m128, xmm1	V	V	Move packed single-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.0F.WIG 11 /r	VMOVUPS xmm2/m128, xmm1	V	V	Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem.
VEX.256.0F.WIG 11 /r	VMOVUPS ymm2/m256, ymm1	V	V	Move unaligned packed single-precision floating-point from ymm1 to ymm2/mem.
				
0F B6 /r 	MOVZX r16, r/m8 	V	V	Move byte to word with zero-extension.
0F B6 /r 	MOVZX r32, r/m8 	V	V	Move byte to doubleword, zero-extension.
REX.W+ 0F B6 /r 	MOVZX r64, r/m8	V	NE	Move byte to quadword, zero-extension.
0F B7 /r 	MOVZX r32, r/m16 	V	V	Move word to doubleword, zero-extension.
REX.W+ 0F B7 /r 	MOVZX r64, r/m16 	V	NE	Move word to quadword, zero-extension.
				
66 0F 3A 42 /r ib 	MPSADBW xmm1, xmm2/m128, imm8	V	V	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm1 and xmm2/m128 and writes the results in xmm1. Starting offsets within xmm1 and xmm2/m128 are determined by imm8.
VEX.NDS.128.66.0F3A.WIG 42 /r ib	VMPSADBW xmm1, xmm2, xmm3/m128, imm8	V	V	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by imm8.
				
F6 /4 	MUL r/m8 	V	V	Unsigned multiply (AX = AL ∗ r/m8).
REX+ F6 /4 	MUL r/m8	V	NE	Unsigned multiply (AX = AL ∗ r/m8).
F7 /4 	MUL r/m16 	V	V	Unsigned multiply (DX:AX = AX ∗ r/m16).
F7 /4 	MUL r/m32 	V	V	Unsigned multiply (EDX:EAX = EAX ∗ r/m32).
REX.W+ F7 /4 	MUL r/m64 	V	NE	Unsigned multiply (RDX:RAX = RAX ∗ r/m64.
				
66 0F 59 /r 	MULPD xmm1, xmm2/m128	V	V	Multiply packed double-precision floating-point values in xmm2/m128 by xmm1.
VEX.NDS.128.66.0F.WIG 59 /r	VMULPD xmm1, xmm2, xmm3/m128	V	V	Multiply packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG 59 /r	VMULPD ymm1, ymm2, ymm3/m256	V	V	Multiply packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
				
0F 59 /r 	MULPS xmm1, xmm2/m128	V	V	Multiply packed single-precision floating-point values in xmm2/mem by xmm1.
VEX.NDS.128.0F.WIG 59 /r	VMULPS xmm1, xmm2, xmm3/m128	V	V	Multiply packed single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.0F.WIG 59 /r	VMULPS ymm1, ymm2, ymm3/m256	V	V	Multiply packed single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
				
F2 0F 59 /r 	MULSD xmm1, xmm2/m64	V	V	Multiply the low double-precision floating-point value in xmm2/mem64 by low double-precision floating-point value in xmm1.
VEX.NDS.LIG.F2.0F.WIG 59 /r	VMULSD xmm1,xmm2, xmm3/m64	V	V	Multiply the low double-precision floating-point value in xmm3/mem64 by low double precision floating-point value in xmm2.
				
F3 0F 59 /r 	MULSS xmm1, xmm2/m32	V	V	Multiply the low single-precision floating-point value in xmm2/mem by the low single-precision floating-point value in xmm1.
VEX.NDS.LIG.F3.0F.WIG 59 /r	VMULSS xmm1,xmm2, xmm3/m32	V	V	Multiply the low single-precision floating-point value in xmm3/mem by the low single-precision floating-point value in xmm2.
				
0F 01 C9 	MWAIT 	V	V	A hint that allow the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.
				
F6 /3 	NEG r/m8 	V	V	Two's complement negate r/m8.
REX+ F6 /3 	NEG r/m8	V	NE	Two's complement negate r/m8.
F7 /3 	NEG r/m16 	V	V	Two's complement negate r/m16.
F7 /3 	NEG r/m32 	V	V	Two's complement negate r/m32.
REX.W+ F7 /3 	NEG r/m64 	V	NE	Two's complement negate r/m64.
				
90	NOP 	V	V	One byte no-operation instruction.
0F 1F /0 	NOP r/m16 	V	V	Multi-byte no-operation instruction.
0F 1F /0 	NOP r/m32 	V	V	Multi-byte no-operation instruction.
				
F6 /2 	NOT r/m8 	V	V	Reverse each bit of r/m8.
REX+ F6 /2 	NOT r/m8	V	NE	Reverse each bit of r/m8.
F7 /2 	NOT r/m16 	V	V	Reverse each bit of r/m16.
F7 /2 	NOT r/m32 	V	V	Reverse each bit of r/m32.
REX.W+ F7 /2 	NOT r/m64 	V	NE	Reverse each bit of r/m64.
				
0C ib 	OR AL, imm8 	V	V	AL OR imm8.
0D iw 	OR AX, imm16 	V	V	AX OR imm16.
0D id 	OR EAX, imm32 	V	V	EAX OR imm32.
REX.W+ 0D id 	OR RAX, imm32 	V	NE	RAX OR imm32 (sign-extended).
80 /1 ib 	OR r/m8, imm8 	V	V	r/m8 OR imm8.
REX+ 80 /1 ib 	OR r/m8, imm8 	V	NE	r/m8 OR imm8.
81 /1 iw 	OR r/m16, imm16 	V	V	r/m16 OR imm16.
81 /1 id 	OR r/m32, imm32 	V	V	r/m32 OR imm32.
REX.W+ 81 /1 id 	OR r/m64, imm32 	V	NE	r/m64 OR imm32 (sign-extended).
83 /1 ib 	OR r/m16, imm8 	V	V	r/m16 OR imm8 (sign-extended).
83 /1 ib 	OR r/m32, imm8 	V	V	r/m32 OR imm8 (sign-extended).
REX.W+ 83 /1 ib 	OR r/m64, imm8 	V	NE	r/m64 OR imm8 (sign-extended).
08 /r 	OR r/m8, r8 	V	V	r/m8 OR r8.
REX+ 08 /r 	OR r/m8, r8	V	NE	r/m8 OR r8.
09 /r 	OR r/m16, r16 	V	V	r/m16 OR r16.
09 /r 	OR r/m32, r32 	V	V	r/m32 OR r32.
REX.W+ 09 /r 	OR r/m64, r64 	V	NE	r/m64 OR r64.
0A /r 	OR r8, r/m8 	V	V	r8 OR r/m8.
REX+ 0A /r 	OR r8, r/m8	V	NE	r8 OR r/m8.
0B /r 	OR r16, r/m16 	V	V	r16 OR r/m16.
0B /r 	OR r32, r/m32 	V	V	r32 OR r/m32.
REX.W+ 0B /r 	OR r64, r/m64 	V	NE	r64 OR r/m64.
				
66 0F 56 /r 	ORPD xmm1, xmm2/m128	V	V	Bitwise OR of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 56 /r 	VORPD xmm1, xmm2, xmm3/m128 	V	V	Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 56 /r 	VORPD ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/mem.
				
0F 56 /r 	ORPS xmm1, xmm2/m128	V	V	Bitwise OR of xmm1 and xmm2/m128.
VEX.NDS.128.0F.WIG 56 /r	VORPS xmm1, xmm2, xmm3/m128	V	V	Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 56 /r	VORPS ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/mem.
				
E6 ib 	OUT imm8, AL 	V	V	Output byte in AL to I/O port address imm8.
E7 ib 	OUT imm8, AX 	V	V	Output word in AX to I/O port address imm8.
E7 ib 	OUT imm8, EAX 	V	V	Output doubleword in EAX to I/O port address imm8.
EE 	OUT DX, AL 	V	V	Output byte in AL to I/O port address in DX.
EF 	OUT DX, AX 	V	V	Output word in AX to I/O port address in DX.
EF 	OUT DX, EAX 	V	V	Output doubleword in EAX to I/O port address in DX.
				
6E 	OUTS DX, m8 	V	V	Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6F 	OUTS DX, m16 	V	V	Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6F 	OUTS DX, m32 	V	V	Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6E 	OUTSB 	V	V	Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
PREF.66+ 6F 	OUTSW 	V	V	Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6F 	OUTSD 	V	V	Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
				
0F 38 1C /r 	PABSB mm1, mm2/m64	V	V	Compute the absolute value of bytes in mm2/m64 and store UNSIGNED result in mm1.
66 0F 38 1C /r	PABSB xmm1, xmm2/m128	V	V	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
0F 38 1D /r	PABSW mm1, mm2/m64	V	V	Compute the absolute value of 16-bit integers in mm2/m64 and store UNSIGNED result in mm1.
66 0F 38 1D /r	PABSW xmm1, xmm2/m128	V	V	Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
0F 38 1E /r	PABSD mm1, mm2/m64	V	V	Compute the absolute value of 32-bit integers in mm2/m64 and store UNSIGNED result in mm1.
66 0F 38 1E /r	PABSD xmm1, xmm2/m128	V	V	Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.128.66.0F38.WIG 1C /r	VPABSB xmm1, xmm2/m128	V	V	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.128.66.0F38.WIG 1D /r	VPABSW xmm1, xmm2/m128	V	V	Compute the absolute value of 16- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.128.66.0F38.WIG 1E /r	VPABSD xmm1, xmm2/m128	V	V	Compute the absolute value of 32- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
				
0F 63 /r 	PACKSSWB mm1, mm2/m64	V	V	Converts 4 packed signed word integers from mm1 and from mm2/m64 into 8 packed signed byte integers in mm1 using signed saturation.
66 0F 63 /r	PACKSSWB xmm1, xmm2/m128	V	V	Converts 8 packed signed word integers from xmm1 and from xxm2/m128 into 16 packed signed byte integers in xxm1 using signed saturation.
0F 6B /r	PACKSSDW mm1, mm2/m64	V	V	Converts 2 packed signed doubleword integers from mm1 and from mm2/m64 into 4 packed signed word integers in mm1 using signed saturation.
66 0F 6B /r	PACKSSDW xmm1, xmm2/m128	V	V	Converts 4 packed signed doubleword integers from xmm1 and from xxm2/m128 into 8 packed signed word integers in xxm1 using signed saturation.
VEX.NDS.128.66.0F.WIG 63 /r	VPACKSSWB xmm1, xmm2, xmm3/m128	V	V	Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation.
VEX.NDS.128.66.0F.WIG 6B /r	VPACKSSDW xmm1, xmm2, xmm3/m128	V	V	Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation.
				
66 0F 38 2B /r 	PACKUSDW xmm1, xmm2/m128 	V	V	Convert 4 packed signed doubleword integers from xmm1 and 4 packed signed doubleword integers from xmm2/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
VEX.NDS.128.66.0F38.WIG 2B /r 	PACKUSDW xmm1, xmm2, xmm3/m128 	V	V	Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
				
0F 67 /r 	PACKUSWB mm, mm/m64	V	V	Converts 4 signed word integers from mm and 4 signed word integers from mm/m64 into 8 unsigned byte integers in mm using unsigned saturation.
66 0F 67 /r	PACKUSWB xmm1, xmm2/m128	V	V	Converts 8 signed word integers from xmm1 and 8 signed word integers from xmm2/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
VEX.NDS.128.66.0F.WIG 67 /r	VPACKUSWB xmm1, xmm2, xmm3/m128	V	V	Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
				
0F FC /r 	PADDB mm, mm/m64	V	V	Add packed byte integers from mm/m64 and mm.
66 0F FC /r	PADDB xmm1, xmm2/m128	V	V	Add packed byte integers from xmm2/m128 and xmm1.
0F FD /r	PADDW mm, mm/m64	V	V	Add packed word integers from mm/m64 and mm.
66 0F FD /r	PADDW xmm1, xmm2/m128	V	V	Add packed word integers from xmm2/m128 and xmm1.
0F FE /r	PADDD mm, mm/m64	V	V	Add packed doubleword integers from mm/m64 and mm.
66 0F FE /r	PADDD xmm1, xmm2/m128	V	V	Add packed doubleword integers from xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG FC /r	VPADDB xmm1, xmm2, xmm3/m128	V	V	Add packed byte integers from xmm3/m128 and xmm2.
VEX.NDS.128.66.0F.WIG FD /r	VPADDW xmm1, xmm2, xmm3/m128	V	V	Add packed word integers from xmm3/m128 and xmm2.
VEX.NDS.128.66.0F.WIG FE /r	VPADDD xmm1, xmm2, xmm3/m128	V	V	Add packed doubleword integers from xmm3/m128 and xmm2.
				
0F D4 /r 	PADDQ mm1, mm2/m64	V	V	Add quadword integer mm2/m64 to mm1.
66 0F D4 /r	PADDQ xmm1, xmm2/m128	V	V	Add packed quadword integers xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG D4 /r	VPADDQ xmm1, xmm2, xmm3/m128	V	V	Add packed quadword integers xmm3/m128 and xmm2.
				
0F EC /r 	PADDSB mm, mm/m64	V	V	Add packed signed byte integers from mm/m64 and mm and saturate the results.
66 0F EC /r	PADDSB xmm1, xmm2/m128	V	V	Add packed signed byte integers from xmm2/m128 and xmm1 saturate the results.
0F ED /r	PADDSW mm, mm/m64	V	V	Add packed signed word integers from mm/m64 and mm and saturate the results.
66 0F ED /r	PADDSW xmm1, xmm2/m128	V	V	Add packed signed word integers from xmm2/m128 and xmm1 and saturate the results.
VEX.NDS.128.66.0F.WIG EC /r 	VPADDSB xmm1, xmm2, xmm3/m128 	V	V	Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results.
VEX.NDS.128.66.0F.WIG ED /r 	VPADDSW xmm1, xmm2, xmm3/m128	V	V	Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results.
				
0F DC /r 	PADDUSB mm, mm/m64	V	V	Add packed unsigned byte integers from mm/m64 and mm and saturate the results.
66 0F DC /r	PADDUSB xmm1, xmm2/m128	V	V	Add packed unsigned byte integers from xmm2/m128 and xmm1 saturate the results.
0F DD /r	PADDUSW mm, mm/m64	V	V	Add packed unsigned word integers from mm/m64 and mm and saturate the results.
66 0F DD /r	PADDUSW xmm1, xmm2/m128	V	V	Add packed unsigned word integers from xmm2/m128 to xmm1 and saturate the results.
VEX.NDS.128.660F.WIG DC /r	VPADDUSB xmm1, xmm2, xmm3/m128	V	V	Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results.
VEX.NDS.128.66.0F.WIG DD /r	VPADDUSW xmm1, xmm2, xmm3/m128	V	V	Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results.
				
0F 3A 0F 	PALIGNR mm1, mm2/m64, imm8	V	V	Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into mm1.
66 0F 3A 0F	PALIGNR xmm1, xmm2/m128, imm8	V	V	Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into xmm1
VEX.NDS.128.66.0F3A.WIG 0F /r ib	VPALIGNR xmm1, xmm2, xmm3/m128, imm8	V	V	Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1.
				
0F DB /r 	PAND mm, mm/m64	V	V	Bitwise AND mm/m64 and mm.
66 0F DB /r	PAND xmm1, xmm2/m128	V	V	Bitwise AND of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG DB /r	VPAND xmm1, xmm2, xmm3/m128	V	V	Bitwise AND of xmm3/m128 and xmm.
				
0F DF /r 	PANDN mm, mm/m64	V	V	Bitwise AND NOT of mm/m64 and mm.
66 0F DF /r	PANDN xmm1, xmm2/m128	V	V	Bitwise AND NOT of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG DF /r	VPANDN xmm1, xmm2, xmm3/m128	V	V	Bitwise AND NOT of xmm3/m128 and xmm2.
				
F3 90 	PAUSE 	V	V	Gives hint to processor that improves performance of spin-wait loops.
				
0F E0 /r 	PAVGB mm1, mm2/m64	V	V	Average packed unsigned byte integers from mm2/m64 and mm1 with rounding.
66 0F E0, /r	PAVGB xmm1, xmm2/m128	V	V	Average packed unsigned byte integers from xmm2/m128 and xmm1 with rounding.
0F E3 /r	PAVGW mm1, mm2/m64	V	V	Average packed unsigned word integers from mm2/m64 and mm1 with rounding.
66 0F E3 /r	PAVGW xmm1, xmm2/m128	V	V	Average packed unsigned word integers from xmm2/m128 and xmm1 with rounding.
VEX.NDS.128.66.0F.WIG E0 /r	VPAVGB xmm1, xmm2, xmm3/m128	V	V	Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding.
VEX.NDS.128.66.0F.WIG E3 /r	VPAVGW xmm1, xmm2, xmm3/m128	V	V	Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding.
				
66 0F 38 10 /r 	PBLENDVB xmm1, xmm2/m128, <XMM0> 	V	V	Select byte values from xmm1 and xmm2/m128 from mask specified in the high bit of each byte in XMM0 and store the values into xmm1.
VEX.NDS.128.66.0F3A.W0 4C /r /is4 	VPBLENDVB xmm1, xmm2, xmm3/m128, xmm4 	V	V	Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1.
				
66 0F 3A 0E /r ib 	PBLENDW xmm1, xmm2/m128, imm8 	V	V	Select words from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.128.66.0F3A.WIG 0E /r ib 	VPBLENDW xmm1, xmm2, xmm3/m128, imm8 	V	V	Select words from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
				
66 0F 3A 44 /r ib 	PCLMULQDQ xmm1, xmm2/m128, imm8 	V	V	Carry-less multiplication of one quadword of xmm1 by one quadword of xmm2/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm1 and xmm2/m128 should be used.
VEX.NDS.128.66.0F3A.WIG 44 /r ib 	VPCLMULQDQ xmm1, xmm2, xmm3/m128, imm8 	V	V	Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm2 and xmm3/m128 should be used.
				
0F 74 /r 	PCMPEQB mm, mm/m64	V	V	Compare packed bytes in mm/m64 and mm for equality.
66 0F 74 /r	PCMPEQB xmm1, xmm2/m128	V	V	Compare packed bytes in xmm2/m128 and xmm1 for equality.
0F 75 /r	PCMPEQW mm, mm/m64	V	V	Compare packed words in mm/m64 and mm for equality.
66 0F 75 /r	PCMPEQW xmm1, xmm2/m128	V	V	Compare packed words in xmm2/m128 and xmm1 for equality.
0F 76 /r	PCMPEQD mm, mm/m64	V	V	Compare packed doublewords in mm/m64 and mm for equality.
66 0F 76 /r	PCMPEQD xmm1, xmm2/m128	V	V	Compare packed doublewords in xmm2/m128 and xmm1 for equality.
VEX.NDS.128.66.0F.WIG 74 /r	VPCMPEQB xmm1, xmm2, xmm3/m128	V	V	Compare packed bytes in xmm3/m128 and xmm2 for equality.
VEX.NDS.128.66.0F.WIG 75 /r	VPCMPEQW xmm1, xmm2, xmm3/m128	V	V	Compare packed words in xmm3/m128 and xmm2 for equality.
VEX.NDS.128.66.0F.WIG 76 /r	VPCMPEQD xmm1, xmm2, xmm3/m128	V	V	Compare packed doublewords in xmm3/m128 and xmm2 for equality.
				
66 0F 38 29 /r 	PCMPEQQ xmm1, xmm2/m128 	V	V	Compare packed qwords in xmm2/m128 and xmm1 for equality.
VEX.NDS.128.66.0F38.WIG 29 /r 	VPCMPEQQ xmm1, xmm2, xmm3/m128 	V	V	Compare packed quadwords in xmm3/m128 and xmm2 for equality.
				
66 0F 3A 61 /r	PCMPESTRI xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
VEX.128.66.0F3A.WIG 61 /r ib 	VPCMPESTRI xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
				
66 0F 3A 60 /r	PCMPESTRM xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0
VEX.128.66.0F3A.WIG 60 /r ib 	VPCMPESTRM xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0.
				
0F 64 /r 	PCMPGTB mm, mm/m64	V	V	Compare packed signed byte integers in mm and mm/m64 for greater than.
66 0F 64 /r	PCMPGTB xmm1, xmm2/m128	V	V	Compare packed signed byte integers in xmm1 and xmm2/m128 for greater than.
0F 65 /r	PCMPGTW mm, mm/m64	V	V	Compare packed signed word integers in mm and mm/m64 for greater than.
66 0F 65 /r	PCMPGTW xmm1, xmm2/m128	V	V	Compare packed signed word integers in xmm1 and xmm2/m128 for greater than.
0F 66 /r	PCMPGTD mm, mm/m64	V	V	Compare packed signed doubleword integers in mm and mm/m64 for greater than.
66 0F 66 /r	PCMPGTD xmm1, xmm2/m128	V	V	Compare packed signed doubleword integers in xmm1 and xmm2/m128 for greater than.
VEX.NDS.128.66.0F.WIG 64 /r	VPCMPGTB xmm1, xmm2, xmm3/m128	V	V	Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than.
VEX.NDS.128.66.0F.WIG 65 /r	VPCMPGTW xmm1, xmm2, xmm3/m128	V	V	Compare packed signed word integers in xmm2 and xmm3/m128 for greater than.
VEX.NDS.128.66.0F.WIG 66 /r	VPCMPGTD xmm1, xmm2, xmm3/m128	V	V	Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than.
				
66 0F 38 37 /r 	PCMPGTQ xmm1, xmm2/m128 	V	V	Compare packed signed qwords in xmm2/m128 and xmm1 for greater than.
VEX.NDS.128.66.0F38.WIG 37 /r 	VPCMPGTQ xmm1, xmm2, xmm3/m128 	V	V	Compare packed signed qwords in xmm2 and xmm3/m128 for greater than.
				
66 0F 3A 63 /r	PCMPISTRI xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
VEX.128.66.0F3A.WIG 63 /r ib 	VPCMPISTRI xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
				
66 0F 3A 62 /r	PCMPISTRM xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with implicit lengths, generating a mask, and storing the result in XMM0.
VEX.128.66.0F3A.WIG 62 /r ib 	VPCMPISTRM xmm1, xmm2/m128, imm8 	V	V	Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0.
				
66 0F 3A 14 /r ib 	PEXTRB reg/m8, xmm2, imm8 	V	V	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into rreg or m8. The upper bits of r32 or r64 are zeroed.
66 0F 3A 16 /r ib 	PEXTRD r/m32, xmm2, imm8 	V	V	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r/m32.
66 REX.W+ 0F 3A 16 /r ib 	PEXTRQ r/m64, xmm2, imm8 	V	NE	Extract a qword integer value from xmm2 at the source qword offset specified by imm8 into r/m64.
VEX.128.66.0F3A.W0 14 /r ib 	VPEXTRB reg/m8, xmm2, imm8 	V	V	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with
VEX.128.66.0F3A.W0 16 /r ib 	VPEXTRD r32/m32, xmm2, imm8 	V	V	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
VEX.128.66.0F3A.W1 16 /r ib 	VPEXTRQ r64/m64, xmm2, imm8 	V	I	Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
				
0F C5 /r ib 	PEXTRW reg, mm, imm8	V	V	Extract the word specified by imm8 from mm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
66 0F C5 /r ib	PEXTRW reg, xmm, imm8	V	V	Extract the word specified by imm8 from xmm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
66 0F 3A 15 /r ib 	PEXTRW reg/m16, xmm, imm8 	V	V	Extract the word specified by imm8 from xmm and copy it to lowest 16 bits of reg or m16. Zero-extend the result in the destination, r32 or r64.
VEX.128.66.0F.W0 C5 /r ib 	VPEXTRW reg, xmm1, imm8 	V	V	Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros.
VEX.128.66.0F3A.W0 15 /r ib 	VPEXTRW reg/m16, xmm2, imm8 	V	V	Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros.
				
0F 38 01 /r 	PHADDW mm1, mm2/m64	V	V	Add 16-bit integers horizontally, pack to MM1.
66 0F 38 01 /r	PHADDW xmm1, xmm2/m128	V	V	Add 16-bit integers horizontally, pack to XMM1.
0F 38 02 /r	PHADDD mm1, mm2/m64	V	V	Add 32-bit integers horizontally, pack to MM1.
66 0F 38 02 /r	PHADDD xmm1, xmm2/m128	V	V	Add 32-bit integers horizontally, pack to XMM1.
VEX.NDS.128.66.0F38.WIG 01 /r	VPHADDW xmm1, xmm2, xmm3/m128	V	V	Add 16-bit integers horizontally, pack to xmm1.
VEX.NDS.128.66.0F38.WIG 02 /r	VPHADDD xmm1, xmm2, xmm3/m128	V	V	Add 32-bit integers horizontally, pack to xmm1.
				
0F 38 03 /r 	PHADDSW mm1, mm2/m64	V	V	Add 16-bit signed integers horizontally, pack saturated integers to MM1.
66 0F 38 03 /r	PHADDSW xmm1, xmm2/m128	V	V	Add 16-bit signed integers horizontally, pack saturated integers to XMM1.
VEX.NDS.128.66.0F38.WIG 03 /r	VPHADDSW xmm1, xmm2, xmm3/m128	V	V	Add 16-bit signed integers horizontally, pack saturated integers to xmm1.
				
66 0F 38 41 /r 	PHMINPOSUW xmm1, xmm2/m128 	V	V	Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1.
VEX.128.66.0F38.WIG 41 /r 	VPHMINPOSUW xmm1, xmm2/m128 	V	V	Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1.
				
0F 38 05 /r 	PHSUBW mm1, mm2/m64	V	V	Subtract 16-bit signed integers horizontally, pack to MM1.
66 0F 38 05 /r	PHSUBW xmm1, xmm2/m128	V	V	Subtract 16-bit signed integers horizontally, pack to XMM1.
0F 38 06 /r	PHSUBD mm1, mm2/m64	V	V	Subtract 32-bit signed integers horizontally, pack to MM1.
66 0F 38 06 /r	PHSUBD xmm1, xmm2/m128	V	V	Subtract 32-bit signed integers horizontally, pack to XMM1
VEX.NDS.128.66.0F38.WIG 05 /r	VPHSUBW xmm1, xmm2, xmm3/m128	V	V	Subtract 16-bit signed integers horizontally, pack to xmm1.
VEX.NDS.128.66.0F38.WIG 06 /r	VPHSUBD xmm1, xmm2, xmm3/m128	V	V	Subtract 32-bit signed integers horizontally, pack to xmm1.
				
0F 38 07 /r 	PHSUBSW mm1, mm2/m64	V	V	Subtract 16-bit signed integer horizontally, pack saturated integers to MM1.
66 0F 38 07 /r	PHSUBSW xmm1, xmm2/m128	V	V	Subtract 16-bit signed integer horizontally, pack saturated integers to XMM1
VEX.NDS.128.66.0F38.WIG 07 /r	VPHSUBSW xmm1, xmm2, xmm3/m128	V	V	Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1.
				
66 0F 3A 20 /r ib 	PINSRB xmm1, r32/m8, imm8 	V	V	Insert a byte integer value from r32/m8 into xmm1 at the destination element in xmm1 specified by imm8.
66 0F 3A 22 /r ib 	PINSRD xmm1, r/m32, imm8 	V	V	Insert a dword integer value from r/m32 into the xmm1 at the destination element specified by imm8.
66 REX.W+ 0F 3A 22 /r ib 	PINSRQ xmm1, r/m64, imm8 	NE	V	Insert a qword integer value from r/m64 into the xmm1 at the destination element specified by imm8.
VEX.NDS.128.66.0F3A.W0 20 /r ib 	VPINSRB xmm1, xmm2, r32/m8, imm8 	V	V	Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
VEX.NDS.128.66.0F3A.W0 22 /r ib 	VPINSRD xmm1, xmm2, r32/m32, imm8 	V	V	Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
VEX.NDS.128.66.0F3A.W1 22 /r ib 	VPINSRQ xmm1, xmm2, r64/m64, imm8 	V	I	Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
				
0F C4 /r ib 	PINSRW mm, r32/m16, imm8	V	V	Insert the low word from r32 or from m16 into mm at the word position specified by imm8
66 0F C4 /r ib	PINSRW xmm, r32/m16, imm8	V	V	Move the low word of r32 or from m16 into xmm at the word position specified by imm8.
VEX.NDS.128.66.0F.W0 C4 /r ib	VPINSRW xmm1, xmm2, r32/m16, imm8	V	V	Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8.
				
0F 38 04 /r 	PMADDUBSW mm1, mm2/m64	V	V	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to MM1.
66 0F 38 04 /r	PMADDUBSW xmm1, xmm2/m128	V	V	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to XMM1.
VEX.NDS.128.66.0F38.WIG 04 /r	VPMADDUBSW xmm1, xmm2, xmm3/m128	V	V	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1.
				
0F F5 /r 	PMADDWD mm, mm/m64	V	V	Multiply the packed words in mm by the packed words in mm/m64, add adjacent doubleword results, and store in mm.
66 0F F5 /r	PMADDWD xmm1, xmm2/m128	V	V	Multiply the packed word integers in xmm1 by the packed word integers in xmm2/m128, add adjacent doubleword results, and store in xmm1.
VEX.NDS.128.66.0F.WIG F5 /r	VPMADDWD xmm1, xmm2, xmm3/m128	V	V	Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1.
				
66 0F 38 3C /r 	PMAXSB xmm1, xmm2/m128 	V	V	Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3C /r 	VPMAXSB xmm1, xmm2, xmm3/m128 	V	V	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
				
66 0F 38 3D /r 	PMAXSD xmm1, xmm2/m128 	V	V	Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3D /r 	VPMAXSD xmm1, xmm2, xmm3/m128 	V	V	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
				
0F EE /r 	PMAXSW mm1, mm2/m64	V	V	Compare signed word integers in mm2/m64 and mm1 and return maximum values.
66 0F EE /r	PMAXSW xmm1, xmm2/m128	V	V	Compare signed word integers in xmm2/m128 and xmm1 and return maximum values.
VEX.NDS.128.66.0F.WIG EE /r	VPMAXSW xmm1, xmm2, xmm3/m128	V	V	Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1.
				
0F DE /r 	PMAXUB mm1, mm2/m64	V	V	Compare unsigned byte integers in mm2/m64 and mm1 and returns maximum values.
66 0F DE /r	PMAXUB xmm1, xmm2/m128	V	V	Compare unsigned byte integers in xmm2/m128 and xmm1 and returns maximum values.
VEX.NDS.128.66.0F.WIG DE /r	VPMAXUB xmm1, xmm2, xmm3/m128	V	V	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
				
66 0F 38 3F /r 	PMAXUD xmm1, xmm2/m128 	V	V	Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3F /r 	VPMAXUD xmm1, xmm2, xmm3/m128 	V	V	Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
				
66 0F 38 3E /r 	PMAXUW xmm1, xmm2/m128 	V	V	Compare packed unsigned word integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3E /r 	VPMAXUW xmm1, xmm2, xmm3/m128 	V	V	Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1.
				
66 0F 38 38 /r 	PMINSB xmm1, xmm2/m128 	V	V	Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 38 /r 	VPMINSB xmm1, xmm2, xmm3/m128 	V	V	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
				
66 0F 38 39 /r 	PMINSD xmm1, xmm2/m128 	V	V	Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 39 /r 	VPMINSD xmm1, xmm2, xmm3/m128 	V	V	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
				
0F EA /r 	PMINSW mm1, mm2/m64	V	V	Compare signed word integers in mm2/m64 and mm1 and return minimum values.
66 0F EA /r	PMINSW xmm1, xmm2/m128	V	V	Compare signed word integers in xmm2/m128 and xmm1 and return minimum values.
VEX.NDS.128.66.0F.WIG EA /r	VPMINSW xmm1, xmm2, xmm3/m128	V	V	Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
				
0F DA /r 	PMINUB mm1, mm2/m64	V	V	Compare unsigned byte integers in mm2/m64 and mm1 and returns minimum values.
66 0F DA /r	PMINUB xmm1, xmm2/m128	V	V	Compare unsigned byte integers in xmm2/m128 and xmm1 and returns minimum values.
VEX.NDS.128.66.0F.WIG DA /r	VPMINUB xmm1, xmm2, xmm3/m128	V	V	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
				
66 0F 38 3B /r 	PMINUD xmm1, xmm2/m128 	V	V	Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3B /r 	VPMINUD xmm1, xmm2, xmm3/m128 	V	V	Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
				
66 0F 38 3A /r 	PMINUW xmm1, xmm2/m128 	V	V	Compare packed unsigned word integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3A /r 	VPMINUW xmm1, xmm2, xmm3/m128 	V	V	Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
				
0F D7 /r 	PMOVMSKB reg, mm	V	V	Move a byte mask of mm to reg. The upper bits of r32 or r64 are zeroed
66 0F D7 /r	PMOVMSKB reg, xmm	V	V	Move a byte mask of xmm to reg. The upper bits of r32 or r64 are zeroed
VEX.128.66.0F.WIG D7 /r	VPMOVMSKB reg, xmm1	V	V	Move a byte mask of xmm1 to reg. The upper bits of r32 or r64 are filled with zeros.
				
66 0f 38 20 /r 	PMOVSXBW xmm1, xmm2/m64 	V	V	Sign extend 8 packed signed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed signed 16-bit integers in xmm1.
66 0f 38 21 /r 	PMOVSXBD xmm1, xmm2/m32 	V	V	Sign extend 4 packed signed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed signed 32-bit integers in xmm1.
66 0f 38 22 /r 	PMOVSXBQ xmm1, xmm2/m16	V	V	Sign extend 2 packed signed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed signed 64-bit integers in xmm1.
66 0f 38 23 /r 	PMOVSXWD xmm1, xmm2/m64 	V	V	Sign extend 4 packed signed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed signed 32-bit integers in xmm1.
66 0f 38 24 /r 	PMOVSXWQ xmm1, xmm2/m32 	V	V	Sign extend 2 packed signed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed signed 64-bit integers in xmm1.
66 0f 38 25 /r 	PMOVSXDQ xmm1, xmm2/m64 	V	V	Sign extend 2 packed signed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed signed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 20 /r 	VPMOVSXBW xmm1, xmm2/m64 	V	V	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VEX.128.66.0F38.WIG 21 /r 	VPMOVSXBD xmm1, xmm2/m32 	V	V	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 22 /r 	VPMOVSXBQ xmm1, xmm2/m16 	V	V	Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 23 /r 	VPMOVSXWD xmm1, xmm2/m64 	V	V	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 24 /r 	VPMOVSXWQ xmm1, xmm2/m32 	V	V	Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 25 /r 	VPMOVSXDQ xmm1, xmm2/m64 	V	V	Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1
				
66 0f 38 30 /r 	PMOVZXBW xmm1, xmm2/m64 	V	V	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
66 0f 38 31 /r 	PMOVZXBD xmm1, xmm2/m32 	V	V	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
66 0f 38 32 /r 	PMOVZXBQ xmm1, xmm2/m16 	V	V	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
66 0f 38 33 /r 	PMOVZXWD xmm1, xmm2/m64 	V	V	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
66 0f 38 34 /r 	PMOVZXWQ xmm1, xmm2/m32 	V	V	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
66 0f 38 35 /r 	PMOVZXDQ xmm1, xmm2/m64 	V	V	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 30 /r 	VPMOVZXBW xmm1, xmm2/m64 	V	V	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VEX.128.66.0F38.WIG 31 /r 	VPMOVZXBD xmm1, xmm2/m32 	V	V	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 32 /r 	VPMOVZXBQ xmm1, xmm2/m16 	V	V	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 33 /r 	VPMOVZXWD xmm1, xmm2/m64 	V	V	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 34 /r 	VPMOVZXWQ xmm1, xmm2/m32 	V	V	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 35 /r 	VPMOVZXDQ xmm1, xmm2/m64 	V	V	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
				
66 0F 38 28 /r 	PMULDQ xmm1, xmm2/m128 	V	V	Multiply the packed signed dword integers in xmm1 and xmm2/m128 and store the quadword product in xmm1.
VEX.NDS.128.66.0F38.WIG 28 /r 	VPMULDQ xmm1, xmm2, xmm3/m128 	V	V	Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1.
				
0F 38 0B /r 	PMULHRSW mm1, mm2/m64	V	V	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to MM1.
66 0F 38 0B /r	PMULHRSW xmm1, xmm2/m128	V	V	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to XMM1.
VEX.NDS.128.66.0F38.WIG 0B /r	VPMULHRSW xmm1, xmm2, xmm3/m128	V	V	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1.
				
0F E4 /r 	PMULHUW mm1, mm2/m64	V	V	Multiply the packed unsigned word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
66 0F E4 /r	PMULHUW xmm1, xmm2/m128	V	V	Multiply the packed unsigned word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
VEX.NDS.128.66.0F.WIG E4 /r	VPMULHUW xmm1, xmm2, xmm3/m128	V	V	Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
				
0F E5 /r 	PMULHW mm, mm/m64	V	V	Multiply the packed signed word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
66 0F E5 /r	PMULHW xmm1, xmm2/m128	V	V	Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
VEX.NDS.128.66.0F.WIG E5 /r	VPMULHW xmm1, xmm2, xmm3/m128	V	V	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
				
66 0F 38 40 /r 	PMULLD xmm1, xmm2/m128 	V	V	Multiply the packed dword signed integers in xmm1 and xmm2/m128 and store the low 32 bits of each product in xmm1.
VEX.NDS.128.66.0F38.WIG 40 /r 	VPMULLD xmm1, xmm2, xmm3/m128 	V	V	Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
				
0F D5 /r 	PMULLW mm, mm/m64	V	V	Multiply the packed signed word integers in mm1 register and mm2/m64, and store the low 16 bits of the results in mm1.
66 0F D5 /r	PMULLW xmm1, xmm2/m128	V	V	Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the low 16 bits of the results in xmm1.
VEX.NDS.128.66.0F.WIG D5 /r	VPMULLW xmm1, xmm2, xmm3/m128	V	V	Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
				
0F F4 /r 	PMULUDQ mm1, mm2/m64	V	V	Multiply unsigned doubleword integer in mm1 by unsigned doubleword integer in mm2/m64, and store the quadword result in mm1.
66 0F F4 /r	PMULUDQ xmm1, xmm2/m128	V	V	Multiply packed unsigned doubleword integers in xmm1 by packed unsigned doubleword integers in xmm2/m128, and store the quadword results in xmm1.
VEX.NDS.128.66.0F.WIG F4 /r	VPMULUDQ xmm1, xmm2, xmm3/m128	V	V	Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1.
				
8F /0 	POP r/m16 	V	V	Pop top of stack into m16; increment stack pointer.
8F /0 	POP r/m32 	NE	V	Pop top of stack into m32; increment stack pointer.
8F /0 	POP r/m64 	V	NE	Pop top of stack into m64; increment stack pointer. Cannot encode 32-bit operand size.
58 +rw 	POP r16 	V	V	Pop top of stack into r16; increment stack pointer.
58 +rd 	POP r32 	NE	V	Pop top of stack into r32; increment stack pointer.
58 +rd 	POP r64 	V	NE	Pop top of stack into r64; increment stack pointer. Cannot encode 32-bit operand size.
1F 	POP DS 	I	V	Pop top of stack into DS; increment stack pointer.
07	POP ES 	I	V	Pop top of stack into ES; increment stack pointer.
17	POP SS 	I	V	Pop top of stack into SS; increment stack pointer.
PREF.66+ 0F A1 	POP FS, p66	V	V	Pop top of stack into FS; increment stack pointer by 16 bits.
0F A1 	POP FS 	NE	V	Pop top of stack into FS; increment stack pointer by 32 bits.
0F A1 	POP FS 	V	NE	Pop top of stack into FS; increment stack pointer by 64 bits.
PREF.66+ 0F A9 	POP GS, p66	V	V	Pop top of stack into GS; increment stack pointer by 16 bits.
0F A9 	POP GS 	NE	V	Pop top of stack into GS; increment stack pointer by 32 bits.
0F A9 	POP GS 	V	NE	Pop top of stack into GS; increment stack pointer by 64 bits.
				
61	POPA 	I	V	Pop DI, SI, BP, BX, DX, CX, and AX.
61	POPAD 	I	V	Pop EDI, ESI, EBP, EBX, EDX, ECX, and EAX.
				
F3 0F B8 /r 	POPCNT r16, r/m16 	V	V	POPCNT on r/m16
F3 0F B8 /r 	POPCNT r32, r/m32 	V	V	POPCNT on r/m32
F3 REX.W+ 0F B8 /r 	POPCNT r64, r/m64 	V	NE	POPCNT on r/m64
				
PREF.66+ 9D 	POPF 	V	V	Pop top of stack into lower 16 bits of EFLAGS.
9D 	POPFD 	NE	V	Pop top of stack into EFLAGS.
REX.W+ 9D 	POPFQ 	V	NE	Pop top of stack and zero-extend into RFLAGS.
				
0F EB /r 	POR mm, mm/m64	V	V	Bitwise OR of mm/m64 and mm.
66 0F EB /r	POR xmm1, xmm2/m128	V	V	Bitwise OR of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG EB /r	VPOR xmm1, xmm2, xmm3/m128	V	V	Bitwise OR of xmm2/m128 and xmm3.
				
0F 18 /1 	PREFETCHT0 m8 	V	V	Move data from m8 closer to the processor using T0 hint.
0F 18 /2 	PREFETCHT1 m8 	V	V	Move data from m8 closer to the processor using T1 hint.
0F 18 /3 	PREFETCHT2 m8 	V	V	Move data from m8 closer to the processor using T2 hint.
0F 18 /0 	PREFETCHNTA m8 	V	V	Move data from m8 closer to the processor using NTA hint.
				
0F F6 /r 	PSADBW mm1, mm2/m64	V	V	Computes the absolute differences of the packed unsigned byte integers from mm2 /m64 and mm1; differences are then summed to produce an unsigned word integer result.
66 0F F6 /r	PSADBW xmm1, xmm2/m128	V	V	Computes the absolute differences of the packed unsigned byte integers from xmm2 /m128 and xmm1; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
VEX.NDS.128.66.0F.WIG F6 /r	VPSADBW xmm1, xmm2, xmm3/m128	V	V	Computes the absolute differences of the packed unsigned byte integers from xmm3 /m128 and xmm2; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
				
0F 38 00 /r 	PSHUFB mm1, mm2/m64	V	V	Shuffle bytes in mm1 according to contents of mm2/m64.
66 0F 38 00 /r	PSHUFB xmm1, xmm2/m128	V	V	Shuffle bytes in xmm1 according to contents of xmm2/m128.
VEX.NDS.128.66.0F38.WIG 00 /r	VPSHUFB xmm1, xmm2, xmm3/m128	V	V	Shuffle bytes in xmm2 according to contents of xmm3/m128.
				
66 0F 70 /r ib 	PSHUFD xmm1, xmm2/m128, imm8	V	V	Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.128.66.0F.WIG 70 /r ib	VPSHUFD xmm1, xmm2/m128, imm8	V	V	Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
				
F3 0F 70 /r ib 	PSHUFHW xmm1, xmm2/m128, imm8	V	V	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.128.F3.0F.WIG 70 /r ib	VPSHUFHW xmm1, xmm2/m128, imm8	V	V	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
				
F2 0F 70 /r ib 	PSHUFLW xmm1, xmm2/m128, imm8	V	V	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.128.F2.0F.WIG 70 /r ib	VPSHUFLW xmm1, xmm2/m128, imm8	V	V	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
				
0F 70 /r ib 	PSHUFW mm1, mm2/m64, imm8	V	V	Shuffle the words in mm2/m64 based on the encoding in imm8 and store the result in mm1.
				
0F 38 08 /r 	PSIGNB mm1, mm2/m64	V	V	Negate/zero/preserve packed byte integers in mm1 depending on the corresponding sign in mm2/m64
66 0F 38 08 /r	PSIGNB xmm1, xmm2/m128	V	V	Negate/zero/preserve packed byte integers in xmm1 depending on the corresponding sign in xmm2/m128.
0F 38 09 /r	PSIGNW mm1, mm2/m64	V	V	Negate/zero/preserve packed word integers in mm1 depending on the corresponding sign in mm2/m128.
66 0F 38 09 /r	PSIGNW xmm1, xmm2/m128	V	V	Negate/zero/preserve packed word integers in xmm1 depending on the corresponding sign in xmm2/m128.
0F 38 0A /r	PSIGND mm1, mm2/m64	V	V	Negate/zero/preserve packed doubleword integers in mm1 depending on the corresponding sign in mm2/m128.
66 0F 38 0A /r	PSIGND xmm1, xmm2/m128	V	V	Negate/zero/preserve packed doubleword integers in xmm1 depending on the corresponding sign in xmm2/m128.
VEX.NDS.128.66.0F38.WIG 08 /r	VPSIGNB xmm1, xmm2, xmm3/m128	V	V	Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128.
VEX.NDS.128.66.0F38.WIG 09 /r	VPSIGNW xmm1, xmm2, xmm3/m128	V	V	Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128.
VEX.NDS.128.66.0F38.WIG 0A /r	VPSIGND xmm1, xmm2, xmm3/m128	V	V	Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128.
				
66 0F 73 /7 ib 	PSLLDQ xmm1, imm8	V	V	Shift xmm1 left by imm8 bytes while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /7 ib	VPSLLDQ xmm1, xmm2, imm8	V	V	Shift xmm2 left by imm8 bytes while shifting in 0s and store result in xmm1.
				
0F F1 /r	PSLLW mm, mm/m64	V	V	Shift words in mm left mm/m64 while shifting in 0s.
66 0F F1 /r	PSLLW xmm1, xmm2/m128	V	V	Shift words in xmm1 left by xmm2/m128 while shifting in 0s.
0F 71 /6 ib	PSLLW xmm1, imm8	V	V	Shift words in mm left by imm8 while shifting in 0s.
66 0F 71 /6 ib	PSLLW xmm1, imm8	V	V	Shift words in xmm1 left by imm8 while shifting in 0s.
0F F2 /r	PSLLD mm, mm/m64	V	V	Shift doublewords in mm left by mm/m64 while shifting in 0s.
66 0F F2 /r	PSLLD xmm1, xmm2/m128	V	V	Shift doublewords in xmm1 left by xmm2/m128 while shifting in 0s.
0F 72 /6 ib	PSLLD mm, imm8	V	V	Shift doublewords in mm left by imm8 while shifting in 0s.
66 0F 72 /6 ib	PSLLD xmm1, imm8	V	V	Shift doublewords in xmm1 left by imm8 while shifting in 0s.
0F F3 /r	PSLLQ mm, mm/m64	V	V	Shift quadword in mm left by mm/m64 while shifting in 0s.
66 0F F3 /r	PSLLQ xmm1, xmm2/m128	V	V	Shift quadwords in xmm1 left by xmm2/m128 while shifting in 0s.
0F 73 /6 ib	PSLLQ mm, imm8	V	V	Shift quadword in mm left by imm8 while shifting in 0s.
66 0F 73 /6 ib	PSLLQ xmm1, imm8	V	V	Shift quadwords in xmm1 left by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG F1 /r	VPSLLW xmm1, xmm2, xmm3/m128	V	V	Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 71 /6 ib	VPSLLW xmm1, xmm2, imm8	V	V	Shift words in xmm2 left by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG F2 /r	VPSLLD xmm1, xmm2, xmm3/m128	V	V	Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 72 /6 ib	VPSLLD xmm1, xmm2, imm8	V	V	Shift doublewords in xmm2 left by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG F3 /r	VPSLLQ xmm1, xmm2, xmm3/m128	V	V	Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /6 ib	VPSLLQ xmm1, xmm2, imm8	V	V	Shift quadwords in xmm2 left by imm8 while shifting in 0s.
				
0F E1 /r	PSRAW mm, mm/m64	V	V	Shift words in mm right by mm/m64 while shifting in sign bits.
66 0F E1 /r	PSRAW xmm1, xmm2/m128	V	V	Shift words in xmm1 right by xmm2/m128 while shifting in sign bits.
0F 71 /4 ib	PSRAW mm, imm8	V	V	Shift words in mm right by imm8 while shifting in sign bits
66 0F 71 /4 ib	PSRAW xmm1, imm8	V	V	Shift words in xmm1 right by imm8 while shifting in sign bits
0F E2 /r	PSRAD mm, mm/m64	V	V	Shift doublewords in mm right by mm/m64 while shifting in sign bits.
66 0F E2 /r	PSRAD xmm1, xmm2/m128	V	V	Shift doubleword in xmm1 right by xmm2 /m128 while shifting in sign bits.
0F 72 /4 ib	PSRAD mm, imm8	V	V	Shift doublewords in mm right by imm8 while shifting in sign bits.
66 0F 72 /4 ib	PSRAD xmm1, imm8	V	V	Shift doublewords in xmm1 right by imm8 while shifting in sign bits.
VEX.NDS.128.66.0F.WIG E1 /r	VPSRAW xmm1, xmm2, xmm3/m128	V	V	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VEX.NDD.128.66.0F.WIG 71 /4 ib	VPSRAW xmm1, xmm2, imm8	V	V	Shift words in xmm2 right by imm8 while shifting in sign bits.
VEX.NDS.128.66.0F.WIG E2 /r	VPSRAD xmm1, xmm2, xmm3/m128	V	V	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VEX.NDD.128.66.0F.WIG 72 /4 ib	VPSRAD xmm1, xmm2, imm8	V	V	Shift doublewords in xmm2 right by imm8 while shifting in sign bits.
				
66 0F 73 /3 ib	PSRLDQ xmm1, imm8	V	V	Shift xmm1 right by imm8 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /3 ib	VPSRLDQ xmm1, xmm2, imm8	V	V	Shift xmm2 right by imm8 bytes while shifting in 0s.
				
0F D1 /r	PSRLW mm, mm/m64	V	V	Shift words in mm right by amount specified in mm/m64 while shifting in 0s.
66 0F D1 /r	PSRLW xmm1, xmm2/m128	V	V	Shift words in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
0F 71 /2 ib	PSRLW mm, imm8	V	V	Shift words in mm right by imm8 while shifting in 0s.
66 0F 71 /2 ib	PSRLW xmm1, imm8	V	V	Shift words in xmm1 right by imm8 while shifting in 0s.
0F D2 /r	PSRLD mm, mm/m64	V	V	Shift doublewords in mm right by amount specified in mm/m64 while shifting in 0s.
66 0F D2 /r	PSRLD xmm1, xmm2/m128	V	V	Shift doublewords in xmm1 right by amount specified in xmm2 /m128 while shifting in 0s.
0F 72 /2 ib	PSRLD mm, imm8	V	V	Shift doublewords in mm right by imm8 while shifting in 0s.
66 0F 72 /2 ib	PSRLD xmm1, imm8	V	V	Shift doublewords in xmm1 right by imm8 while shifting in 0s.
0F D3 /r	PSRLQ mm, mm/m64	V	V	Shift mm right by amount specified in mm/m64 while shifting in 0s.
66 0F D3 /r	PSRLQ xmm1, xmm2/m128	V	V	Shift quadwords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
0F 73 /2 ib	PSRLQ mm, imm8	V	V	Shift mm right by imm8 while shifting in 0s. 
66 0F 73 /2 ib	PSRLQ xmm1, imm8	V	V	Shift quadwords in xmm1 right by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG D1 /r	VPSRLW xmm1, xmm2, xmm3/m128	V	V	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 71 /2 ib	VPSRLW xmm1, xmm2, imm8	V	V	Shift words in xmm2 right by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG D2 /r	VPSRLD xmm1, xmm2, xmm3/m128	V	V	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 72 /2 ib	VPSRLD xmm1, xmm2, imm8	V	V	Shift doublewords in xmm2 right by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG D3 /r	VPSRLQ xmm1, xmm2, xmm3/m128	V	V	Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /2 ib	VPSRLQ xmm1, xmm2, imm8	V	V	Shift quadwords in xmm2 right by imm8 while shifting in 0s.
				
0F F8 /r	PSUBB mm, mm/m64	V	V	Subtract packed byte integers in mm/m64 from packed byte integers in mm.
66 0F F8 /r	PSUBB xmm1, xmm2/m128	V	V	Subtract packed byte integers in xmm2/m128 from packed byte integers in xmm1.
0F F9 /r	PSUBW mm, mm/m64	V	V	Subtract packed word integers in mm/m64 from packed word integers in mm.
66 0F F9 /r	PSUBW xmm1, xmm2/m128	V	V	Subtract packed word integers in xmm2/m128 from packed word integers in xmm1.
0F FA /r	PSUBD mm, mm/m64	V	V	Subtract packed doubleword integers in mm/m64 from packed doubleword integers in mm.
66 0F FA /r	PSUBD xmm1, xmm2/m128	V	V	Subtract packed doubleword integers in xmm2/mem128 from packed doubleword integers in xmm1.
VEX.NDS.128.66.0F.WIG F8 /r	VPSUBB xmm1, xmm2, xmm3/m128	V	V	Subtract packed byte integers in xmm3/m128 from xmm2.
VEX.NDS.128.66.0F.WIG F9 /r	VPSUBW xmm1, xmm2, xmm3/m128	V	V	Subtract packed word integers in xmm3/m128 from xmm2.
VEX.NDS.128.66.0F.WIG FA /r	VPSUBD xmm1, xmm2, xmm3/m128	V	V	Subtract packed doubleword integers in xmm3/m128 from xmm2.
				
0F FB /r	PSUBQ mm1, mm2/m64	V	V	Subtract quadword integer in mm1 from mm2 /m64.
66 0F FB /r	PSUBQ xmm1, xmm2/m128	V	V	Subtract packed quadword integers in xmm1 from xmm2 /m128.
VEX.NDS.128.66.0F.WIG FB /r	VPSUBQ xmm1, xmm2, xmm3/m128	V	V	Subtract packed quadword integers in xmm3/m128 from xmm2.
				
0F E8 /r	PSUBSB mm, mm/m64	V	V	Subtract signed packed bytes in mm/m64 from signed packed bytes in mm and saturate results.
66 0F E8 /r	PSUBSB xmm1, xmm2/m128	V	V	Subtract packed signed byte integers in xmm2/m128 from packed signed byte integers in xmm1 and saturate results.
0F E9 /r	PSUBSW mm, mm/m64	V	V	Subtract signed packed words in mm/m64 from signed packed words in mm and saturate results.
66 0F E9 /r	PSUBSW xmm1, xmm2/m128	V	V	Subtract packed signed word integers in xmm2/m128 from packed signed word integers in xmm1 and saturate results.
VEX.NDS.128.66.0F.WIG E8 /r	VPSUBSB xmm1, xmm2, xmm3/m128	V	V	Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results.
VEX.NDS.128.66.0F.WIG E9 /r	VPSUBSW xmm1, xmm2, xmm3/m128	V	V	Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results.
				
0F D8 /r	PSUBUSB mm, mm/m64	V	V	Subtract unsigned packed bytes in mm/m64 from unsigned packed bytes in mm and saturate result.
66 0F D8 /r	PSUBUSB xmm1, xmm2/m128	V	V	Subtract packed unsigned byte integers in xmm2/m128 from packed unsigned byte integers in xmm1 and saturate result.
0F D9 /r	PSUBUSW mm, mm/m64	V	V	Subtract unsigned packed words in mm/m64 from unsigned packed words in mm and saturate result.
66 0F D9 /r	PSUBUSW xmm1, xmm2/m128	V	V	Subtract packed unsigned word integers in xmm2/m128 from packed unsigned word integers in xmm1 and saturate result.
VEX.NDS.128.66.0F.WIG D8 /r	VPSUBUSB xmm1, xmm2, xmm3/m128	V	V	Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result.
VEX.NDS.128.66.0F.WIG D9 /r	VPSUBUSW xmm1, xmm2, xmm3/m128	V	V	Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result.
				
66 0F 38 17 /r	PTEST xmm1, xmm2/m128	V	V	Set ZF if xmm2/m128 AND xmm1 result is all 0s. Set CF if xmm2/m128 AND NOT xmm1 result is all 0s.
VEX.128.66.0F38.WIG 17 /r	VPTEST xmm1, xmm2/m128	V	V	Set ZF and CF depending on bitwise AND and ANDN of sources.
VEX.256.66.0F38.WIG 17 /r	VPTEST ymm1, ymm2/m256	V	V	Set ZF and CF depending on bitwise AND and ANDN of sources.
				
0F 68 /r	PUNPCKHBW mm, mm/m64	V	V	Unpack and interleave high-order bytes from mm and mm/m64 into mm.
66 0F 68 /r	PUNPCKHBW xmm1, xmm2/m128	V	V	Unpack and interleave high-order bytes from xmm1 and xmm2/m128 into xmm1.
0F 69 /r	PUNPCKHWD mm, mm/m64	V	V	Unpack and interleave high-order words from mm and mm/m64 into mm.
66 0F 69 /r	PUNPCKHWD xmm1, xmm2/m128	V	V	Unpack and interleave high-order words from xmm1 and xmm2/m128 into xmm1.
0F 6A /r	PUNPCKHDQ mm, mm/m64	V	V	Unpack and interleave high-order doublewords from mm and mm/m64 into mm.
66 0F 6A /r	PUNPCKHDQ xmm1, xmm2/m128	V	V	Unpack and interleave high-order doublewords from xmm1 and xmm2/m128 into xmm1.
66 0F 6D /r	PUNPCKHQDQ xmm1, xmm2/m128	V	V	Unpack and interleave high-order quadwords from xmm1 and xmm2/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 68 /r	VPUNPCKHBW xmm1, xmm2, xmm3/m128	V	V	Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 69 /r	VPUNPCKHWD xmm1, xmm2, xmm3/m128	V	V	Interleave high-order words from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 6A /r	VPUNPCKHDQ xmm1, xmm2, xmm3/m128	V	V	Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 6D /r 	VPUNPCKHQDQ xmm1, xmm2, xmm3/m128	V	V	Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register.
				
0F 60 /r	PUNPCKLBW mm, mm/m32	V	V	Interleave low-order bytes from mm and mm/m32 into mm.
66 0F 60 /r	PUNPCKLBW xmm1, xmm2/m128	V	V	Interleave low-order bytes from xmm1 and xmm2/m128 into xmm1.
0F 61 /r	PUNPCKLWD mm, mm/m32	V	V	Interleave low-order words from mm and mm/m32 into mm.
66 0F 61 /r	PUNPCKLWD xmm1, xmm2/m128	V	V	Interleave low-order words from xmm1 and xmm2/m128 into xmm1.
0F 62 /r	PUNPCKLDQ mm, mm/m32	V	V	Interleave low-order doublewords from mm and mm/m32 into mm.
66 0F 62 /r	PUNPCKLDQ xmm1, xmm2/m128	V	V	Interleave low-order doublewords from xmm1 and xmm2/m128 into xmm1.
66 0F 6C /r	PUNPCKLQDQ xmm1, xmm2/m128	V	V	Interleave low-order quadword from xmm1 and xmm2/m128 into xmm1 register.
VEX.NDS.128.66.0F.WIG 60 /r	VPUNPCKLBW xmm1, xmm2, xmm3/m128	V	V	Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 61 /r	VPUNPCKLWD xmm1, xmm2, xmm3/m128	V	V	Interleave low-order words from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 62 /r	VPUNPCKLDQ xmm1, xmm2, xmm3/m128	V	V	Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 6C /r	VPUNPCKLQDQ xmm1, xmm2, xmm3/m128	V	V	Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register.
				
FF /6	PUSH r/m16 	V	V	Push r/m16.
FF /6	PUSH r/m32	NE	V	Push r/m32.
FF /6	PUSH r/m64	V	NE	Push r/m64.
50 +rw	PUSH r16	V	V	Push r16.
50 +rd	PUSH r32	NE	V	Push r32.
50 +rd	PUSH r64	V	NE	Push r64.
6A	PUSH imm8	V	V	Push imm8.
68	PUSH imm16	V	V	Push imm16.
68	PUSH imm32	V	V	Push imm32.
0E	PUSH CS	I	V	Push CS.
16	PUSH SS	I	V	Push SS.
1E	PUSH DS	I	V	Push DS.
06	PUSH ES	I	V	Push ES.
0F A0	PUSH FS	V	V	Push FS.
0F A8	PUSH GS	V	V	Push GS.
				
60	PUSHA	I	V	Push AX, CX, DX, BX, original SP, BP, SI, and DI.
60	PUSHAD	I	V	Push EAX, ECX, EDX, EBX, original ESP, EBP, ESI, and EDI.
				
PREF.66+ 9C	PUSHF	V	V	Push lower 16 bits of EFLAGS.
9C	PUSHFD	NE	V	Push EFLAGS.
9C	PUSHFQ	V	NE	Push RFLAGS.
				
0F EF /r	PXOR mm, mm/m64	V	V	Bitwise XOR of mm/m64 and mm.
66 0F EF /r	PXOR xmm1, xmm2/m128	V	V	Bitwise XOR of xmm2/m128 and xmm1. 
VEX.NDS.128.66.0F.WIG EF /r	VPXOR xmm1, xmm2, xmm3/m128	V	V	Bitwise XOR of xmm3/m128 and xmm2.
				
D0 /2	RCL r/m8, 1	V	V	Rotate 9 bits (CF, r/m8) left once.
REX+ D0 /2	RCL r/m8, 1	V	NE	Rotate 9 bits (CF, r/m8) left once.
D2 /2	RCL r/m8, CL	V	V	Rotate 9 bits (CF, r/m8) left CL times.
REX+ D2 /2	RCL r/m8, CL	V	NE	Rotate 9 bits (CF, r/m8) left CL times.
C0 /2 ib	RCL r/m8, imm8	V	V	Rotate 9 bits (CF, r/m8) left imm8 times.
REX+ C0 /2 ib	RCL r/m8, imm8	V	NE	Rotate 9 bits (CF, r/m8) left imm8 times.
D1 /2	RCL r/m16, 1	V	V	Rotate 17 bits (CF, r/m16) left once.
D3 /2	RCL r/m16, CL	V	V	Rotate 17 bits (CF, r/m16) left CL times.
C1 /2 ib	RCL r/m16, imm8	V	V	Rotate 17 bits (CF, r/m16) left imm8 times.
D1 /2	RCL r/m32, 1	V	V	Rotate 33 bits (CF, r/m32) left once.
REX.W+ D1 /2	RCL r/m64, 1	V	NE	Rotate 65 bits (CF, r/m64) left once. Uses a 6 bit count.
D3 /2	RCL r/m32, CL	V	V	Rotate 33 bits (CF, r/m32) left CL times.
REX.W+ D3 /2	RCL r/m64, CL	V	NE	Rotate 65 bits (CF, r/m64) left CL times. Uses a 6 bit count.
C1 /2 ib	RCL r/m32, imm8	V	V	Rotate 33 bits (CF, r/m32) left imm8 times.
REX.W+ C1 /2 ib	RCL r/m64, imm8	V	NE	Rotate 65 bits (CF, r/m64) left imm8 times. Uses a 6 bit count.
D0 /3	RCR r/m8, 1	V	V	Rotate 9 bits (CF, r/m8) right once.
REX+ D0 /3	RCR r/m8, 1	V	NE	Rotate 9 bits (CF, r/m8) right once.
D2 /3	RCR r/m8, CL	V	V	Rotate 9 bits (CF, r/m8) right CL times.
REX+ D2 /3	RCR r/m8, CL	V	NE	Rotate 9 bits (CF, r/m8) right CL times.
C0 /3 ib	RCR r/m8, imm8	V	V	Rotate 9 bits (CF, r/m8) right imm8 times.
REX+ C0 /3 ib	RCR r/m8, imm8	V	NE	Rotate 9 bits (CF, r/m8) right imm8 times.
D1 /3	RCR r/m16, 1	V	V	Rotate 17 bits (CF, r/m16) right once.
D3 /3	RCR r/m16, CL	V	V	Rotate 17 bits (CF, r/m16) right CL times.
C1 /3 ib	RCR r/m16, imm8	V	V	Rotate 17 bits (CF, r/m16) right imm8 times.
D1 /3	RCR r/m32, 1	V	V	Rotate 33 bits (CF, r/m32) right once. Uses a 6 bit count.
REX.W+ D1 /3	RCR r/m64, 1	V	NE	Rotate 65 bits (CF, r/m64) right once. Uses a 6 bit count.
D3 /3	RCR r/m32, CL	V	V	Rotate 33 bits (CF, r/m32) right CL times.
REX.W+ D3 /3	RCR r/m64, CL	V	NE	Rotate 65 bits (CF, r/m64) right CL times. Uses a 6 bit count.
C1 /3 ib	RCR r/m32, imm8	V	V	Rotate 33 bits (CF, r/m32) right imm8 times.
REX.W+ C1 /3 ib	RCR r/m64, imm8	V	NE	Rotate 65 bits (CF, r/m64) right imm8 times. Uses a 6 bit count.
D0 /0	ROL r/m8, 1	V	V	Rotate 8 bits r/m8 left once.
REX+ D0 /0	ROL r/m8, 1	V	NE	Rotate 8 bits r/m8 left once.
D2 /0	ROL r/m8, CL	V	V	Rotate 8 bits r/m8 left CL times.
REX+ D2 /0	ROL r/m8, CL	V	NE	Rotate 8 bits r/m8 left CL times.
C0 /0 ib	ROL r/m8, imm8	V	V	Rotate 8 bits r/m8 left imm8 times.
REX+ C0 /0 ib	ROL r/m8, imm8	V	NE	Rotate 8 bits r/m8 left imm8 times.
D1 /0	ROL r/m16, 1	V	V	Rotate 16 bits r/m16 left once.
D3 /0	ROL r/m16, CL	V	V	Rotate 16 bits r/m16 left CL times.
C1 /0 ib	ROL r/m16, imm8	V	V	Rotate 16 bits r/m16 left imm8 times.
D1 /0	ROL r/m32, 1	V	V	Rotate 32 bits r/m32 left once.
REX.W+ D1 /0	ROL r/m64, 1	V	NE	Rotate 64 bits r/m64 left once. Uses a 6 bit count.
D3 /0	ROL r/m32, CL	V	V	Rotate 32 bits r/m32 left CL times.
REX.W+ D3 /0	ROL r/m64, CL	V	NE	Rotate 64 bits r/m64 left CL times. Uses a 6 bit count.
C1 /0 ib	ROL r/m32, imm8	V	V	Rotate 32 bits r/m32 left imm8 times.
C1 /0 ib	ROL r/m64, imm8	V	NE	Rotate 64 bits r/m64 left imm8 times. Uses a 6 bit count.
D0 /1	ROR r/m8, 1	V	V	Rotate 8 bits r/m8 right once.
REX+ D0 /1	ROR r/m8, 1	V	NE	Rotate 8 bits r/m8 right once.
D2 /1	ROR r/m8, CL	V	V	Rotate 8 bits r/m8 right CL times.
REX+ D2 /1	ROR r/m8, CL	V	NE	Rotate 8 bits r/m8 right CL times.
C0 /1 ib	ROR r/m8, imm8	V	V	Rotate 8 bits r/m16 right imm8 times.
REX+ C0 /1 ib	ROR r/m8, imm8	V	NE	Rotate 8 bits r/m16 right imm8 times.
D1 /1	ROR r/m16, 1	V	V	Rotate 16 bits r/m16 right once.
D3 /1	ROR r/m16, CL	V	V	Rotate 16 bits r/m16 right CL times.
C1 /1 ib	ROR r/m16, imm8	V	V	Rotate 16 bits r/m16 right imm8 times.
D1 /1	ROR r/m32, 1	V	V	Rotate 32 bits r/m32 right once.
REX.W+ D1 /1	ROR r/m64, 1	V	NE	Rotate 64 bits r/m64 right once. Uses a 6 bit count.
D3 /1	ROR r/m32, CL	V	V	Rotate 32 bits r/m32 right CL times.
REX.W+ D3 /1	ROR r/m64, CL	V	NE	Rotate 64 bits r/m64 right CL times. Uses a 6 bit count.
C1 /1 ib	ROR r/m32, imm8	V	V	Rotate 32 bits r/m32 right imm8 times.
REX.W+ C1 /1 ib	ROR r/m64, imm8	V	NE	Rotate 64 bits r/m64 right imm8 times. Uses a 6 bit count.
				
0F 53 /r	RCPPS xmm1, xmm2/m128	V	V	Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.0F.WIG 53 /r	VRCPPS xmm1, xmm2/m128	V	V	Computes the approximate reciprocals of packed single-precision values in xmm2/mem and stores the results in xmm1.
VEX.256.0F.WIG 53 /r	VRCPPS ymm1, ymm2/m256	V	V	Computes the approximate reciprocals of packed single-precision values in ymm2/mem and stores the results in ymm1.
				
F3 0F 53 /r	RCPSS xmm1, xmm2/m32	V	V	Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm2/m32 and stores the result in xmm1.
VEX.NDS.LIG.F3.0F.WIG 53 /r	VRCPSS xmm1, xmm2, xmm3/m32	V	V	Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the result in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
				
F3 0F AE /0	RDFSBASE r32	V	I	Load the 32-bit destination register with the FS base address.
REX.W+ F3 0F AE /0	RDFSBASE r64	V	I	Load the 64-bit destination register with the FS base address.
F3 0F AE /1	RDGSBASE r32	V	I	Load the 32-bit destination register with the GS base address.
REX.W+ F3 0F AE /1	RDGSBASE r64	V	I	Load the 64-bit destination register with the GS base address.
				
0F 32	RDMSR	V	V	Read MSR specified by ECX into EDX:EAX.
				
0F 33	RDPMC	V	V	Read performance-monitoring counter specified by ECX into EDX:EAX.
				
0F C7 /6	RDRAND r16	V	V	Read a 16-bit random number and store in the destination register.
0F C7 /6	RDRAND r32	V	V	Read a 32-bit random number and store in the destination register.
REX.W+ 0F C7 /6	RDRAND r64	V	I	Read a 64-bit random number and store in the destination register.
				
0F 31	RDTSC	V	V	Read time-stamp counter into EDX:EAX.
				
0F 01 F9	RDTSCP	V	V	Read 64-bit time-stamp counter and 32-bit IA32_TSC_AUX value into EDX:EAX and ECX.
				
F3 6C	REP_INS m8, DX 	V	V	Input (E)CX bytes from port DX into ES:[(E)DI].
F3 REX.w+ 6C	REP_INS m8, DX	V	NE	Input RCX bytes from port DX into [RDI].
F3 6D	REP_INS m16, DX	V	V	Input (E)CX words from port DX into ES:[(E)DI.]
F3 6D	REP_INS m32, DX	V	V	Input (E)CX doublewords from port DX into ES:[(E)DI].
F3 REX.w+ 6D	REP_INS m32, DX	V	NE	Input RCX default size from port DX into [RDI].
F3 A4	REP_MOVS m8, m8	V	V	Move (E)CX bytes from DS:[(E)SI] to ES:[(E)DI].
F3 REX.w+ A4	REP_MOVS m8, m8	V	NE	Move RCX bytes from [RSI] to [RDI].
F3 A5	REP_MOVS m16, m16	V	V	Move (E)CX words from DS:[(E)SI] to ES:[(E)DI].
F3 A5	REP_MOVS m32, m32	V	V	Move (E)CX doublewords from DS:[(E)SI] to ES:[(E)DI].
F3 REX.w+ A5	REP_MOVS m64, m64	V	NE	Move RCX quadwords from [RSI] to [RDI].
F3 6E	REP_OUTS DX, m8	V	V	Output (E)CX bytes from DS:[(E)SI] to port DX.
F3 REX.w+ 6E	REP_OUTS DX, m8	V	NE	Output RCX bytes from [RSI] to port DX.
F3 6F	REP_OUTS DX, m16	V	V	Output (E)CX words from DS:[(E)SI] to port DX.
F3 6F	REP_OUTS DX, m32	V	V	Output (E)CX doublewords from DS:[(E)SI] to port DX.
F3 REX.w+ 6F	REP_OUTS DX, m32	V	NE	Output RCX default size from [RSI] to port DX.
F3 AC	REP_LODS AL	V	V	Load (E)CX bytes from DS:[(E)SI] to AL.
F3 REX.w+ AC	REP_LODS AL	V	NE	Load RCX bytes from [RSI] to AL.
F3 AD	REP_LODS AX	V	V	Load (E)CX words from DS:[(E)SI] to AX.
F3 AD	REP_LODS EAX	V	V	Load (E)CX doublewords from DS:[(E)SI] to EAX.
F3 REX.w+ AD	REP_LODS RAX	V	NE	Load RCX quadwords from [RSI] to RAX.
F3 AA	REP_STOS m8	V	V	Fill (E)CX bytes at ES:[(E)DI] with AL.
F3 REX.w+ AA	REP_STOS m8	V	NE	Fill RCX bytes at [RDI] with AL.
F3 AB	REP_STOS m16	V	V	Fill (E)CX words at ES:[(E)DI] with AX.
F3 AB	REP_STOS m32	V	V	Fill (E)CX doublewords at ES:[(E)DI] with EAX.
F3 REX.w+ AB	REP_STOS m64	V	NE	Fill RCX quadwords at [RDI] with RAX.
F3 A6	REPE_CMPS m8, m8	V	V	Find nonmatching bytes in ES:[(E)DI] and DS:[(E)SI].
F3 REX.w+ A6	REPE_CMPS m8, m8	V	NE	Find non-matching bytes in [RDI] and [RSI].
F3 A7	REPE_CMPS m16, m16	V	V	Find nonmatching words in ES:[(E)DI] and DS:[(E)SI].
F3 A7	REPE_CMPS m32, m32	V	V	Find nonmatching doublewords in ES:[(E)DI] and DS:[(E)SI].
F3 REX.w+ A7	REPE_CMPS m64, m64	V	NE	Find non-matching quadwords in [RDI] and [RSI].
F3 AE	REPE_SCAS m8	V	V	Find non-AL byte starting at ES:[(E)DI].
F3 REX.w+ AE	REPE_SCAS m8	V	NE	Find non-AL byte starting at [RDI].
F3 AF	REPE_SCAS m16	V	V	Find non-AX word starting at ES:[(E)DI].
F3 AF	REPE_SCAS m32	V	V	Find non-EAX doubleword starting at ES:[(E)DI].
F3 REX.w+ AF	REPE_SCAS m64	V	NE	Find non-RAX quadword starting at [RDI].
F2 A6	REPNE_CMPS m8, m8	V	V	Find matching bytes in ES:[(E)DI] and DS:[(E)SI].
F2 REX.w+ A6	REPNE_CMPS m8, m8	V	NE	Find matching bytes in [RDI] and [RSI].
F2 A7	REPNE_CMPS m16, m16	V	V	Find matching words in ES:[(E)DI] and DS:[(E)SI].
F2 A7	REPNE_CMPS m32, m32	V	V	Find matching doublewords in ES:[(E)DI] and DS:[(E)SI].
F2 REX.w+ A7	REPNE_CMPS m64, m64	V	NE	Find matching doublewords in [RDI] and [RSI].
F2 AE	REPNE_SCAS m8	V	V	Find AL, starting at ES:[(E)DI].
F2 REX.w+ AE	REPNE_SCAS m8	V	NE	Find AL, starting at [RDI].
F2 AF	REPNE_SCAS m16	V	V	Find AX, starting at ES:[(E)DI].
F2 AF	REPNE_SCAS m32	V	V	Find EAX, starting at ES:[(E)DI].
F2 REX.w+ AF	REPNE_SCAS m64	V	NE	Find RAX, starting at [RDI].
				
C3	RET	V	V	Near return to calling procedure.
CB	RET far	V	V	Far return to calling procedure.
C2 iw	RET imm16	V	V	Near return to calling procedure and pop imm16 bytes from stack.
CA iw	RET imm16, far	V	V	Far return to calling procedure and pop imm16 bytes from stack.
				
66 0F 3A 09 /r ib	ROUNDPD xmm1, xmm2/m128, imm8	V	V	Round packed double precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.128.66.0F3A.WIG 09 /r ib	VROUNDPD xmm1, xmm2/m128, imm8	V	V	Round packed double-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.256.66.0F3A.WIG 09 /r ib	VROUNDPD ymm1, ymm2/m256, imm8	V	V	Round packed double-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
				
66 0F 3A 08 /r ib	ROUNDPS xmm1, xmm2/m128, imm8	V	V	Round packed single precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.128.66.0F3A.WIG 08 /r ib	VROUNDPS xmm1, xmm2/m128, imm8	V	V	Round packed single-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.256.66.0F3A.WIG 08 /r ib	VROUNDPS ymm1, ymm2/m256, imm8	V	V	Round packed single-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
				
66 0F 3A 0B /r ib	ROUNDSD xmm1, xmm2/m64, imm8	V	V	Round the low packed double precision floating-point value in xmm2/m64 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.NDS.LIG.66.0F3A.WIG 0B /r ib	VROUNDSD xmm1, xmm2, xmm3/m64, imm8	V	V	Round the low packed double precision floating-point value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by imm8. Upper packed double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
				
66 0F 3A 0A /r ib	ROUNDSS xmm1, xmm2/m32, imm8	V	V	Round the low packed single precision floating-point value in xmm2/m32 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.NDS.LIG.66.0F3A.WIG 0A ib	VROUNDSS xmm1, xmm2, xmm3/m32, imm8	V	V	Round the low packed single precision floating-point value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by imm8. Also, upper packed single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
				
0F AA	RSM	I	V	Resume operation of interrupted program.
				
0F 52 /r	RSQRTPS xmm1, xmm2/m128	V	V	Computes the approximate reciprocals of the square roots of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.0F.WIG 52 /r	VRSQRTPS xmm1, xmm2/m128	V	V	Computes the approximate reciprocals of the square roots of packed single-precision values in xmm2/mem and stores the results in xmm1.
VEX.256.0F.WIG 52 /r	VRSQRTPS ymm1, ymm2/m256	V	V	Computes the approximate reciprocals of the square roots of packed single-precision values in ymm2/mem and stores the results in ymm1.
				
F3 0F 52 /r	RSQRTSS xmm1, xmm2/m32	V	V	Computes the approximate reciprocal of the square root of the low single-precision floating-point value in xmm2/m32 and stores the results in xmm1.
VEX.NDS.LIG.F3.0F.WIG 52 /r	VRSQRTSS xmm1, xmm2, xmm3/m32	V	V	Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
				
9E	SAHF	V	V	Loads SF, ZF, AF, PF, and CF from AH into EFLAGS register.
				
D0 /4	SAL r/m8, 1	V	V	Multiply r/m8 by 2, once.
REX+ D0 /4	SAL r/m8, 1	V	NE	Multiply r/m8 by 2, once.
D2 /4	SAL r/m8, CL	V	V	Multiply r/m8 by 2, CL times.
REX+ D2 /4	SAL r/m8, CL	V	NE	Multiply r/m8 by 2, CL times.
C0 /4 ib	SAL r/m8, imm8	V	V	Multiply r/m8 by 2, imm8 times.
REX+ C0 /4 ib	SAL r/m8, imm8	V	NE	Multiply r/m8 by 2, imm8 times.
D1 /4	SAL r/m16, 1	V	V	Multiply r/m16 by 2, once.
D3 /4	SAL r/m16, CL	V	V	Multiply r/m16 by 2, CL times.
C1 /4 ib	SAL r/m16, imm8	V	V	Multiply r/m16 by 2, imm8 times.
D1 /4	SAL r/m32, 1	V	V	Multiply r/m32 by 2, once.
REX.W+ D1 /4	SAL r/m64, 1	V	NE	Multiply r/m64 by 2, once.
D3 /4	SAL r/m32, CL	V	V	Multiply r/m32 by 2, CL times.
REX.W+ D3 /4	SAL r/m64, CL	V	NE	Multiply r/m64 by 2, CL times.
C1 /4 ib	SAL r/m32, imm8	V	V	Multiply r/m32 by 2, imm8 times.
REX.W+ C1 /4 ib	SAL r/m64, imm8	V	NE	Multiply r/m64 by 2, imm8 times.
D0 /7	SAR r/m8, 1	V	V	Signed divide r/m8 by 2, once.
REX+ D0 /7	SAR r/m8, 1	V	NE	Signed divide r/m8 by 2, once.
D2 /7	SAR r/m8, CL	V	V	Signed divide r/m8 by 2, CL times.
REX+ D2 /7	SAR r/m8, CL	V	NE	Signed divide r/m8 by 2, CL times.
C0 /7 ib	SAR r/m8, imm8	V	V	Signed divide r/m8 by 2, imm8 time.
REX+ C0 /7 ib	SAR r/m8, imm8	V	NE	Signed divide r/m8 by 2, imm8 time.
D1 /7	SAR r/m16,1	V	V	Signed divide r/m16 by 2, once.
D3 /7	SAR r/m16, CL	V	V	Signed divide r/m16 by 2, CL times.
C1 /7 ib	SAR r/m16, imm8	V	V	Signed divide r/m16 by 2, imm8 times.
D1 /7	SAR r/m32, 1	V	V	Signed divide r/m32 by 2, once.
REX.W+ D1 /7	SAR r/m64, 1	V	NE	Signed divide r/m32 by 2, once.
D3 /7	SAR r/m32, CL	V	V	Signed divide r/m32 by 2, CL times.
REX.W+ D3 /7	SAR r/m64, CL	V	NE	Signed divide r/m32 by 2, CL times.
C1 /7 ib	SAR r/m32, imm8	V	V	Signed divide r/m32 by 2, imm8 times.
REX.W+ C1 /7 ib	SAR r/m64, imm8	V	NE	Signed divide r/m32 by 2, imm8 times.
D0 /4	SHL r/m8, 1	V	V	Multiply r/m8 by 2, once.
REX+ D0 /4	SHL r/m8, 1	V	NE	Multiply r/m8 by 2, once.
D2 /4	SHL r/m8, CL	V	V	Multiply r/m8 by 2, CL times.
REX+ D2 /4	SHL r/m8, CL	V	NE	Multiply r/m8 by 2, CL times.
C0 /4 ib	SHL r/m8, imm8	V	V	Multiply r/m8 by 2, imm8 times.
REX+ C0 /4 ib	SHL r/m8, imm8	V	NE	Multiply r/m8 by 2, imm8 times.
D1 /4	SHL r/m16,1	V	V	Multiply r/m16 by 2, once.
D3 /4	SHL r/m16, CL	V	V	Multiply r/m16 by 2, CL times.
C1 /4 ib	SHL r/m16, imm8	V	V	Multiply r/m16 by 2, imm8 times.
D1 /4	SHL r/m32,1	V	V	Multiply r/m32 by 2, once.
REX.W+ D1 /4	SHL r/m64,1	V	NE	Multiply r/m64 by 2, once.
D3 /4	SHL r/m32, CL	V	V	Multiply r/m32 by 2, CL times.
REX.W+ D3 /4	SHL r/m64, CL	V	NE	Multiply r/m32 by 2, CL times.
C1 /4 ib	SHL r/m32, imm8	V	V	Multiply r/m32 by 2, imm8 times.
REX.W+ C1 /4 ib	SHL r/m64, imm8	V	NE	Multiply r/m32 by 2, imm8 times.
D0 /5	SHR r/m8,1	V	V	Unsigned divide r/m8 by 2, once.
REX+ D0 /5	SHR r/m8, 1	V	NE	Unsigned divide r/m8 by 2, once.
D2 /5	SHR r/m8, CL	V	V	Unsigned divide r/m8 by 2, CL times.
REX+ D2 /5	SHR r/m8, CL	V	NE	Unsigned divide r/m8 by 2, CL times.
C0 /5 ib	SHR r/m8, imm8	V	V	Unsigned divide r/m8 by 2, imm8 times.
REX+ C0 /5 ib	SHR r/m8, imm8	V	NE	Unsigned divide r/m8 by 2, imm8 times.
D1 /5	SHR r/m16, 1	V	V	Unsigned divide r/m16 by 2, once.
D3 /5	SHR r/m16, CL	V	V	Unsigned divide r/m16 by 2, CL times
C1 /5 ib	SHR r/m16, imm8	V	V	Unsigned divide r/m16 by 2, imm8 times.
D1 /5	SHR r/m32, 1	V	V	Unsigned divide r/m32 by 2, once.
REX.W+ D1 /5	SHR r/m64, 1	V	NE	Unsigned divide r/m32 by 2, once.
D3 /5	SHR r/m32, CL	V	V	Unsigned divide r/m32 by 2, CL times.
REX.W+ D3 /5	SHR r/m64, CL	V	NE	Unsigned divide r/m32 by 2, CL times.
C1 /5 ib	SHR r/m32, imm8	V	V	Unsigned divide r/m32 by 2, imm8 times.
REX.W+ C1 /5 ib	SHR r/m64, imm8	V	NE	Unsigned divide r/m32 by 2, imm8 times.
				
1C ib	SBB AL, imm8	V	V	Subtract with borrow imm8 from AL.
1D iw	SBB AX, imm16	V	V	Subtract with borrow imm16 from AX.
1D id	SBB EAX, imm32	V	V	Subtract with borrow imm32 from EAX.
REX.W+ 1D id	SBB RAX, imm32	V	NE	Subtract with borrow sign-extended imm.32 to 64-bits from RAX.
80 /3 ib	SBB r/m8, imm8	V	V	Subtract with borrow imm8 from r/m8.
REX+ 80 /3 ib	SBB r/m8, imm8	V	NE	Subtract with borrow imm8 from r/m8.
81 /3 iw	SBB r/m16, imm16	V	V	Subtract with borrow imm16 from r/m16.
81 /3 id	SBB r/m32, imm32	V	V	Subtract with borrow imm32 from r/m32.
REX.W+ 81 /3 id	SBB r/m64, imm32	V	NE	Subtract with borrow sign-extended imm32 to 64-bits from r/m64.
83 /3 ib	SBB r/m16, imm8	V	V	Subtract with borrow sign-extended imm8 from r/m16.
83 /3 ib	SBB r/m32, imm8	V	V	Subtract with borrow sign-extended imm8 from r/m32.
REX.W+ 83 /3 ib	SBB r/m64, imm8	V	NE	Subtract with borrow sign-extended imm8 from r/m64.
18 /r	SBB r/m8, r8	V	V	Subtract with borrow r8 from r/m8.
REX+ 18 /r	SBB r/m8, r8	V	NE	Subtract with borrow r8 from r/m8.
19 /r	SBB r/m16, r16	V	V	Subtract with borrow r16 from r/m16.
19 /r	SBB r/m32, r32	V	V	Subtract with borrow r32 from r/m32.
REX.W+ 19 /r	SBB r/m64, r64	V	NE	Subtract with borrow r64 from r/m64.
1A /r	SBB r8, r/m8	V	V	Subtract with borrow r/m8 from r8.
REX+ 1A /r	SBB r8, r/m8	V	NE	Subtract with borrow r/m8 from r8.
1B /r	SBB r16, r/m16	V	V	Subtract with borrow r/m16 from r16.
1B /r	SBB r32, r/m32	V	V	Subtract with borrow r/m32 from r32.
REX.W+ 1B /r	SBB r64, r/m64	V	NE	Subtract with borrow r/m64 from r64.
				
AE	SCAS m8	V	V	Compare AL with byte at ES:(E)DI or RDI, then set status flags.
AF	SCAS m16	V	V	Compare AX with word at ES:(E)DI or RDI, then set status flags.
AF	SCAS m32	V	V	Compare EAX with doubleword at ES(E)DI or RDI then set status flags.
REX.W+ AF	SCAS m64	V	NE	Compare RAX with quadword at RDI or EDI then set status flags.
AE	SCASB	V	V	Compare AL with byte at ES:(E)DI or RDI then set status flags.
PREF.66+ AF	SCASW	V	V	Compare AX with word at ES:(E)DI or RDI then set status flags.
AF	SCASD	V	V	Compare EAX with doubleword at ES:(E)DI or RDI then set status flags.
REX.W+ AF	SCASQ	V	NE	Compare RAX with quadword at RDI or EDI then set status flags.
				
0F 97	SETA r/m8	V	V	Set byte if above (CF=0 and ZF=0).
REX+ 0F 97	SETA r/m8	V	NE	Set byte if above (CF=0 and ZF=0).
0F 93	SETAE r/m8	V	V	Set byte if above or equal (CF=0).
REX+ 0F 93	SETAE r/m8	V	NE	Set byte if above or equal (CF=0).
0F 92	SETB r/m8	V	V	Set byte if below (CF=1).
REX+ 0F 92	SETB r/m8	V	NE	Set byte if below (CF=1).
0F 96	SETBE r/m8	V	V	Set byte if below or equal (CF=1 or ZF=1).
REX+ 0F 96	SETBE r/m8	V	NE	Set byte if below or equal (CF=1 or ZF=1).
0F 92	SETC r/m8	V	V	Set byte if carry (CF=1).
REX+ 0F 92	SETC r/m8	V	NE	Set byte if carry (CF=1).
0F 94	SETE r/m8	V	V	Set byte if equal (ZF=1).
REX+ 0F 94	SETE r/m8	V	NE	Set byte if equal (ZF=1).
0F 9F	SETG r/m8	V	V	Set byte if greater (ZF=0 and SF=OF).
REX+ 0F 9F	SETG r/m8	V	NE	Set byte if greater (ZF=0 and SF=OF).
0F 9D	SETGE r/m8	V	V	Set byte if greater or equal (SF=OF).
REX+ 0F 9D	SETGE r/m8	V	NE	Set byte if greater or equal (SF=OF).
0F 9C	SETL r/m8	V	V	Set byte if less (SF!= OF).
REX+ 0F 9C	SETL r/m8	V	NE	Set byte if less (SF!= OF).
0F 9E	SETLE r/m8	V	V	Set byte if less or equal (ZF=1 or SF!= OF).
REX+ 0F 9E	SETLE r/m8	V	NE	Set byte if less or equal (ZF=1 or SF!= OF).
0F 96	SETNA r/m8	V	V	Set byte if not above (CF=1 or ZF=1).
REX+ 0F 96	SETNA r/m8	V	NE	Set byte if not above (CF=1 or ZF=1).
0F 92	SETNAE r/m8	V	V	Set byte if not above or equal (CF=1).
REX+ 0F 92	SETNAE r/m8	V	NE	Set byte if not above or equal (CF=1).
0F 93	SETNB r/m8	V	V	Set byte if not below (CF=0).
REX+ 0F 93	SETNB r/m8	V	NE	Set byte if not below (CF=0).
0F 97	SETNBE r/m8	V	V	Set byte if not below or equal (CF=0 and ZF=0).
REX+ 0F 97	SETNBE r/m8	V	NE	Set byte if not below or equal (CF=0 and ZF=0).
0F 93	SETNC r/m8	V	V	Set byte if not carry (CF=0).
REX+ 0F 93	SETNC r/m8	V	NE	Set byte if not carry (CF=0).
0F 95	SETNE r/m8	V	V	Set byte if not equal (ZF=0).
REX+ 0F 95	SETNE r/m8	V	NE	Set byte if not equal (ZF=0).
0F 9E	SETNG r/m8	V	V	Set byte if not greater (ZF=1 or SF!= OF)
REX+ 0F 9E	SETNG r/m8	V	NE	Set byte if not greater (ZF=1 or SF!= OF)
0F 9C	SETNGE r/m8	V	V	Set byte if not greater or equal (SF!= OF).
REX+ 0F 9C	SETNGE r/m8	V	NE	Set byte if not greater or equal (SF!= OF).
0F 9D	SETNL r/m8	V	V	Set byte if not less (SF=OF).
REX+ 0F 9D	SETNL r/m8	V	NE	Set byte if not less (SF=OF).
0F 9F	SETNLE r/m8	V	V	Set byte if not less or equal (ZF=0 and SF=OF).
REX+ 0F 9F	SETNLE r/m8	V	NE	Set byte if not less or equal (ZF=0 and SF=OF).
0F 91	SETNO r/m8	V	V	Set byte if not overflow (OF=0).
REX+ 0F 91	SETNO r/m8	V	NE	Set byte if not overflow (OF=0).
0F 9B	SETNP r/m8	V	V	Set byte if not parity (PF=0).
REX+ 0F 9B	SETNP r/m8	V	NE	Set byte if not parity (PF=0).
0F 99	SETNS r/m8	V	V	Set byte if not sign (SF=0).
REX+ 0F 99	SETNS r/m8	V	NE	Set byte if not sign (SF=0).
0F 95	SETNZ r/m8	V	V	Set byte if not zero (ZF=0).
REX+ 0F 95	SETNZ r/m8	V	NE	Set byte if not zero (ZF=0).
0F 90	SETO r/m8	V	V	Set byte if overflow (OF=1)
REX+ 0F 90	SETO r/m8	V	NE	Set byte if overflow (OF=1)
0F 9A	SETP r/m8	V	V	Set byte if parity (PF=1).
REX+ 0F 9A	SETP r/m8	V	NE	Set byte if parity (PF=1).
0F 9A	SETPE r/m8	V	V	Set byte if parity even (PF=1).
REX+ 0F 9A	SETPE r/m8	V	NE	Set byte if parity even (PF=1).
0F 9B	SETPO r/m8	V	V	Set byte if parity odd (PF=0).
REX+ 0F 9B	SETPO r/m8	V	NE	Set byte if parity odd (PF=0).
0F 98	SETS r/m8	V	V	Set byte if sign (SF=1).
REX+ 0F 98	SETS r/m8	V	NE	Set byte if sign (SF=1).
0F 94	SETZ r/m8	V	V	Set byte if zero (ZF=1).
REX+ 0F 94	SETZ r/m8	V	NE	Set byte if zero (ZF=1).
				
0F AE /7	SFENCE	V	V	Serializes store operations.
				
0F 01 /0	SGDT m	V	V	Store GDTR to m.
				
0F A4	SHLD r/m16, r16, imm8	V	V	Shift r/m16 to left imm8 places while shifting bits from r16 in from the right.
0F A5	SHLD r/m16, r16, CL	V	V	Shift r/m16 to left CL places while shifting bits from r16 in from the right.
0F A4	SHLD r/m32, r32, imm8	V	V	Shift r/m32 to left imm8 places while shifting bits from r32 in from the right.
REX.W+ 0F A4 	SHLD r/m64, r64, imm8	V	NE	Shift r/m64 to left imm8 places while shifting bits from r64 in from the right.
0F A5	SHLD r/m32, r32, CL	V	V	Shift r/m32 to left CL places while shifting bits from r32 in from the right.
REX.W+ 0F A5	SHLD r/m64, r64, CL	V	NE	Shift r/m64 to left CL places while shifting bits from r64 in from the right.
				
0F AC	SHRD r/m16, r16, imm8	V	V	Shift r/m16 to right imm8 places while shifting bits from r16 in from the left.
0F AD	SHRD r/m16, r16, CL	V	V	Shift r/m16 to right CL places while shifting bits from r16 in from the left.
0F AC	SHRD r/m32, r32, imm8	V	V	Shift r/m32 to right imm8 places while shifting bits from r32 in from the left.
REX.W+ 0F AC	SHRD r/m64, r64, imm8	V	NE	Shift r/m64 to right imm8 places while shifting bits from r64 in from the left.
0F AD	SHRD r/m32, r32, CL	V	V	Shift r/m32 to right CL places while shifting bits from r32 in from the left.
REX.W+ 0F AD	SHRD r/m64, r64, CL	V	NE	Shift r/m64 to right CL places while shifting bits from r64 in from the left.
				
66 0F C6 /r ib	SHUFPD xmm1, xmm2/m128, imm8	V	V	Shuffle packed double-precision floating- point values selected by imm8 from xmm1 and xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG C6 /r ib	VSHUFPD xmm1, xmm2, xmm3/m128, imm8	V	V	Shuffle Packed double-precision floating- point values selected by imm8 from xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG C6 /r ib	VSHUFPD ymm1, ymm2, ymm3/m256, imm8	V	V	Shuffle Packed double-precision floating- point values selected by imm8 from ymm2 and ymm3/mem.
				
0F C6 /r ib	SHUFPS xmm1, xmm2/m128, imm8	V	V	Shuffle packed single-precision floating-point values selected by imm8 from xmm1 and xmm1/m128 to xmm1.
VEX.NDS.128.0F.WIG C6 /r ib	VSHUFPS xmm1, xmm2, xmm3/m128, imm8	V	V	Shuffle Packed single-precision floating-point values selected by imm8 from xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG C6 /r ib	VSHUFPS ymm1, ymm2, ymm3/m256, imm8	V	V	Shuffle Packed single-precision floating-point values selected by imm8 from ymm2 and ymm3/mem.
				
0F 01 /1	SIDT m	V	V	Store IDTR to m.
				
0F 00 /0	SLDT r/m16	V	V	Stores segment selector from LDTR in r/m16.
REX.W+ 0F 00 /0	SLDT r64/m16	V	V	Stores segment selector from LDTR in r64/m16.
				
0F 01 /4	SMSW r/m16	V	V	Store machine status word to r/m16.
0F 01 /4	SMSW r32/m16	V	V	Store machine status word in low-order 16 bits of r32/m16; high-order 16 bits of r32 are undefined.
REX.W+ 0F 01 /4	SMSW r64/m16	V	V	Store machine status word in low-order 16 bits of r64/m16; high-order 16 bits of r32 are undefined.
				
66 0F 51 /r	SQRTPD xmm1, xmm2/m128	V	V	Computes square roots of the packed double- precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.66.0F.WIG 51 /r	VSQRTPD xmm1, xmm2/m128	V	V	Computes Square Roots of the packed double- precision floating-point values in xmm2/m128 and stores the result in xmm1.
VEX.256.66.0F.WIG 51 /r	VSQRTPD ymm1, ymm2/m256	V	V	Computes Square Roots of the packed double- precision floating-point values in ymm2/m256 and stores the result in ymm1
				
0F 51 /r	SQRTPS xmm1, xmm2/m128	V	V	Computes square roots of the packed single- precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.0F.WIG 51 /r	VSQRTPS xmm1, xmm2/m128	V	V	Computes Square Roots of the packed single- precision floating-point values in xmm2/m128 and stores the result in xmm1.
VEX.256.0F.WIG 51 /r	VSQRTPS ymm1, ymm2/m256	V	V	Computes Square Roots of the packed single- precision floating-point values in ymm2/m256 and stores the result in ymm1.
				
F2 0F 51 /r	SQRTSD xmm1, xmm2/m64	V	V	Computes square root of the low double- precision floating-point value in xmm2/m64 and stores the results in xmm1.
VEX.NDS.LIG.F2.0F.WIG 51 /r	VSQRTSD xmm1,xmm2, xmm3/m64	V	V	Computes square root of the low double- precision floating point value in xmm3/m64 and stores the results in xmm2. Also, upper double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
				
F3 0F 51 /r	SQRTSS xmm1, xmm2/m32	V	V	Computes square root of the low single- precision floating-point value in xmm2/m32 and stores the results in xmm1.
VEX.NDS.LIG.F3.0F.WIG 51	VSQRTSS xmm1, xmm2, xmm3/m32	V	V	Computes square root of the low single- precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
				
F9	STC	V	V	Set CF flag.
				
FD	STD	V	V	Set DF flag.
				
FB	STI	V	V	Set interrupt flag; external, maskable interrupts enabled at the end of the next instruction.
				
0F AE /3	STMXCSR m32	V	V	Store contents of MXCSR register to m32. 
VEX.LZ.0F.WIG AE /3	VSTMXCSR m32	V	V	Store contents of MXCSR register to m32.
				
AA	STOS m8	V	V	For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
AB	STOS m16	V	V	For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
AB	STOS m32	V	V	For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
REX.W+ AB	STOS m64	V	NE	Store RAX at address RDI or EDI.
AA	STOSB	V	V	For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
PREF.66+ AB	STOSW	V	V	For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
AB	STOSD	V	V	For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
REX.W+ AB	STOSQ	V	NE	Store RAX at address RDI or EDI.
				
0F 00 /1	STR r/m16	V	V	Stores segment selector from TR in r/m16.
				
2C ib	SUB AL, imm8	V	V	Subtract imm8 from AL.
2D iw	SUB AX, imm16	V	V	Subtract imm16 from AX.
2D id	SUB EAX, imm32	V	V	Subtract imm32 from EAX.
REX.W+ 2D id	SUB RAX, imm32	V	NE	Subtract imm32 sign-extended to 64-bits from RAX.
80 /5 ib	SUB r/m8, imm8	V	V	Subtract imm8 from r/m8.
REX+ 80 /5 ib	SUB r/m8, imm8	V	NE	Subtract imm8 from r/m8.
81 /5 iw	SUB r/m16, imm16	V	V	Subtract imm16 from r/m16.
81 /5 id	SUB r/m32, imm32	V	V	Subtract imm32 from r/m32.
REX.W+ 81 /5 id	SUB r/m64, imm32	V	NE	Subtract imm32 sign-extended to 64-bits from r/m64.
83 /5 ib	SUB r/m16, imm8	V	V	Subtract sign-extended imm8 from r/m16.
83 /5 ib	SUB r/m32, imm8	V	V	Subtract sign-extended imm8 from r/m32.
REX.W+ 83 /5 ib	SUB r/m64, imm8	V	NE	Subtract sign-extended imm8 from r/m64.
28 /r	SUB r/m8, r8	V	V	Subtract r8 from r/m8.
REX+ 28 /r	SUB r/m8, r8	V	NE	Subtract r8 from r/m8.
29 /r	SUB r/m16, r16	V	V	Subtract r16 from r/m16.
29 /r	SUB r/m32, r32	V	V	Subtract r32 from r/m32.
REX.W+ 29 /r	SUB r/m64, r32	V	NE	Subtract r64 from r/m64.
2A /r	SUB r8, r/m8	V	V	Subtract r/m8 from r8.
REX+ 2A /r	SUB r8, r/m8	V	NE	Subtract r/m8 from r8.
2B /r	SUB r16, r/m16	V	V	Subtract r/m16 from r16.
2B /r	SUB r32, r/m32	V	V	Subtract r/m32 from r32.
REX.W+ 2B /r	SUB r64, r/m64	V	NE	Subtract r/m64 from r64.
				
66 0F 5C /r	SUBPD xmm1, xmm2/m128	V	V	Subtract packed double-precision floating- point values in xmm2/m128 from xmm1.
VEX.NDS.128.66.0F.WIG 5C /r	VSUBPD xmm1, xmm2, xmm3/m128	V	V	Subtract packed double-precision floating- point values in xmm3/mem from xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG 5C /r	VSUBPD ymm1, ymm2, ymm3/m256	V	V	Subtract packed double-precision floating- point values in ymm3/mem from ymm2 and stores result in ymm1.
				
0F 5C /r	SUBPS xmm1, xmm2/m128	V	V	Subtract packed single-precision floating-point values in xmm2/mem from xmm1.
VEX.NDS.128.0F.WIG 5C /r	VSUBPS xmm1, xmm2, xmm3/m128	V	V	Subtract packed single-precision floating-point values in xmm3/mem from xmm2 and stores result in xmm1.
VEX.NDS.256.0F.WIG 5C /r	VSUBPS ymm1, ymm2, ymm3/m256	V	V	Subtract packed single-precision floating-point values in ymm3/mem from ymm2 and stores result in ymm1.
				
F2 0F 5C /r	SUBSD xmm1, xmm2/m64	V	V	Subtracts the low double-precision floating- point values in xmm2/mem64 from xmm1.
VEX.NDS.LIG.F2.0F.WIG 5C /r	VSUBSD xmm1, xmm2, xmm3/m64	V	V	Subtract the low double-precision floating- point value in xmm3/mem from xmm2 and store the result in xmm1.
				
F3 0F 5C /r	SUBSS xmm1, xmm2/m32	V	V	Subtract the lower single-precision floating- point values in xmm2/m32 from xmm1.
VEX.NDS.LIG.F3.0F.WIG 5C /r	VSUBSS xmm1, xmm2, xmm3/m32	V	V	Subtract the low single-precision floating- point value in xmm3/mem from xmm2 and store the result in xmm1.
				
0F 01 F8	SWAPGS	V	I	Exchanges the current GS base register value with the value contained in MSR address C0000102H.
				
0F 05	SYSCALL	V	I	Fast call to privilege level 0 system procedures.
				
0F 34	SYSENTER	V	V	Fast call to privilege level 0 system procedures.
				
0F 35	SYSEXIT	V	V	Fast return to privilege level 3 user code.
REX.W+ 0F 35	SYSEXIT pw	V	V	Fast return to 64-bit mode privilege level 3 user code.
				
0F 07	SYSRET	V	I	Return to compatibility mode from fast system call
REX.W+ 0F 07	SYSRET pw	V	I	Return to 64-bit mode from fast system call
				
A8 ib	TEST AL, imm8	V	V	AND imm8 with AL; set SF, ZF, PF according to result.
A9 iw	TEST AX, imm16	V	V	AND imm16 with AX; set SF, ZF, PF according to result.
A9 id	TEST EAX, imm32	V	V	AND imm32 with EAX; set SF, ZF, PF according to result.
REX.W+ A9 id	TEST RAX, imm32	V	NE	AND imm32 sign-extended to 64-bits with RAX; set SF, ZF, PF according to result.
F6 /0 ib	TEST r/m8, imm8	V	V	AND imm8 with r/m8; set SF, ZF, PF according to result.
REX+ F6 /0 ib	TEST r/m8, imm8	V	NE	AND imm8 with r/m8; set SF, ZF, PF according to result.
F7 /0 iw	TEST r/m16, imm16	V	V	AND imm16 with r/m16; set SF, ZF, PF according to result.
F7 /0 id	TEST r/m32, imm32	V	V	AND imm32 with r/m32; set SF, ZF, PF according to result.
REX.W+ F7 /0 id	TEST r/m64, imm32	V	NE	AND imm32 sign-extended to 64-bits with r/m64; set SF, ZF, PF according to result.
84 /r	TEST r/m8, r8	V	V	AND r8 with r/m8; set SF, ZF, PF according to result.
REX+ 84 /r	TEST r/m8, r8	V	NE	AND r8 with r/m8; set SF, ZF, PF according to result.
85 /r	TEST r/m16, r16	V	V	AND r16 with r/m16; set SF, ZF, PF according to result.
85 /r	TEST r/m32, r32	V	V	AND r32 with r/m32; set SF, ZF, PF according to result.
REX.W+ 85 /r	TEST r/m64, r64	V	NE	AND r64 with r/m64; set SF, ZF, PF according to result.
				
66 0F 2E /r	UCOMISD xmm1, xmm2/m64	V	V	Compares (unordered) the low double- precision floating-point values in xmm1 and xmm2/m64 and set the EFLAGS accordingly.
VEX.LIG.66.0F.WIG 2E /r	VUCOMISD xmm1, xmm2/m64	V	V	Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
				
0F 2E /r	UCOMISS xmm1, xmm2/m32	V	V	Compare lower single-precision floating-point value in xmm1 register with lower single- precision floating-point value in xmm2/mem and set the status flags accordingly.
VEX.LIG.0F.WIG 2E /r	VUCOMISS xmm1, xmm2/m32	V	V	Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
				
0F 0B	UD2	V	V	Raise invalid opcode exception.
				
66 0F 15 /r	UNPCKHPD xmm1, xmm2/m128	V	V	Unpacks and Interleaves double-precision floating-point values from high quadwords of xmm1 and xmm2/m128.
VEX.NDS.128.66.0F.WIG 15 /r	VUNPCKHPD xmm1, xmm2, xmm3/m128	V	V	Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.66.0F.WIG 15 /r	VUNPCKHPD ymm1, ymm2, ymm3/m256	V	V	Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256.
				
0F 15 /r	UNPCKHPS xmm1, xmm2/m128	V	V	Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm1 and xmm2/mem into xmm1.
VEX.NDS.128.0F.WIG 15 /r	VUNPCKHPS xmm1, xmm2, xmm3/m128	V	V	Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.0F.WIG 15 /r	VUNPCKHPS ymm1, ymm2,ymm3/m256	V	V	Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256.
				
66 0F 14 /r	UNPCKLPD xmm1, xmm2/m128	V	V	Unpacks and Interleaves double-precision floating-point values from low quadwords of xmm1 and xmm2/m128.
VEX.NDS.128.66.0F.WIG 14 /r	VUNPCKLPD xmm1, xmm2, xmm3/m128	V	V	Unpacks and Interleaves double precision floating-point values low high quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.66.0F.WIG 14 /r	VUNPCKLPD ymm1, ymm2, ymm3/m256	V	V	Unpacks and Interleaves double precision floating-point values low high quadwords of ymm2 and ymm3/m256.
				
0F 14 /r	UNPCKLPS xmm1, xmm2/m128	V	V	Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm1 and xmm2/mem into xmm1.
VEX.NDS.128.0F.WIG 14 /r	VUNPCKLPS xmm1, xmm2, xmm3/m128	V	V	Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.0F.WIG 14 /r	VUNPCKLPS ymm1, ymm2,ymm3/m256	V	V	Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/m256.
				
VEX.128.66.0F38.W0 18 /r	VBROADCASTSS xmm1, m32	I	V	Broadcast single-precision floating-point element in mem to four locations in xmm1.
VEX.256.66.0F38.W0 18 /r	VBROADCASTSS ymm1, m32	V	V	Broadcast single-precision floating-point element in mem to eight locations in ymm1.
VEX.256.66.0F38.W0 19 /r	VBROADCASTSD ymm1, m64	V	V	Broadcast double-precision floating-point element in mem to four locations in ymm1.
VEX.256.66.0F38.W0 1A /r	VBROADCASTF128 ymm1, m128	V	V	Broadcast 128 bits of floating-point data in mem to low and high 128-bits in ymm1.
				
VEX.256.66.0F38.W0 13 /r	VCVTPH2PS ymm1, xmm2/m128	V	V	Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point value in ymm1.
VEX.128.66.0F38.W0 13 /r	VCVTPH2PS xmm1, xmm2/m64	V	V	Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point value in xmm1.
				
VEX.256.66.0F3A.W0 1D /r ib	VCVTPS2PH xmm1/m128, ymm2, imm8	V	V	Convert eight packed single-precision floating-point value in ymm2 to packed half-precision (16-bit) floating-point value in xmm1/mem. Imm8 provides rounding controls.
VEX.128.66.0F3A.W0.1D /r ib	VCVTPS2PH xmm1/m64, xmm2, imm8	V	V	Convert four packed single-precision float- ing-point value in xmm2 to packed half- precision (16-bit) floating-point value in xmm1/mem. Imm8 provides rounding con- trols.
				
0F 00 /4	VERR r/m16	V	V	Set ZF=1 if segment specified with r/m16 can be read.
0F 00 /5	VERW r/m16	V	V	Set ZF=1 if segment specified with r/m16 can be written.
				
VEX.256.66.0F3A.W0 19 /r ib	VEXTRACTF128 xmm1/m128, ymm2, imm8	V	V	Extract 128 bits of packed floating-point values from ymm2 and store results in xmm1/mem.
				
VEX.NDS.256.66.0F3A.W0 18 /r ib	VINSERTF128 ymm1, ymm2, xmm3/m128, imm8	V	V	Insert a single precision floating-point value selected by imm8 from xmm3/m128 into ymm2 at the specified destination element specified by imm8 and zero out destination elements in ymm1 as indicated in imm8.
				
VEX.NDS.128.66.0F38.W0 2C /r	VMASKMOVPS xmm1, xmm2, m128	V	V	Conditionally load packed single-precision values from m128 using mask in xmm2 and store in xmm1.
VEX.NDS.256.66.0F38.W0 2C /r	VMASKMOVPS ymm1, ymm2, m256	V	V	Conditionally load packed single-precision values from m256 using mask in ymm2 and store in ymm1.
VEX.NDS.128.66.0F38.W0 2D /r	VMASKMOVPD xmm1, xmm2, m128	V	V	Conditionally load packed double-precision values from m128 using mask in xmm2 and store in xmm1.
VEX.NDS.256.66.0F38.W0 2D /r	VMASKMOVPD ymm1, ymm2, m256	V	V	Conditionally load packed double-precision values from m256 using mask in ymm2 and store in ymm1.
VEX.NDS.128.66.0F38.W0 2E /r	VMASKMOVPS m128, xmm1, xmm2	V	V	Conditionally store packed single-precision values from xmm2 using mask in xmm1.
VEX.NDS.256.66.0F38.W0 2E /r	VMASKMOVPS m256, ymm1, ymm2	V	V	Conditionally store packed single-precision values from ymm2 using mask in ymm1.
VEX.NDS.128.66.0F38.W0 2F /r	VMASKMOVPD m128, xmm1, xmm2	V	V	Conditionally store packed double-precision values from xmm2 using mask in xmm1.
VEX.NDS.256.66.0F38.W0 2F /r	VMASKMOVPD m256, ymm1, ymm2	V	V	Conditionally store packed double-precision values from ymm2 using mask in ymm1.
				
VEX.NDS.128.66.0F38.W0 0D /r	VPERMILPD xmm1, xmm2, xmm3/m128	V	V	Permute double-precision floating-point values in xmm2 using controls from xmm3/mem and store result in xmm1.
VEX.NDS.256.66.0F38.W0 0D /r	VPERMILPD ymm1, ymm2, ymm3/m256	V	V	Permute double-precision floating-point values in ymm2 using controls from ymm3/mem and store result in ymm1.
VEX.128.66.0F3A.W0 05 /r ib	VPERMILPD xmm1, xmm2/m128, imm8	V	V	Permute double-precision floating-point values in xmm2/mem using controls from imm8.
VEX.256.66.0F3A.W0 05 /r ib	VPERMILPD ymm1, ymm2/m256, imm8	V	V	Permute double-precision floating-point values in ymm2/mem using controls from imm8.
				
VEX.NDS.128.66.0F38.W0 0C /r	VPERMILPS xmm1, xmm2, xmm3/m128	V	V	Permute single-precision floating-point values in xmm2 using controls from xmm3/mem and store result in xmm1.
VEX.128.66.0F3A.W0 04 /r ib	VPERMILPS xmm1, xmm2/m128, imm8	V	V	Permute single-precision floating-point values in xmm2/mem using controls from imm8 and store result in xmm1.
VEX.NDS.256.66.0F38.W0 0C /r	VPERMILPS ymm1, ymm2, ymm3/m256	V	V	Permute single-precision floating-point values in ymm2 using controls from ymm3/mem and store result in ymm1.
VEX.256.66.0F3A.W0 04 /r ib	VPERMILPS ymm1, ymm2/m256, imm8	V	V	Permute single-precision floating-point values in ymm2/mem using controls from imm8 and store result in ymm1.
				
VEX.NDS.256.66.0F3A.W0 06 /r ib	VPERM2F128 ymm1, ymm2, ymm3/m256, imm8	V	V	Permute 128-bit floating-point fields in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
				
VEX.128.66.0F38.W0 0E /r	VTESTPS xmm1, xmm2/m128	V	V	Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating- point sources.
VEX.256.66.0F38.W0 0E /r	VTESTPS ymm1, ymm2/m256	V	V	Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating- point sources.
VEX.128.66.0F38.W0 0F /r	VTESTPD xmm1, xmm2/m128	V	V	Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating- point sources.
VEX.256.66.0F38.W0 0F /r	VTESTPD ymm1, ymm2/m256	V	V	Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating- point sources.
				
VEX.256.0F.WIG 77	VZEROALL	V	V	Zero all YMM registers.
				
VEX.128.0F.WIG 77	VZEROUPPER	V	V	Zero upper 128 bits of all YMM registers.
				
9B	WAIT	V	V	Check pending unmasked floating-point exceptions.
9B	FWAIT	V	V	Check pending unmasked floating-point exceptions.
				
0F 09	WBINVD	V	V	Write back and flush Internal caches; initiate writing-back and flushing of external caches.
				
F3 0F AE /2	WRFSBASE r32	V	I	Load the FS base address with the 32-bit value in the source register.
REX.W+ F3 0F AE /2	WRFSBASE r64	V	I	Load the FS base address with the 64-bit value in the source register.
F3 0F AE /3	WRGSBASE r32	V	I	Load the GS base address with the 32-bit value in the source register.
REX.W+ F3 0F AE /3	WRGSBASE r64	V	I	Load the GS base address with the 64-bit value in the source register.
				
0F 30	WRMSR	V	V	Write the value in EDX:EAX to MSR specified by ECX.
				
0F C0 /r	XADD r/m8, r8	V	V	Exchange r8 and r/m8; load sum into r/m8.
REX+ 0F C0 /r	XADD r/m8, r8	V	NE	Exchange r8 and r/m8; load sum into r/m8.
0F C1 /r	XADD r/m16, r16	V	V	Exchange r16 and r/m16; load sum into r/m16.
0F C1 /r	XADD r/m32, r32	V	V	Exchange r32 and r/m32; load sum into r/m32.
REX.W+ 0F C1 /r	XADD r/m64, r64	V	NE	Exchange r64 and r/m64; load sum into r/m64.
				
90 +rw	XCHG AX, r16	V	V	Exchange r16 with AX.
90 +rw	XCHG r16, AX	V	V	Exchange AX with r16.
90 +rd	XCHG EAX, r32	V	V	Exchange r32 with EAX.
REX.W+ 90 +rd	XCHG RAX, r64	V	NE	Exchange r64 with RAX.
90 +rd	XCHG r32, EAX	V	V	Exchange EAX with r32.
REX.W+ 90 +rd	XCHG r64, RAX	V	NE	Exchange RAX with r64.
86 /r	XCHG r/m8, r8	V	V	Exchange r8 (byte register) with byte from r/m8.
REX+ 86 /r	XCHG r/m8, r8	V	NE	Exchange r8 (byte register) with byte from r/m8.
86 /r	XCHG r8, r/m8	V	V	Exchange byte from r/m8 with r8 (byte register).
REX+ 86 /r	XCHG r8, r/m8	V	NE	Exchange byte from r/m8 with r8 (byte register).
87 /r	XCHG r/m16, r16	V	V	Exchange r16 with word from r/m16.
87 /r	XCHG r16, r/m16	V	V	Exchange word from r/m16 with r16.
87 /r	XCHG r/m32, r32	V	V	Exchange r32 with doubleword from r/m32.
REX.W+ 87 /r	XCHG r/m64, r64	V	NE	Exchange r64 with quadword from r/m64.
87 /r	XCHG r32, r/m32	V	V	Exchange doubleword from r/m32 with r32.
REX.W+ 87 /r	XCHG r64, r/m64	V	NE	Exchange quadword from r/m64 with r64.
				
0F 01 D0	XGETBV	V	V	Reads an XCR specified by ECX into EDX:EAX.
				
D7	XLAT m8	V	V	Set AL to memory byte DS:[(E)BX + unsigned AL].
D7	XLATB	V	V	Set AL to memory byte DS:[(E)BX + unsigned AL].
REX.W+ D7	XLATB	V	NE	Set AL to memory byte [RBX + unsigned AL].
				
34 ib	XOR AL, imm8	V	V	AL XOR imm8.
35 iw	XOR AX, imm16	V	V	AX XOR imm16.
35 id	XOR EAX, imm32	V	V	EAX XOR imm32.
REX.W+ 35 id	XOR RAX, imm32	V	NE	RAX XOR imm32 (sign-extended).
80 /6 ib	XOR r/m8, imm8	V	V	r/m8 XOR imm8.
REX+ 80 /6 ib	XOR r/m8, imm8	V	NE	r/m8 XOR imm8.
81 /6 iw	XOR r/m16, imm16	V	V	r/m16 XOR imm16.
81 /6 id	XOR r/m32, imm32	V	V	r/m32 XOR imm32.
REX.W+ 81 /6 id	XOR r/m64, imm32	V	NE	r/m64 XOR imm32 (sign-extended).
83 /6 ib	XOR r/m16, imm8	V	V	r/m16 XOR imm8 (sign-extended).
83 /6 ib	XOR r/m32, imm8	V	V	r/m32 XOR imm8 (sign-extended).
REX.W+ 83 /6 ib	XOR r/m64, imm8	V	NE	r/m64 XOR imm8 (sign-extended).
30 /r	XOR r/m8, r8	V	V	r/m8 XOR r8.
REX+ 30 /r	XOR r/m8, r8	V	NE	r/m8 XOR r8.
31 /r	XOR r/m16, r16	V	V	r/m16 XOR r16.
31 /r	XOR r/m32, r32	V	V	r/m32 XOR r32.
REX.W+ 31 /r	XOR r/m64, r64	V	NE	r/m64 XOR r64.
32 /r	XOR r8, r/m8	V	V	r8 XOR r/m8.
REX+ 32 /r	XOR r8, r/m8	V	NE	r8 XOR r/m8.
33 /r	XOR r16, r/m16	V	V	r16 XOR r/m16.
33 /r	XOR r32, r/m32	V	V	r32 XOR r/m32.
REX.W+ 33 /r	XOR r64, r/m64	V	NE	r64 XOR r/m64.
				
66 0F 57 /r	XORPD xmm1, xmm2/m128	V	V	Bitwise exclusive-OR of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 57 /r	VXORPD xmm1, xmm2, xmm3/m128	V	V	Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 57 /r	VXORPD ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/mem.
				
0F 57 /r	XORPS xmm1, xmm2/m128	V	V	Bitwise exclusive-OR of xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 57 /r	VXORPS xmm1, xmm2, xmm3/m128	V	V	Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 57 /r	VXORPS ymm1, ymm2, ymm3/m256	V	V	Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/mem.
				
0F AE /5	XRSTOR mem	V	V	Restore processor extended states from memory. The states are specified by EDX:EAX
REX.W+ 0F AE /5	XRSTOR64 mem	V	NE	Restore processor extended states from memory. The states are specified by EDX:EAX
				
0F AE /4	XSAVE mem	V	V	Save processor extended states to memory. The states are specified by EDX:EAX
REX.W+ 0F AE /4	XSAVE64 mem	V	NE	Save processor extended states to memory. The states are specified by EDX:EAX
				
0F AE /6	XSAVEOPT mem	V	V	Save processor extended states specified in EDX:EAX to memory, optimizing the state save operation if possible.
REX.W+ 0F AE /6	XSAVEOPT64 mem	V	V	Save processor extended states specified in EDX:EAX to memory, optimizing the state save operation if possible.
				
0F 01 D1	XSETBV	V	V	Write the value in EDX:EAX to the XCR specified by ECX.
